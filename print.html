<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">

<head>
    <!-- Book generated using mdBook -->
    <meta charset="UTF-8">
    <title>Using dbcrossbar</title>
    <meta name="robots" content="noindex" />
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta name="description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#ffffff" />

    <!-- dbcrossbar metadata for social media "unfurling" -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://www.dbcrossbar.org/">
    <meta property="og:title" content="Using dbcrossbar">
    <meta property="og:description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta property="og:image" content="https://www.dbcrossbar.org/dbcrossbar_intro.png">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="www.dbcrossbar.org">
    <meta property="twitter:title" content="Using dbcrossbar">
    <meta property="twitter:description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta property="twitter:image" content="https://www.dbcrossbar.org/dbcrossbar_intro.png">
    <meta property="twitter:url" content="https://www.dbcrossbar.org/">

    <link rel="shortcut icon" href="">
    <link rel="stylesheet" href="css/variables.css">
    <link rel="stylesheet" href="css/general.css">
    <link rel="stylesheet" href="css/chrome.css">
    <link rel="stylesheet" href="css/print.css" media="print">

    <!-- Fonts -->
    <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
    <link
        href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

    <!-- Highlight.js Stylesheets -->
    <link rel="stylesheet" href="highlight.css">
    <link rel="stylesheet" href="tomorrow-night.css">
    <link rel="stylesheet" href="ayu-highlight.css">

    <!-- Custom theme stylesheets -->
</head>

<body>
    <!-- Provide site root to javascript -->
    <script type="text/javascript">
        var path_to_root = "";
        var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
    </script>

    <!-- Work around some values being stored in localStorage wrapped in quotes -->
    <script type="text/javascript">
        try {
            var theme = localStorage.getItem('mdbook-theme');
            var sidebar = localStorage.getItem('mdbook-sidebar');

            if (theme.startsWith('"') && theme.endsWith('"')) {
                localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
            }

            if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
            }
        } catch (e) { }
    </script>

    <!-- Set the theme before any content is loaded, prevents flash -->
    <script type="text/javascript">
        var theme;
        try { theme = localStorage.getItem('mdbook-theme'); } catch (e) { }
        if (theme === null || theme === undefined) { theme = default_theme; }
        var html = document.querySelector('html');
        html.classList.remove('no-js')
        html.classList.remove('light')
        html.classList.add(theme);
        html.classList.add('js');
    </script>

    <!-- Hide / unhide sidebar before it is displayed -->
    <script type="text/javascript">
        var html = document.querySelector('html');
        var sidebar = 'hidden';
        if (document.body.clientWidth >= 1080) {
            try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch (e) { }
            sidebar = sidebar || 'visible';
        }
        html.classList.remove('sidebar-visible');
        html.classList.add("sidebar-" + sidebar);
    </script>

    <nav id="sidebar" class="sidebar" aria-label="Table of contents">
        <div class="sidebar-scrollbox">
            <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">dbcrossbar Guide</a></li><li class="chapter-item expanded "><a href="features.html"><strong aria-hidden="true">1.</strong> Features &amp; philosophy</a></li><li class="chapter-item expanded "><a href="installing.html"><strong aria-hidden="true">2.</strong> Installing</a></li><li class="chapter-item expanded "><a href="how.html"><strong aria-hidden="true">3.</strong> How it works</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="csv_interchange.html"><strong aria-hidden="true">3.1.</strong> CSV interchange format</a></li><li class="chapter-item expanded "><a href="schema.html"><strong aria-hidden="true">3.2.</strong> Portable table schema</a></li></ol></li><li class="chapter-item expanded "><a href="config.html"><strong aria-hidden="true">4.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="commands.html"><strong aria-hidden="true">5.</strong> Commands</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cp.html"><strong aria-hidden="true">5.1.</strong> cp: Copying tables</a></li><li class="chapter-item expanded "><a href="count.html"><strong aria-hidden="true">5.2.</strong> count: Counting records</a></li><li class="chapter-item expanded "><a href="conv.html"><strong aria-hidden="true">5.3.</strong> schema conv: Transforming schemas</a></li></ol></li><li class="chapter-item expanded "><a href="drivers.html"><strong aria-hidden="true">6.</strong> Drivers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bigml.html"><strong aria-hidden="true">6.1.</strong> BigML</a></li><li class="chapter-item expanded "><a href="bigquery.html"><strong aria-hidden="true">6.2.</strong> BigQuery</a></li><li class="chapter-item expanded "><a href="csv.html"><strong aria-hidden="true">6.3.</strong> CSV</a></li><li class="chapter-item expanded "><a href="file.html"><strong aria-hidden="true">6.4.</strong> File</a></li><li class="chapter-item expanded "><a href="gs.html"><strong aria-hidden="true">6.5.</strong> Google Cloud Storage</a></li><li class="chapter-item expanded "><a href="postgres.html"><strong aria-hidden="true">6.6.</strong> PostgreSQL</a></li><li class="chapter-item expanded "><a href="redshift.html"><strong aria-hidden="true">6.7.</strong> RedShift</a></li><li class="chapter-item expanded "><a href="s3.html"><strong aria-hidden="true">6.8.</strong> S3</a></li><li class="chapter-item expanded "><a href="shopify.html"><strong aria-hidden="true">6.9.</strong> Shopify (UNSTABLE, DEPRECATED)</a></li></ol></li><li class="chapter-item expanded "><a href="schemas.html"><strong aria-hidden="true">7.</strong> Specifying table schemas</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="postgres-sql.html"><strong aria-hidden="true">7.1.</strong> Postgres CREATE TABLE</a></li><li class="chapter-item expanded "><a href="bigquery-schema.html"><strong aria-hidden="true">7.2.</strong> BigQuery JSON schemas</a></li><li class="chapter-item expanded "><a href="dbcrossbar-schema.html"><strong aria-hidden="true">7.3.</strong> Native dbcrossbar schemas</a></li><li class="chapter-item expanded "><a href="dbcrossbar-ts.html"><strong aria-hidden="true">7.4.</strong> TypeScript schemas (UNSTABLE)</a></li></ol></li><li class="chapter-item expanded "><a href="changes.html"><strong aria-hidden="true">8.</strong> Changes</a></li><li class="chapter-item expanded affix "><a href="credits.html">Credits and contributors</a></li></ol>
        </div>
        <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
    </nav>

    <div id="page-wrapper" class="page-wrapper">

        <div class="page">
            <div id="menu-bar-hover-placeholder"></div>
            <div id="menu-bar" class="menu-bar sticky bordered">
                <div class="left-buttons">
                    <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents"
                        aria-label="Toggle Table of Contents" aria-controls="sidebar">
                        <i class="fa fa-bars"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button" type="button" title="Change theme"
                        aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                        <i class="fa fa-paint-brush"></i>
                    </button>
                    <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                        <li role="none"><button role="menuitem" class="theme"
                                id="light">Light (default)</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="rust">Rust</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="coal">Coal</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="navy">Navy</button></li>
                        <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button>
                        </li>
                    </ul>
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)"
                        aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S"
                        aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                </div>

                <h1 class="menu-title">Using dbcrossbar</h1>

                <div class="right-buttons">
                    <a href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a href="https://github.com/dbcrossbar/dbcrossbar" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>

            <div id="search-wrapper" class="hidden">
                <form id="searchbar-outer" class="searchbar-outer">
                    <input type="search" name="search" id="searchbar" name="searchbar"
                        placeholder="Search this book ..." aria-controls="searchresults-outer"
                        aria-describedby="searchresults-header">
                </form>
                <div id="searchresults-outer" class="searchresults-outer hidden">
                    <div id="searchresults-header" class="searchresults-header"></div>
                    <ul id="searchresults">
                    </ul>
                </div>
            </div>
            <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
            <script type="text/javascript">
                document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                Array.from(document.querySelectorAll('#sidebar a')).forEach(function (link) {
                    link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                });
            </script>

            <div id="content" class="content">
                <main>
                    <h1 id="dbcrossbar-guide"><a class="header" href="#dbcrossbar-guide"><code>dbcrossbar</code> Guide</a></h1>
<p><code>dbcrossbar</code> is an <a href="https://github.com/dbcrossbar/dbcrossbar">open source</a> tool that copies large, tabular datasets between many different databases and storage formats. Data can be copied from any source to any destination.</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="514pt" height="368pt"
 viewBox="0.00 0.00 514.16 368.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 364)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-364 510.16,-364 510.16,4 -4,4"/><!-- csv_in --><g id="node1" class="node"><title>csv_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-342" rx="30.59" ry="18"/><text text-anchor="middle" x="77.34" y="-338.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar --><g id="node3" class="node"><title>dbcrossbar</title><ellipse fill="none" stroke="black" cx="253.08" cy="-180" rx="62.29" ry="18"/><text text-anchor="middle" x="253.08" y="-176.3" font-family="Times,serif" font-size="14.00">dbcrossbar</text></g><!-- csv_in&#45;&gt;dbcrossbar --><g id="edge1" class="edge"><title>csv_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M106.44,-336.4C121.79,-332.33 140.51,-325.62 154.69,-315 193.3,-286.08 222.97,-237.47 238.94,-207.15"/><polygon fill="black" stroke="black" points="242.18,-208.49 243.64,-198 235.96,-205.29 242.18,-208.49"/></g><!-- csv_out --><g id="node2" class="node"><title>csv_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-342" rx="30.59" ry="18"/><text text-anchor="middle" x="428.82" y="-338.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar&#45;&gt;csv_out --><g id="edge2" class="edge"><title>dbcrossbar&#45;&gt;csv_out</title><path fill="none" stroke="black" d="M262.53,-198C276.88,-226.72 308.68,-282.95 351.48,-315 362.77,-323.46 376.95,-329.44 390.02,-333.58"/><polygon fill="black" stroke="black" points="389.14,-336.97 399.72,-336.4 391.09,-330.25 389.14,-336.97"/></g><!-- postgres_out --><g id="node5" class="node"><title>postgres_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-288" rx="65.79" ry="18"/><text text-anchor="middle" x="428.82" y="-284.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- dbcrossbar&#45;&gt;postgres_out --><g id="edge4" class="edge"><title>dbcrossbar&#45;&gt;postgres_out</title><path fill="none" stroke="black" d="M270.96,-197.29C289.35,-215.29 320.31,-243.29 351.48,-261 358.26,-264.86 365.7,-268.31 373.19,-271.36"/><polygon fill="black" stroke="black" points="372.06,-274.67 382.65,-275 374.57,-268.14 372.06,-274.67"/></g><!-- bigquery_out --><g id="node7" class="node"><title>bigquery_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-234" rx="54.69" ry="18"/><text text-anchor="middle" x="428.82" y="-230.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- dbcrossbar&#45;&gt;bigquery_out --><g id="edge6" class="edge"><title>dbcrossbar&#45;&gt;bigquery_out</title><path fill="none" stroke="black" d="M296.14,-193.07C321.07,-200.82 352.79,-210.68 378.85,-218.78"/><polygon fill="black" stroke="black" points="377.94,-222.16 388.53,-221.79 380.02,-215.48 377.94,-222.16"/></g><!-- s3_out --><g id="node9" class="node"><title>s3_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-180" rx="27" ry="18"/><text text-anchor="middle" x="428.82" y="-176.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- dbcrossbar&#45;&gt;s3_out --><g id="edge8" class="edge"><title>dbcrossbar&#45;&gt;s3_out</title><path fill="none" stroke="black" d="M315.82,-180C341.14,-180 369.56,-180 391.44,-180"/><polygon fill="black" stroke="black" points="391.62,-183.5 401.62,-180 391.62,-176.5 391.62,-183.5"/></g><!-- gs_out --><g id="node11" class="node"><title>gs_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-126" rx="77.19" ry="18"/><text text-anchor="middle" x="428.82" y="-122.3" font-family="Times,serif" font-size="14.00">Cloud Storage</text></g><!-- dbcrossbar&#45;&gt;gs_out --><g id="edge10" class="edge"><title>dbcrossbar&#45;&gt;gs_out</title><path fill="none" stroke="black" d="M296.14,-166.93C318.92,-159.85 347.37,-151 371.99,-143.35"/><polygon fill="black" stroke="black" points="373.06,-146.69 381.57,-140.37 370.98,-140 373.06,-146.69"/></g><!-- redshift_out --><g id="node13" class="node"><title>redshift_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-72" rx="51.19" ry="18"/><text text-anchor="middle" x="428.82" y="-68.3" font-family="Times,serif" font-size="14.00">RedShift</text></g><!-- dbcrossbar&#45;&gt;redshift_out --><g id="edge12" class="edge"><title>dbcrossbar&#45;&gt;redshift_out</title><path fill="none" stroke="black" d="M270.96,-162.71C289.35,-144.71 320.31,-116.71 351.48,-99 359.91,-94.21 369.35,-90.03 378.63,-86.5"/><polygon fill="black" stroke="black" points="379.99,-89.73 388.21,-83.04 377.61,-83.14 379.99,-89.73"/></g><!-- etc_out --><g id="node15" class="node"><title>etc_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="428.82" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- dbcrossbar&#45;&gt;etc_out --><g id="edge14" class="edge"><title>dbcrossbar&#45;&gt;etc_out</title><path fill="none" stroke="black" d="M262.53,-162C276.88,-133.28 308.68,-77.05 351.48,-45 363.61,-35.91 379.06,-29.69 392.89,-25.53"/><polygon fill="black" stroke="black" points="393.89,-28.89 402.6,-22.86 392.03,-22.14 393.89,-28.89"/></g><!-- postgres_in --><g id="node4" class="node"><title>postgres_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-288" rx="65.79" ry="18"/><text text-anchor="middle" x="77.34" y="-284.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- postgres_in&#45;&gt;dbcrossbar --><g id="edge3" class="edge"><title>postgres_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M123.52,-275C134.06,-271.16 145.03,-266.49 154.69,-261 181.96,-245.5 209.07,-222.13 227.74,-204.47"/><polygon fill="black" stroke="black" points="230.42,-206.75 235.2,-197.29 225.57,-201.7 230.42,-206.75"/></g><!-- bigquery_in --><g id="node6" class="node"><title>bigquery_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-234" rx="54.69" ry="18"/><text text-anchor="middle" x="77.34" y="-230.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- bigquery_in&#45;&gt;dbcrossbar --><g id="edge5" class="edge"><title>bigquery_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M117.77,-221.75C142.18,-214.16 173.81,-204.33 200.24,-196.11"/><polygon fill="black" stroke="black" points="201.57,-199.36 210.08,-193.05 199.5,-192.68 201.57,-199.36"/></g><!-- s3_in --><g id="node8" class="node"><title>s3_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-180" rx="27" ry="18"/><text text-anchor="middle" x="77.34" y="-176.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- s3_in&#45;&gt;dbcrossbar --><g id="edge7" class="edge"><title>s3_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M104.35,-180C124.5,-180 153.42,-180 180.34,-180"/><polygon fill="black" stroke="black" points="180.54,-183.5 190.54,-180 180.54,-176.5 180.54,-183.5"/></g><!-- gs_in --><g id="node10" class="node"><title>gs_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-126" rx="77.19" ry="18"/><text text-anchor="middle" x="77.34" y="-122.3" font-family="Times,serif" font-size="14.00">Cloud Storage</text></g><!-- gs_in&#45;&gt;dbcrossbar --><g id="edge9" class="edge"><title>gs_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M124.45,-140.33C147.77,-147.58 176.17,-156.41 200.25,-163.89"/><polygon fill="black" stroke="black" points="199.35,-167.27 209.93,-166.9 201.42,-160.59 199.35,-167.27"/></g><!-- redshift_in --><g id="node12" class="node"><title>redshift_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-72" rx="51.19" ry="18"/><text text-anchor="middle" x="77.34" y="-68.3" font-family="Times,serif" font-size="14.00">RedShift</text></g><!-- redshift_in&#45;&gt;dbcrossbar --><g id="edge11" class="edge"><title>redshift_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M117.96,-83.04C130.14,-87.21 143.32,-92.54 154.69,-99 181.96,-114.5 209.07,-137.87 227.74,-155.53"/><polygon fill="black" stroke="black" points="225.57,-158.3 235.2,-162.71 230.42,-153.25 225.57,-158.3"/></g><!-- etc_in --><g id="node14" class="node"><title>etc_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="77.34" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- etc_in&#45;&gt;dbcrossbar --><g id="edge13" class="edge"><title>etc_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M103.56,-22.86C119.4,-26.84 139.61,-33.71 154.69,-45 193.3,-73.92 222.97,-122.53 238.94,-152.85"/><polygon fill="black" stroke="black" points="235.96,-154.71 243.64,-162 242.18,-151.51 235.96,-154.71"/></g></g></svg></div>
<h2 id="an-example"><a class="header" href="#an-example">An example</a></h2>
<p>If we have a CSV file <code>my_table.csv</code> containing data:</p>
<pre><code class="language-csv">id,name,quantity
1,Blue widget,10
2,Red widget,50
</code></pre>
<p>And a file <code>my_table.sql</code> containing a table definition:</p>
<pre><code class="language-sql">CREATE TABLE my_table (
    id INT NOT NULL,
    name TEXT NOT NULL,
    quantity INT NOT NULL
);
</code></pre>
<p>Then we can use these to create a PostgreSQL table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
<p>If we want to use this data to update an existing table in BigQuery, we can upsert into BigQuery using the <code>id</code> column:</p>
<pre><code class="language-sh">dbcrossbar config add temporary gs://$GS_TEMP_BUCKET
dbcrossbar config add temporary bigquery:$GCLOUD_PROJECT:temp_dataset
dbcrossbar cp \
    --if-exists=upsert-on:id \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table
</code></pre>
<p>This will stream the data out of PostgreSQL, upload it to <code>$GS_TEMP_BUCKET</code>, import it into a temporary BigQuery table, and run an appropriate <code>MERGE</code> command. The <code>config add temporary</code> commands tell <code>dbcrossbar</code> what cloud bucket and BigQuery dataset should be used for temporary files and tables, respectively.</p>
<p>Notice that we don't need to specify <code>--schema</code>, because <code>dbcrossbar</code> will automatically translate the PostgreSQL column types to corresponding BigQuery types.</p>
<h2 id="credits"><a class="header" href="#credits">Credits</a></h2>
<p><code>dbcrossbar</code> is generously supported by <a href="http://faraday.io/">Faraday</a> and by open source contributors. Please see the <a href="./credits.html">credits</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="features--philosophy"><a class="header" href="#features--philosophy">Features &amp; philosophy</a></h1>
<p><code>dbcrossbar</code> is designed to do a few things well. Typically, <code>dbcrossbar</code> is used for loading raw data, and for moving data back and forth between production databases and data warehouses. It supports a few core features:</p>
<ol>
<li>Copying tables.</li>
<li>Counting records in tables.</li>
<li>Converting between different table schema formats, including PostgreSQL <code>CREATE TABLE</code> statements and BigQuery schema JSON.</li>
</ol>
<p><code>dbcrossbar</code> offers a number of handy features:</p>
<ul>
<li>A single static binary on Linux, with no dependencies.</li>
<li>A stream-based architecture that limits the use of RAM and requires no temporary files.</li>
<li>Support for appending, overwriting or upserting into existing tables.</li>
<li>Support for selecting records using <code>--where</code>.</li>
</ul>
<p><code>dbcrossbar</code> also supports a rich variety of portable column types:</p>
<ul>
<li>Common types, including booleans, dates, timestamps, floats, integers, and text.</li>
<li>UUIDs.</li>
<li>JSON.</li>
<li>GeoJSON.</li>
<li>Arrays.</li>
</ul>
<h2 id="non-features"><a class="header" href="#non-features">Non-features</a></h2>
<p>The following features are explicitly excluded from <code>dbcrossbar</code>'s mission:</p>
<ul>
<li>Data cleaning and transformation.</li>
<li>Fixing invalid column names.</li>
<li>Copying multiple tables at time.</li>
<li>Automatically copying constraints, foreign keys, etc.</li>
</ul>
<p>If you need these features, then take a look at tools like <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> and <a href="https://pgloader.io/"><code>pgloader</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing"><a class="header" href="#installing">Installing</a></h1>
<p>Pre-built binaries for <code>dbcrossbar</code> are <a href="https://github.com/dbcrossbar/dbcrossbar/releases">available on GitHub</a>. These currently include:</p>
<ol>
<li>Fully-static Linux x86_64 binaries, which should work on any modern distribution (including Alpine Linux containers).</li>
<li>MacOS X binaries. Note that these are unsigned, and you may need to use <code>xattr -d com.apple.quarantine dbcrossbar</code> and <code>spctl --add dbcrossbar</code> to make them runnable.</li>
</ol>
<p>Windows binaries are not available at this time, but it may be possible to build them with a little work.</p>
<h2 id="required-tools"><a class="header" href="#required-tools">Required tools</a></h2>
<p>To use the S3 and RedShift drivers, you will need to install the <a href="https://aws.amazon.com/cli/">AWS CLI tools</a>. We plan to replace the AWS CLI tools with native Rust libraries before the 1.0 release.</p>
<h2 id="installing-using-cargo"><a class="header" href="#installing-using-cargo">Installing using <code>cargo</code></a></h2>
<p>You can also install <code>dbcrossbar</code> using <code>cargo</code>. First, you will need to make sure you have the necessary C dependencies installed:</p>
<pre><code class="language-sh"># Ubuntu Linux (might be incomplete).
sudo apt install build-essential
</code></pre>
<p>Then, you can install using <code>cargo</code>:</p>
<pre><code class="language-sh">cargo install dbcrossbar
</code></pre>
<h2 id="building-from-source"><a class="header" href="#building-from-source">Building from source</a></h2>
<p>The source code is available <a href="https://github.com/dbcrossbar/dbcrossbar">on GitHub</a>. First, install the build dependencies as described above. Then run:</p>
<pre><code class="language-sh">git clone https://github.com/dbcrossbar/dbcrossbar.git
cd dbcrossbar
cargo build --release
</code></pre>
<p>This will create <code>target/release/dbcrossbar</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h1>
<p><code>dbcrossbar</code> uses pluggable input and output drivers, allowing any input to be copied to any output:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="467pt" height="260pt"
 viewBox="0.00 0.00 467.37 260.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 256)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-256 463.37,-256 463.37,4 -4,4"/><!-- csv_in --><g id="node1" class="node"><title>csv_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-234" rx="30.59" ry="18"/><text text-anchor="middle" x="65.64" y="-230.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar --><g id="node3" class="node"><title>dbcrossbar</title><ellipse fill="none" stroke="black" cx="229.68" cy="-126" rx="62.29" ry="18"/><text text-anchor="middle" x="229.68" y="-122.3" font-family="Times,serif" font-size="14.00">dbcrossbar</text></g><!-- csv_in&#45;&gt;dbcrossbar --><g id="edge1" class="edge"><title>csv_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M92.67,-225.17C104.82,-220.54 119.22,-214.31 131.29,-207 157.95,-190.85 184.95,-167.75 203.73,-150.35"/><polygon fill="black" stroke="black" points="206.37,-152.67 211.26,-143.28 201.57,-147.57 206.37,-152.67"/></g><!-- csv_out --><g id="node2" class="node"><title>csv_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-234" rx="30.59" ry="18"/><text text-anchor="middle" x="393.72" y="-230.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar&#45;&gt;csv_out --><g id="edge2" class="edge"><title>dbcrossbar&#45;&gt;csv_out</title><path fill="none" stroke="black" d="M248.11,-143.28C266.7,-161.01 297.61,-188.54 328.08,-207 337.13,-212.48 347.49,-217.36 357.23,-221.4"/><polygon fill="black" stroke="black" points="356.11,-224.72 366.69,-225.17 358.7,-218.22 356.11,-224.72"/></g><!-- postgres_out --><g id="node5" class="node"><title>postgres_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-180" rx="65.79" ry="18"/><text text-anchor="middle" x="393.72" y="-176.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- dbcrossbar&#45;&gt;postgres_out --><g id="edge4" class="edge"><title>dbcrossbar&#45;&gt;postgres_out</title><path fill="none" stroke="black" d="M271.15,-139.49C292.56,-146.62 319.06,-155.45 341.85,-163.05"/><polygon fill="black" stroke="black" points="340.76,-166.37 351.36,-166.22 342.98,-159.73 340.76,-166.37"/></g><!-- bigquery_out --><g id="node7" class="node"><title>bigquery_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-126" rx="54.69" ry="18"/><text text-anchor="middle" x="393.72" y="-122.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- dbcrossbar&#45;&gt;bigquery_out --><g id="edge6" class="edge"><title>dbcrossbar&#45;&gt;bigquery_out</title><path fill="none" stroke="black" d="M292.4,-126C304.3,-126 316.79,-126 328.76,-126"/><polygon fill="black" stroke="black" points="329.11,-129.5 339.11,-126 329.11,-122.5 329.11,-129.5"/></g><!-- s3_out --><g id="node9" class="node"><title>s3_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-72" rx="27" ry="18"/><text text-anchor="middle" x="393.72" y="-68.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- dbcrossbar&#45;&gt;s3_out --><g id="edge8" class="edge"><title>dbcrossbar&#45;&gt;s3_out</title><path fill="none" stroke="black" d="M271.15,-112.51C298.46,-103.41 334.05,-91.55 359.66,-83.02"/><polygon fill="black" stroke="black" points="360.9,-86.29 369.29,-79.81 358.69,-79.65 360.9,-86.29"/></g><!-- etc_out --><g id="node11" class="node"><title>etc_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="393.72" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- dbcrossbar&#45;&gt;etc_out --><g id="edge10" class="edge"><title>dbcrossbar&#45;&gt;etc_out</title><path fill="none" stroke="black" d="M248.11,-108.72C266.7,-90.99 297.61,-63.46 328.08,-45 337.85,-39.08 349.16,-33.87 359.56,-29.64"/><polygon fill="black" stroke="black" points="361.12,-32.79 369.17,-25.9 358.58,-26.26 361.12,-32.79"/></g><!-- postgres_in --><g id="node4" class="node"><title>postgres_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-180" rx="65.79" ry="18"/><text text-anchor="middle" x="65.64" y="-176.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- postgres_in&#45;&gt;dbcrossbar --><g id="edge3" class="edge"><title>postgres_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M107.95,-166.23C129.35,-159.1 155.66,-150.33 178.25,-142.81"/><polygon fill="black" stroke="black" points="179.63,-146.04 188.01,-139.55 177.42,-139.4 179.63,-146.04"/></g><!-- bigquery_in --><g id="node6" class="node"><title>bigquery_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-126" rx="54.69" ry="18"/><text text-anchor="middle" x="65.64" y="-122.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- bigquery_in&#45;&gt;dbcrossbar --><g id="edge5" class="edge"><title>bigquery_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M120.6,-126C132.25,-126 144.75,-126 156.96,-126"/><polygon fill="black" stroke="black" points="157.12,-129.5 167.12,-126 157.12,-122.5 157.12,-129.5"/></g><!-- s3_in --><g id="node8" class="node"><title>s3_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-72" rx="27" ry="18"/><text text-anchor="middle" x="65.64" y="-68.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- s3_in&#45;&gt;dbcrossbar --><g id="edge7" class="edge"><title>s3_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M90.21,-79.85C113.23,-87.53 149.02,-99.45 178.44,-109.26"/><polygon fill="black" stroke="black" points="177.56,-112.65 188.15,-112.49 179.77,-106.01 177.56,-112.65"/></g><!-- etc_in --><g id="node10" class="node"><title>etc_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="65.64" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- etc_in&#45;&gt;dbcrossbar --><g id="edge9" class="edge"><title>etc_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M90.19,-25.9C102.82,-30.61 118.39,-37.19 131.29,-45 157.95,-61.15 184.95,-84.25 203.73,-101.65"/><polygon fill="black" stroke="black" points="201.57,-104.43 211.26,-108.72 206.37,-99.33 201.57,-104.43"/></g></g></svg></div>
<h2 id="parallel-data-streams"><a class="header" href="#parallel-data-streams">Parallel data streams</a></h2>
<p>Internally, <code>dbcrossbar</code> uses parallel data streams. If we copy <code>s3://example/</code> to <code>csv:out/</code> using <code>--max-streams=4</code>, this will run up to 4 copies in parallel:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="451pt" height="206pt"
 viewBox="0.00 0.00 450.86 206.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 202)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-202 446.86,-202 446.86,4 -4,4"/><!-- src1 --><g id="node1" class="node"><title>src1</title><ellipse fill="none" stroke="black" cx="114.39" cy="-180" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-176.3" font-family="Times,serif" font-size="14.00">s3://example/file_1.csv</text></g><!-- dest1 --><g id="node2" class="node"><title>dest1</title><ellipse fill="none" stroke="black" cx="353.82" cy="-180" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-176.3" font-family="Times,serif" font-size="14.00">csv:out/file_1.csv</text></g><!-- src1&#45;&gt;dest1 --><g id="edge1" class="edge"><title>src1&#45;&gt;dest1</title><path fill="none" stroke="black" d="M228.89,-180C237.45,-180 246.05,-180 254.5,-180"/><polygon fill="black" stroke="black" points="254.73,-183.5 264.73,-180 254.73,-176.5 254.73,-183.5"/></g><!-- src2 --><g id="node3" class="node"><title>src2</title><ellipse fill="none" stroke="black" cx="114.39" cy="-126" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-122.3" font-family="Times,serif" font-size="14.00">s3://example/file_2.csv</text></g><!-- src1&#45;&gt;src2 --><!-- dest2 --><g id="node4" class="node"><title>dest2</title><ellipse fill="none" stroke="black" cx="353.82" cy="-126" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-122.3" font-family="Times,serif" font-size="14.00">csv:out/file_2.csv</text></g><!-- src2&#45;&gt;dest2 --><g id="edge2" class="edge"><title>src2&#45;&gt;dest2</title><path fill="none" stroke="black" d="M228.89,-126C237.45,-126 246.05,-126 254.5,-126"/><polygon fill="black" stroke="black" points="254.73,-129.5 264.73,-126 254.73,-122.5 254.73,-129.5"/></g><!-- src3 --><g id="node5" class="node"><title>src3</title><ellipse fill="none" stroke="black" cx="114.39" cy="-72" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-68.3" font-family="Times,serif" font-size="14.00">s3://example/file_3.csv</text></g><!-- src2&#45;&gt;src3 --><!-- dest3 --><g id="node6" class="node"><title>dest3</title><ellipse fill="none" stroke="black" cx="353.82" cy="-72" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-68.3" font-family="Times,serif" font-size="14.00">csv:out/file_3.csv</text></g><!-- src3&#45;&gt;dest3 --><g id="edge3" class="edge"><title>src3&#45;&gt;dest3</title><path fill="none" stroke="black" d="M228.89,-72C237.45,-72 246.05,-72 254.5,-72"/><polygon fill="black" stroke="black" points="254.73,-75.5 264.73,-72 254.73,-68.5 254.73,-75.5"/></g><!-- src4 --><g id="node8" class="node"><title>src4</title><ellipse fill="none" stroke="black" cx="114.39" cy="-18" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-14.3" font-family="Times,serif" font-size="14.00">s3://example/file_4.csv</text></g><!-- src3&#45;&gt;src4 --><!-- dest4 --><g id="node7" class="node"><title>dest4</title><ellipse fill="none" stroke="black" cx="353.82" cy="-18" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-14.3" font-family="Times,serif" font-size="14.00">csv:out/file_4.csv</text></g><!-- src4&#45;&gt;dest4 --><g id="edge4" class="edge"><title>src4&#45;&gt;dest4</title><path fill="none" stroke="black" d="M228.89,-18C237.45,-18 246.05,-18 254.5,-18"/><polygon fill="black" stroke="black" points="254.73,-21.5 264.73,-18 254.73,-14.5 254.73,-21.5"/></g></g></svg></div>
<p>As soon as one stream finishes, a new one will be started:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="451pt" height="44pt"
 viewBox="0.00 0.00 450.86 44.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 40)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-40 446.86,-40 446.86,4 -4,4"/><!-- src5 --><g id="node1" class="node"><title>src5</title><ellipse fill="none" stroke="black" cx="114.39" cy="-18" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-14.3" font-family="Times,serif" font-size="14.00">s3://example/file_5.csv</text></g><!-- dest5 --><g id="node2" class="node"><title>dest5</title><ellipse fill="none" stroke="black" cx="353.82" cy="-18" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-14.3" font-family="Times,serif" font-size="14.00">csv:out/file_5.csv</text></g><!-- src5&#45;&gt;dest5 --><g id="edge1" class="edge"><title>src5&#45;&gt;dest5</title><path fill="none" stroke="black" d="M228.89,-18C237.45,-18 246.05,-18 254.5,-18"/><polygon fill="black" stroke="black" points="254.73,-21.5 264.73,-18 254.73,-14.5 254.73,-21.5"/></g></g></svg></div>
<p><code>dbcrossbar</code> accomplishes this using a <strong>stream of CSV streams.</strong> This allows us to make extensive use of <a href="https://ferd.ca/queues-don-t-fix-overload.html">backpressure</a> to control how data flows through the system, eliminating the need for temporary files. This makes it easier to work with 100GB+ CSV files and 1TB+ datasets.</p>
<h2 id="shortcuts"><a class="header" href="#shortcuts">Shortcuts</a></h2>
<p>When copying between certain drivers, <code>dbcrossbar</code> supports &quot;shortcuts.&quot; For example, it can load data directly from Google Cloud Storage into BigQuery.</p>
<h2 id="multi-threaded-asynchronous-rust"><a class="header" href="#multi-threaded-asynchronous-rust">Multi-threaded, asynchronous Rust</a></h2>
<p><code>dbcrossbar</code> is written using <a href="https://rust-lang.github.io/async-book/">asynchronous</a> <a href="https://www.rust-lang.org/">Rust</a>, and it makes heavy use of a multi-threaded worker pool. Internally, it works something like a set of classic Unix pipelines running in parallel. Thanks to Rust, it has been possible to get native performance and multithreading without spending too much time debugging.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csv-interchange-format"><a class="header" href="#csv-interchange-format">CSV interchange format</a></h1>
<p>Internally, <code>dbcrossbar</code> converts all data into CSV streams. For many standard types, all input drivers are required to provide byte-for-byte identical CSV data:</p>
<pre><code class="language-csv">id,test_bool,test_date,test_int16,test_int32,test_int64,test_text,test_timestamp_without_time_zone,test_timestamp_with_time_zone,test_uuid,select,testCapitalized,test_enum
1,t,1969-07-20,-32768,-2147483648,-9223372036854775808,hello,1969-07-20T20:17:39,1969-07-20T20:17:39Z,084ec3bb-3193-4ffb-8b74-99a288e8432c,,,red
2,f,2001-01-01,32767,2147483647,9223372036854775807,,,,,,,green
3,,,,,,,,,,,,
</code></pre>
<p>For more complex types such as arrays, structs, JSON, and GeoJSON data, we embed JSON into the CSV file:</p>
<pre><code class="language-csv">test_null,test_not_null,test_bool,test_bool_array,test_date,test_date_array,test_float32,test_float32_array,test_float64,test_float64_array,test_geojson,test_geojson_3857,test_int16,test_int16_array,test_int32,test_int32_array,test_int64,test_int64_array,test_json,test_text,test_text_array,test_timestamp_without_time_zone,test_timestamp_without_time_zone_array,test_timestamp_with_time_zone,test_timestamp_with_time_zone_array,test_uuid,test_uuid_array,test_enum
,hi,t,&quot;[true,false]&quot;,1969-07-20,&quot;[&quot;&quot;1969-07-20&quot;&quot;]&quot;,1e+37,&quot;[1e-37,0,100.125,1e+37]&quot;,1e+37,&quot;[1e-37,0,1000.125,1e+37]&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,16,&quot;[-32768,0,32767]&quot;,32,&quot;[-2147483648,0,2147483647]&quot;,64,&quot;[&quot;&quot;-9223372036854775808&quot;&quot;,&quot;&quot;0&quot;&quot;,&quot;&quot;9223372036854775807&quot;&quot;]&quot;,&quot;{&quot;&quot;x&quot;&quot;: 1, &quot;&quot;y&quot;&quot;: 2}&quot;,hello,&quot;[&quot;&quot;hello&quot;&quot;,&quot;&quot;&quot;&quot;]&quot;,1969-07-20T20:17:39.5,&quot;[&quot;&quot;1969-07-20T20:17:39.5&quot;&quot;]&quot;,1969-07-20T20:17:39.5Z,&quot;[&quot;&quot;1969-07-20T20:17:39.5Z&quot;&quot;]&quot;,084ec3bb-3193-4ffb-8b74-99a288e8432c,&quot;[&quot;&quot;084ec3bb-3193-4ffb-8b74-99a288e8432c&quot;&quot;]&quot;,red
</code></pre>
<h2 id="tricks-for-preparing-csv-data"><a class="header" href="#tricks-for-preparing-csv-data">Tricks for preparing CSV data</a></h2>
<p>If your input CSV files use an incompatible format, there are several things that might help. If your CSV files are invalid, non-standard, or full of junk, then you may be able to use <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> or <a href="https://github.com/BurntSushi/xsv"><code>xsv</code></a> to fix the worst problems.</p>
<p>If you need to clean up your data manually, then you may want to consider using <code>dbcrossbar</code> to load your data into BigQuery, and set your columns to type <code>STRING</code>. Once this is done, you can parse and normalize your data quickly using SQL queries.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="portable-table-schema"><a class="header" href="#portable-table-schema">Portable table schema</a></h1>
<p>Internally, <code>dbcrossbar</code> uses a portable table &quot;schema&quot; format. This provides a common ground between PostgreSQL's <code>CREATE TABLE</code> statements, <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery's JSON schemas</a>, and equivalent formats for other databases. For more information, see:</p>
<ul>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/index.html">The <code>dbcrossbar</code> schema format</a>.</li>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/enum.DataType.html">The <code>dbcrossbar</code> column types</a>.</li>
</ul>
<p>All table schemas and column types are converted into the portable format and then into the appropriate destination format.</p>
<p>Normally, you won't need to work with this schema format directly, because <code>dbcrossbar</code> can parse BigQuery schemas, PostgreSQL <code>CREATE TABLE</code> statments, and several other popular schema formats. It can also read schemas directly from some databases. See the <a href="./conv.html"><code>conv</code> command</a> for details.</p>
<h2 id="example-schema"><a class="header" href="#example-schema">Example schema</a></h2>
<pre><code class="language-json">{
    &quot;named_data_types&quot;: [
        {
            &quot;name&quot;: &quot;format&quot;,
            &quot;data_type&quot;: {
                &quot;one_of&quot;: [
                    &quot;gif&quot;,
                    &quot;jpeg&quot;
                ]
            }
        }
    ],
    &quot;tables&quot;: [
        {
            &quot;name&quot;: &quot;images&quot;,
            &quot;columns&quot;: [
                {
                    &quot;name&quot;: &quot;id&quot;,
                    &quot;is_nullable&quot;: false,
                    &quot;data_type&quot;: &quot;uuid&quot;
                },
                {
                    &quot;name&quot;: &quot;url&quot;,
                    &quot;is_nullable&quot;: false,
                    &quot;data_type&quot;: &quot;text&quot;
                },
                {
                    &quot;name&quot;: &quot;format&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: {
                        &quot;named&quot;: &quot;format&quot;
                    }
                },
                {
                    &quot;name&quot;: &quot;metadata&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: &quot;json&quot;
                },
                {
                    &quot;name&quot;: &quot;thumbnails&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: {
                        &quot;array&quot;: {
                            &quot;struct&quot;: [
                                {
                                    &quot;name&quot;: &quot;url&quot;,
                                    &quot;is_nullable&quot;: false,
                                    &quot;data_type&quot;: &quot;text&quot;
                                },
                                {
                                    &quot;name&quot;: &quot;width&quot;,
                                    &quot;data_type&quot;: &quot;float64&quot;,
                                    &quot;is_nullable&quot;: false
                                },
                                {
                                    &quot;name&quot;: &quot;height&quot;,
                                    &quot;data_type&quot;: &quot;float64&quot;,
                                    &quot;is_nullable&quot;: false
                                }
                            ]
                        }
                    }
                }
            ]
        }
    ]
}
</code></pre>
<h2 id="schema-properties"><a class="header" href="#schema-properties">Schema properties</a></h2>
<ul>
<li><code>named_data_types</code> (experimental): Named data types. These are used to declare custom column types. They're analogous to a Postgres <code>CREATE TYPE</code> statement, or a TypeScript <code>interface</code> or <code>type</code> statement. Two types with different names but the same layout are considered to be different types (<a href="https://en.wikipedia.org/wiki/Nominal_type_system">nominal typing</a>). Many database drivers place restrictions on these types for now, but we hope to relax those restrictions in the future.</li>
<li><code>tables</code>: A list of table definitions. For now, this must contain exactly one element.</li>
</ul>
<h2 id="table-properties"><a class="header" href="#table-properties">Table properties</a></h2>
<ul>
<li><code>name</code>: The name of this table. This is normally only used when serializing to schema formats that require a table name.</li>
<li><code>columns</code>: A list of columns in the table.</li>
</ul>
<h2 id="column-properties"><a class="header" href="#column-properties">Column properties</a></h2>
<ul>
<li><code>name</code>: The name of the column.</li>
<li><code>is_nullable</code>: Can the column contain <code>NULL</code> values?</li>
<li><code>data_type</code>: The type of data stored in the column.</li>
</ul>
<h2 id="data-types"><a class="header" href="#data-types">Data types</a></h2>
<p>The <code>data_type</code> field can contain any of:</p>
<ul>
<li><code>{ &quot;array&quot;: element_type }</code>: An array of <code>element_type</code> values.</li>
<li><code>&quot;bool&quot;</code>: A boolean value.</li>
<li><code>&quot;date&quot;</code>: A date, with no associated time value.</li>
<li><code>&quot;decimal&quot;</code>: A decimal integer (can represent currency, etc., without rounding errors).</li>
<li><code>&quot;float32&quot;</code>: A 32-bit floating point number.</li>
<li><code>&quot;float64&quot;</code>: A 64-bit floating point number.</li>
<li><code>{ &quot;geo_json&quot;: srid }</code>: Geodata in GeoJSON format, using the specified <a href="https://en.wikipedia.org/wiki/Spatial_reference_system">SRID</a>, to specify the spatial reference system.</li>
<li><code>&quot;int16&quot;</code>: A 16-bit signed integer.</li>
<li><code>&quot;int32&quot;</code>: A 32-bit signed integer.</li>
<li><code>&quot;int64&quot;</code>: A 64-bit signed integer.</li>
<li><code>&quot;json&quot;</code>: An arbitrary JSON value.</li>
<li><code>{ &quot;named&quot;: name }</code>: A named type from the <code>named_types</code> list in this schema.</li>
<li><code>{ &quot;one_of&quot;: string_list }</code>: One of the specified string values. This may be represented a string enumeration by certain drivers, or as a &quot;categorical&quot; value in machine-learning systems (as opposed to a free-form textual value). <strong>NOTE:</strong> In many cases, it would be more efficient to have a separate table with an <code>(id, value)</code> entry for each enum value, and to refer to it using a foreign key. <code>one_of</code> is most useful when importing cleaned data, and when working with databases that support efficient string enumerations.</li>
<li><code>{ &quot;struct&quot;: fields }</code>: A structure with a list of specific, named fields. Each field has the following properties:
<ul>
<li><code>name</code>: The name of the field.</li>
<li><code>is_nullable</code>: Can the field contain <code>NULL</code> values?</li>
<li><code>data_type</code>: The type of data stored in the field.</li>
</ul>
</li>
<li><code>&quot;text&quot;</code>: A string.</li>
<li><code>&quot;timestamp_without_time_zone&quot;</code>: A date and time without an associated timezone.</li>
<li><code>&quot;timestamp_with_time_zone&quot;</code>: A date and time with an associated timezone.</li>
<li><code>&quot;uuid&quot;</code>: A UUID value.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuring-dbcrossbar"><a class="header" href="#configuring-dbcrossbar">Configuring <code>dbcrossbar</code></a></h1>
<p><code>dbcrossbar</code> can read information from a configuration directory. By default, this can be found at:</p>
<ul>
<li>Linux: <code>~/.config</code></li>
<li>MacOS: <code>~/Library/Preferences</code></li>
</ul>
<p>To override this default location, you can set <code>DBCROSSBAR_CONFIG_DIR</code> to point to an alternate configuration directory.</p>
<p>If a file <code>dbcrossbar.toml</code> appears in this directory, <code>dbcrossbar</code> will read its configuration from that file. Other files may be placed in this directory, including certain local credential files.</p>
<h2 id="modifying-the-configuration-file"><a class="header" href="#modifying-the-configuration-file">Modifying the configuration file</a></h2>
<p>You can modify the <code>dbcrossbar.toml</code> file from the command line using the <code>config</code> subcommand. For example:</p>
<pre><code class="language-sh">dbcrossbar config add temporary s3://example/temp/
dbcrossbar config rm temporary s3://example/temp/
</code></pre>
<p>Using <code>config add temporary</code> allows you to specify default values for <code>--temporary</code> flags. You can still override specific defaults by passing <code>--temporary</code> to commands that use it.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="commands"><a class="header" href="#commands">Commands</a></h1>
<p><code>dbcrossbar</code> supports four main subcommands:</p>
<ul>
<li><code>dbcrossbar cp</code>: Copy tabular data.</li>
<li><code>dbcrossbar count</code>: Count records.</li>
<li><code>dbcrossbar schema conv</code>: Convert table schemas between databases.</li>
</ul>
<p>For more information, type <code>dbcrossbar --help</code> or <code>dbcrossbar $CMD --help</code>.</p>
<p>Not all drivers support all the features of each command. To see the available drivers and what commands they support, run <code>dbcrossbar features</code> and <code>dbcrossbar features $DRIVER_NAME</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cp-copying-tables"><a class="header" href="#cp-copying-tables"><code>cp</code>: Copying tables</a></h1>
<p>The <code>cp</code> command copies tabular data from a source location to a destination location. For example, we can copy a CSV file into PostgreSQL, replacing any existing table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
<p>Or we copy data from PostgreSQL and upsert it into a BigQuery table:</p>
<pre><code class="language-sh">dbcrossbar config add temporary gs://$GS_TEMP_BUCKET
dbcrossbar config add temporary bigquery:$GCLOUD_PROJECT:temp_dataset
dbcrossbar cp \
    --if-exists=upsert-on:id \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table
</code></pre>
<h2 id="command-line-help"><a class="header" href="#command-line-help">Command-line help</a></h2>
<pre><code class="language-txt">
Usage: dbcrossbar cp [OPTIONS] &lt;FROM_LOCATOR&gt; &lt;TO_LOCATOR&gt;

Arguments:
  &lt;FROM_LOCATOR&gt;  The input table
  &lt;TO_LOCATOR&gt;    The output table

Options:
      --if-exists &lt;IF_EXISTS&gt;      One of `error`, `overwrite`, `append` or `upsert-on:COL` [default:
                                   error]
      --schema &lt;SCHEMA&gt;            The schema to use (defaults to input table schema)
      --temporary &lt;TEMPORARIES&gt;    Temporary directories, cloud storage buckets, datasets to use during
                                   transfer (can be repeated)
      --stream-size &lt;STREAM_SIZE&gt;  Specify the approximate size of the CSV streams manipulated by
                                   `dbcrossbar`. This can be used to split a large input into multiple
                                   smaller outputs. Actual data streams may be bigger or smaller
                                   depending on a number of factors. Examples: &quot;100000&quot;, &quot;1Gb&quot;
      --from-arg &lt;FROM_ARGS&gt;       Pass an extra argument of the form `key=value` to the source driver
      --from-format &lt;FROM_FORMAT&gt;  For directory- and file-like data sources, the format to assume. If
                                   not specified, `dbcrossbar` will use the file extension to guess the
                                   format
      --to-arg &lt;TO_ARGS&gt;           Pass an extra argument of the form `key=value` to the destination
                                   driver
  -F, --to-format &lt;TO_FORMAT&gt;      For directory-like data destinations, the format to use. If not
                                   specified, `dbcrossbar` will use the destination file extension (if
                                   provided) or `csv`
      --where &lt;WHERE_CLAUSE&gt;       SQL where clause specifying rows to use
  -J, --max-streams &lt;MAX_STREAMS&gt;  How many data streams should we attempt to copy in parallel? [default:
                                   4]
      --display-output-locators    Display where we wrote our output data
  -h, --help                       Print help

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table
</code></pre>
<h2 id="flags"><a class="header" href="#flags">Flags</a></h2>
<p>Not all command-line options are supported by all drivers. See the chapter on each driver for details.</p>
<h3 id="--where"><a class="header" href="#--where"><code>--where</code></a></h3>
<p>Specify a <code>WHERE</code> clause to include in the SQL query. This can be used to select a subset of the source rows.</p>
<h3 id="--from-arg"><a class="header" href="#--from-arg"><code>--from-arg</code></a></h3>
<p>This can be used to specify driver-specific options for the source driver. See the chapter for that driver.</p>
<h3 id="--if-existserror"><a class="header" href="#--if-existserror"><code>--if-exists=error</code></a></h3>
<p>If the destination location already contains data, exit with an error.</p>
<h3 id="--if-existsappend"><a class="header" href="#--if-existsappend"><code>--if-exists=append</code></a></h3>
<p>If the destination location already contains data, append the new data.</p>
<h3 id="--if-existsoverwrite"><a class="header" href="#--if-existsoverwrite"><code>--if-exists=overwrite</code></a></h3>
<p>If the destination location already contains data, replace it with the new data.</p>
<h3 id="--if-existsupset-oncol1"><a class="header" href="#--if-existsupset-oncol1"><code>--if-exists=upset-on:COL1,..</code></a></h3>
<p>For every row in the new data:</p>
<ul>
<li>If a row with a matching <code>col1</code>, <code>col2</code>, etc., exists, use the new data to update that row.</li>
<li>If no row matching <code>col1</code>, <code>col2</code>, etc., exists, then insert the new row.</li>
</ul>
<p>The columns <code>col1</code>, <code>col2</code>, etc., must be marked as <code>NOT NULL</code>.</p>
<h3 id="--schema"><a class="header" href="#--schema"><code>--schema</code></a></h3>
<p>By default, <code>dbcrossbar</code> will use the schema of the source table. But when this can't be inferred automatically, <code>--schema</code> can be used to specify a table schema:</p>
<ul>
<li><code>--schema=postgres-sql:my_table.sql</code>: A PostgreSQL <code>CREATE TABLE</code> statement.</li>
<li><code>--schema=bigquery-schema:my_table.json</code>: A <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery JSON schema</a>.</li>
<li><code>--schema=dbcrossbar-schema:my_table.json</code>: An <a href="./schema.html">internal <code>dbcrossbar</code> schema</a>.</li>
</ul>
<p>It's also possible to use a schema from an existing database table:</p>
<ul>
<li><code>--schema=postgres://localhost:5432/db#table</code></li>
<li><code>--schema=bigquery:project:dataset.table</code></li>
</ul>
<p>Note that it's possible to create a BigQuery table using a PostgreSQL schema, or vice versa. Internally, all schemes are first converted to the <a href="./schema.html">internal schema format</a>.</p>
<h3 id="--temporary"><a class="header" href="#--temporary"><code>--temporary</code></a></h3>
<p>Specify temporary storage, which is required by certain drivers. Typical values include:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code></li>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code></li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<h3 id="--to-arg"><a class="header" href="#--to-arg"><code>--to-arg</code></a></h3>
<p>This can be used to specify driver-specific options for the destination driver. See the chapter for that driver.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="count-counting-records"><a class="header" href="#count-counting-records">count: Counting records</a></h1>
<p>This command mostly works like the <a href="./cp.html"><code>cp</code></a> command, except that it prints out a number of rows. Check your driver to see if it supports <code>count</code>.</p>
<h2 id="command-line-help-1"><a class="header" href="#command-line-help-1">Command-line help</a></h2>
<pre><code class="language-txt">
Usage: dbcrossbar count [OPTIONS] &lt;LOCATOR&gt;

Arguments:
  &lt;LOCATOR&gt;  The locator specifying the records to count

Options:
      --schema &lt;SCHEMA&gt;            The schema to use (defaults to input table schema)
      --temporary &lt;TEMPORARIES&gt;    Temporary directories, cloud storage buckets, datasets to use during
                                   transfer (can be repeated)
      --from-arg &lt;FROM_ARGS&gt;       Pass an extra argument of the form `key=value` to the source driver
      --from-format &lt;FROM_FORMAT&gt;  For directory- and file-like data sources, the format to assume. If
                                   not specified, `dbcrossbar` will use the file extension to guess the
                                   format
      --where &lt;WHERE_CLAUSE&gt;       SQL where clause specifying rows to use
  -h, --help                       Print help

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-conv-transforming-schemas"><a class="header" href="#schema-conv-transforming-schemas"><code>schema conv</code>: Transforming schemas</a></h1>
<p>The <code>schema conv</code> command can be used to convert between different database schemas. To convert from a PostgreSQL <code>CREATE TABLE</code> statement to a BigQuery schema, run:</p>
<pre><code class="language-sh">dbcrossbar schema conv postgres-sql:table.sql bigquery-schema:table.json
</code></pre>
<p>As a handy trick, you can also use a CSV source, which will generate a <code>CREATE TABLE</code> where all columns have the type <code>TEXT</code>:</p>
<pre><code class="language-sh">dbcrossbar schema conv csv:data.csv postgres-sql:table.sql
</code></pre>
<p>This can then be edited to specify appropriate column types.</p>
<h2 id="command-line-help-2"><a class="header" href="#command-line-help-2">Command-line help</a></h2>
<pre><code class="language-txt">
Usage: dbcrossbar schema conv [OPTIONS] &lt;FROM_LOCATOR&gt; &lt;TO_LOCATOR&gt;

Arguments:
  &lt;FROM_LOCATOR&gt;  The input schema
  &lt;TO_LOCATOR&gt;    The output schema

Options:
      --if-exists &lt;IF_EXISTS&gt;  One of `error`, `overrwrite` or `append` [default: error]
      --from-arg &lt;FROM_ARGS&gt;   Pass an extra argument of the form `key=value` to the source driver
      --to-arg &lt;TO_ARGS&gt;       Pass an extra argument of the form `key=value` to the destination driver
  -h, --help                   Print help

EXAMPLE LOCATORS:
    postgres-sql:table.sql
    postgres://localhost:5432/db#table
    bigquery-schema:table.json
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drivers"><a class="header" href="#drivers">Drivers</a></h1>
<p><code>dbcrossbar</code> uses built-in &quot;drivers&quot; to read and write CSV data and table schemas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigml"><a class="header" href="#bigml">BigML</a></h1>
<p><a href="https://bigml.com/">BigML</a> is a hosted machine-learning service, with support for many common algorithms and server-side batch scripts.</p>
<h2 id="example-locators"><a class="header" href="#example-locators">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>bigml:dataset/$ID</code>: Read data from a BigML dataset.</li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>bigml:source</code>: Create a single BigML &quot;source&quot; resource from the input data.</li>
<li><code>bigml:sources</code>: Create multiple BigML &quot;source&quot; resources from the input data.</li>
<li><code>bigml:dataset</code>: Create a single BigML &quot;dataset&quot; resource from the input data.</li>
<li><code>bigml:datasets</code>: Create multiple BigML &quot;dataset&quot; resources from the input data.</li>
</ul>
<p>If you use BigML as a destination, <code>dbcrossbar</code> will automatically activate <code>--display-output-locators</code>, and it will print locators for all the created resources on standard output. Column types on created &quot;source&quot; resources will be set something appropriate (but see <code>optype_for_text</code> below.)</p>
<h2 id="configuration--authentication"><a class="header" href="#configuration--authentication">Configuration &amp; authentication</a></h2>
<p>The BigML driver requires more configuration than most.</p>
<p>You'll need to set the following environment variables:</p>
<ul>
<li><code>BIGML_USERNAME</code>: Set this to your BigML username.</li>
<li><code>BIGML_API_KEY</code>: Set this to your BigML API key.</li>
<li><code>BIGML_DOMAIN</code> (optional): Set this to the domain name of your BigML instance, if it's not located at the standard address.</li>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials when using BigML as a destination. Do <strong>not</strong> set <code>AWS_SESSION_TOKEN</code>; it will not work with BigML.</li>
</ul>
<p>You'll also need to pass the following on the command line when using:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading into BigML. This is not needed when using BigML as a source.</li>
</ul>
<p>You can also specify the following <code>--to-arg</code> values:</p>
<ul>
<li><code>name</code>: The human-readable name of the resource to create.</li>
<li><code>optype_for_text</code>: The BigML optype to use for text fields. This defaults to <code>text</code>. You may want to set it to <code>categorical</code> if your text fields contain a small set of fixed strings. But you should probably use <code>dbcrossbar</code>'s <code>{ &quot;one_of&quot;: string_list }</code> types instead, which will always map to <code>categorical</code>.</li>
<li><code>tag</code>: This may be specified repeatedly to attach tags to the created resources.</li>
</ul>
<h2 id="supported-features"><a class="header" href="#supported-features">Supported features</a></h2>
<pre><code class="language-txt">bigml features:
- conv FROM
- cp FROM:
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<p>Note that <code>--if-exists</code> is simply ignored, because BigML will always create new resources.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigquery"><a class="header" href="#bigquery">BigQuery</a></h1>
<p>Google's <a href="https://cloud.google.com/bigquery/">BigQuery</a> is a extremely scalable data warehouse that supports rich SQL queries and petabytes of data. If you need to transform or analyze huge data sets, it's an excellent tool.</p>
<p>When loading data into BigQuery, or extracting it, we always go via Google Cloud Storage. This is considerably faster than the load and extract functionality supplied by tools like <code>bq</code>.</p>
<h2 id="example-locators-1"><a class="header" href="#example-locators-1">Example locators</a></h2>
<ul>
<li><code>bigquery:$PROJECT:$DATASET.$TABLE</code>: A BigQuery table.</li>
<li><code>bigquery-test-fixture:$PROJECT:$DATASET.$TABLE</code>: If you only need a tiny, read-only &quot;table&quot; for testing purposes, you may want to try the <code>bigquery-test-fixture:</code> locator. It currently uses <a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/insert"><code>tables.insert</code></a> to pass a <code>table.view.query</code> with all the table data inlined into the <code>VIEW</code> SQL. This runs about 20 times faster than <code>bigquery:</code>, at the expense of not creating a regular table. Note that the implementation details of this method may change, if we discover a faster or better way to create a small, read-only table.</li>
</ul>
<h2 id="configuration--authentication-1"><a class="header" href="#configuration--authentication-1">Configuration &amp; authentication</a></h2>
<p>See <a href="./gs.html#configuration--authentication">the Cloud Storage driver</a> for authentication details.</p>
<p>The following command-line options will usually need to be specified for both sources and destinations:</p>
<ul>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code>: A Google Cloud Storage bucket to use for staging data in both directions.</li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<p>You can also specify Google Cloud resource labels to apply to all BigQuery jobs. Labels are often used to track query costs.</p>
<ul>
<li><code>--from-arg=job_labels[department]=marketing</code></li>
<li><code>--to-arg=job_labels[project]=project1</code></li>
</ul>
<p>You can pick a project to run jobs in, for example for billing purposes.</p>
<ul>
<li><code>--from-arg=job_project_id=my-gcp-billing-project</code></li>
<li><code>--to-arg=job_project_id=my-gcp-billing-project</code></li>
</ul>
<h2 id="supported-features-1"><a class="header" href="#supported-features-1">Supported features</a></h2>
<pre><code class="language-txt">bigquery features:
- conv FROM
- count
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp FROM:
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csv"><a class="header" href="#csv">CSV</a></h1>
<p><code>dbcrossbar</code> works with valid CSV files in our <a href="./csv_interchange.html">CSV interchange format</a>. For invalid CSV files, take a look at <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a>. For CSV files which need further transformation and parsing, considering loading them into BigQuery and cleaning them up using SQL. This works very well even for large datasets.</p>
<h2 id="example-locators-2"><a class="header" href="#example-locators-2">Example locators</a></h2>
<p>The following locators can be used for both input and output:</p>
<ul>
<li><code>csv:file.csv</code>: A single CSV file.</li>
<li><code>csv:dir/</code>: A directory tree containing CSV files.</li>
<li><code>csv:-</code>: Read from standard input, or write to standard output.</li>
</ul>
<p>To concatenate CSV files, use:</p>
<pre><code class="language-sh">dbcrossbar cp csv:input/ csv:merged.csv
</code></pre>
<p>To split a CSV file, use <code>--stream-size</code>:</p>
<pre><code class="language-sh">dbcrossbar cp --stream-size=&quot;100Mb&quot; csv:giant.csv csv:split/
</code></pre>
<h2 id="configuration--authentication-2"><a class="header" href="#configuration--authentication-2">Configuration &amp; authentication</a></h2>
<p>None.</p>
<h2 id="supported-features-2"><a class="header" href="#supported-features-2">Supported features</a></h2>
<pre><code class="language-txt">csv features:
- conv FROM
- cp FROM:
- cp TO:
  --if-exists=error --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="files"><a class="header" href="#files">Files</a></h1>
<p><code>dbcrossbar</code> can read and/or write files in a number of formats, including:</p>
<ul>
<li>Our <a href="./csv_interchange.html">CSV interchange format</a>. For invalid CSV files, take a look at <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a>. For CSV files which need further transformation and parsing, considering loading them into BigQuery and cleaning them up using SQL. This works very well even for large datasets.</li>
<li><a href="https://jsonlines.org/">JSON Lines</a> (input only).</li>
</ul>
<h2 id="example-locators-3"><a class="header" href="#example-locators-3">Example locators</a></h2>
<p>The following locators can be used for both input and output (if supported by the format):</p>
<ul>
<li><code>file:file.csv</code>: A single CSV file.</li>
<li><code>file:dir/</code>: A directory tree containing CSV files.</li>
<li><code>file:-</code>: Read from standard input, or write to standard output.</li>
</ul>
<p>To concatenate CSV files, use:</p>
<pre><code class="language-sh">dbcrossbar cp csv:input/ csv:merged.csv
</code></pre>
<p>To split a CSV file, use <code>--stream-size</code>:</p>
<pre><code class="language-sh">dbcrossbar cp --stream-size=&quot;100Mb&quot; csv:giant.csv csv:split/
</code></pre>
<h2 id="configuration--authentication-3"><a class="header" href="#configuration--authentication-3">Configuration &amp; authentication</a></h2>
<p>None.</p>
<h2 id="supported-features-3"><a class="header" href="#supported-features-3">Supported features</a></h2>
<pre><code class="language-txt">file features:
- cp FROM:
  --format=$FORMAT
- cp TO:
  --format=$FORMAT
  --if-exists=error --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="google-cloud-storage"><a class="header" href="#google-cloud-storage">Google Cloud Storage</a></h1>
<p>Google Cloud Storage is a bucket-based storage system similar to Amazon's S3. It's frequently used in connection with BigQuery and other Google Cloud services.</p>
<h2 id="example-locators-4"><a class="header" href="#example-locators-4">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>gs://bucket/dir/file.csv</code></li>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<h2 id="configuration--authentication-4"><a class="header" href="#configuration--authentication-4">Configuration &amp; authentication</a></h2>
<p>You can authenticate using either a client secret or a service key, which you can create using the <a href="https://console.cloud.google.com/apis/credentials">console credentials page</a>.</p>
<ul>
<li>Client secrets can be stored in <code>$DBCROSSBAR_CONFIG_DIR/gcloud_client_secret.json</code> or in <code>GCLOUD_CLIENT_SECRET</code>. These are strongly recommended for interactive use.</li>
<li>Service account keys can be stored in <code>$DBCROSSBAR_CONFIG_DIR/gcloud_service_account_key.json</code> or in <code>GCLOUD_SERVICE_ACCOUNT_KEY</code>. These are recommended for server and container use.</li>
</ul>
<p>For more information on <code>DBCROSSBAR_CONFIG_DIR</code>, see <a href="./config.html">Configuration</a>.</p>
<p>For a service account, you can use the following permissions:</p>
<ul>
<li>Storage Object Admin (Cloud Storage and BigQuery drivers)</li>
<li>BigQuery Data Editor (BigQuery driver only)</li>
<li>BigQuery Job User (BigQuery driver only)</li>
<li>BigQuery User (BigQuery driver only)</li>
</ul>
<p>There's probably a more limited set of permissions which will work if you set them up manually.</p>
<h2 id="supported-features-4"><a class="header" href="#supported-features-4">Supported features</a></h2>
<pre><code class="language-txt">gs features:
- cp FROM:
- cp TO:
  --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="postgresql"><a class="header" href="#postgresql">PostgreSQL</a></h1>
<p><a href="https://www.postgresql.org/">PostgreSQL</a> is an excellent general-purpose SQL database.</p>
<h2 id="example-locators-5"><a class="header" href="#example-locators-5">Example locators</a></h2>
<p><code>dbcrossbar</code> supports standard PostgreSQL locators followed by <code>#table_name</code>:</p>
<ul>
<li><code>postgres://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<p>Note that PostgreSQL sources will currently output all data as a single stream. This can be split into multiple streams using the <code>--stream-size</code> option if desired.</p>
<h2 id="configuration--authentication-5"><a class="header" href="#configuration--authentication-5">Configuration &amp; authentication</a></h2>
<p>Authentication is currently handled using standard <code>postgres://user:pass@...</code> syntax, similar to <code>psql</code>. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<h2 id="supported-features-5"><a class="header" href="#supported-features-5">Supported features</a></h2>
<pre><code class="language-txt">postgres features:
- conv FROM
- count
  --where=$SQL_EXPR
- cp FROM:
  --where=$SQL_EXPR
- cp TO:
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redshift"><a class="header" href="#redshift">RedShift</a></h1>
<p>Amazon's <a href="https://aws.amazon.com/redshift/">Redshift</a> is a cloud-based data warehouse designed to support analytical queries. This driver receives less testing than our BigQuery driver, because the cheapest possible RedShift test system costs over $100/month. Sponsors are welcome!</p>
<h2 id="example-locators-6"><a class="header" href="#example-locators-6">Example locators</a></h2>
<p>These are identical to <a href="./postgres.html#example-locators">PostgreSQL locators</a>, except that <code>postgres</code> is replaced by <code>redshift</code>:</p>
<ul>
<li><code>redshift://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<h2 id="configuration--authentication-6"><a class="header" href="#configuration--authentication-6">Configuration &amp; authentication</a></h2>
<p>Authentication is currently handled using the <code>redshift://user:pass@...</code> syntax. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<p>The following environment variables are required.</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): This should work, but it hasn't been tested.</li>
</ul>
<p>The following <code>--temporary</code> flag is required:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading or unloading data.</li>
</ul>
<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/loading-data-access-permissions.html">Authentication credentials for <code>COPY</code></a> may be passed using <code>--to-arg</code>. For example:</p>
<ul>
<li><code>--to-arg=iam_role=$ROLE</code></li>
<li><code>--to-arg=region=$REGION</code></li>
</ul>
<p>This may require some experimentation.</p>
<p>If you need to generate &quot;-- partner:&quot; SQL comments for an AWS RedShift partner program, you can do it as follows:</p>
<ul>
<li><code>--to-arg=partner=&quot;myapp v1.0&quot;</code></li>
</ul>
<h2 id="supported-features-6"><a class="header" href="#supported-features-6">Supported features</a></h2>
<pre><code class="language-txt">redshift features:
- conv FROM
- cp FROM:
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="s3"><a class="header" href="#s3">S3</a></h1>
<p>Amazon's <a href="https://aws.amazon.com/s3/">S3</a> is a bucket-based system for storing data in the cloud.</p>
<h2 id="example-locators-7"><a class="header" href="#example-locators-7">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>s3://bucket/dir/file.csv</code></li>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<h2 id="configuration--authentication-7"><a class="header" href="#configuration--authentication-7">Configuration &amp; authentication</a></h2>
<p>The following environment variables are used to authenticate:</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> (required): The ID for your AWS credentials.</li>
<li><code>AWS_SECRET_ACCESS_KEY</code> (required): The secret part of your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): Set this to use temporary AWS crdentials.</li>
<li><code>AWS_DEFAULT_REGION</code> (required): Set this to your AWS region.</li>
</ul>
<h2 id="supported-features-7"><a class="header" href="#supported-features-7">Supported features</a></h2>
<pre><code class="language-txt">s3 features:
- cp FROM:
- cp TO:
  --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shopify-unstable"><a class="header" href="#shopify-unstable">Shopify (UNSTABLE)</a></h1>
<p><strong>WARNING:</strong> This is highly experimental and will likely be removed in a future release in favor of the new JSON Lines support. To use it for now, you must enable it using the <code>--enable-unstable</code> flag.</p>
<p>Shopify is an online e-commerce platform with a REST API for fetching data.</p>
<h2 id="example-locators-8"><a class="header" href="#example-locators-8">Example locators</a></h2>
<p>Locators look just like Shopify REST API URLs, but with <code>https:</code> replaced with <code>shopify</code>:</p>
<ul>
<li><code>shopify://$SHOP/admin/api/2020-04/orders.json?status=any</code></li>
</ul>
<p>For a schema, download <a href="https://github.com/dbcrossbar/dbcrossbar/blob/master/dbcrossbar/fixtures/shopify.ts">shopify.ts</a>, and refer to it as follows:</p>
<ul>
<li><code>--schema=&quot;dbcrossbar-ts:shopify.ts#Order&quot;</code></li>
</ul>
<p>We do not currently include a default Shopify schema in <code>dbcrossbar</code> itself, because it's still undergoing significant changes.</p>
<h2 id="configuration--authentication-8"><a class="header" href="#configuration--authentication-8">Configuration &amp; authentication</a></h2>
<p>The following environment variables are required:</p>
<ul>
<li><code>SHOPIFY_AUTH_TOKEN</code>: The Shopify authorization token to use. (We don't yet support password authentication, but it would be easy enough to add.)</li>
</ul>
<h2 id="supported-features-8"><a class="header" href="#supported-features-8">Supported features</a></h2>
<pre><code class="language-txt">shopify features:
- cp FROM:

This driver is UNSTABLE and may change without warning.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-drivers"><a class="header" href="#schema-drivers">Schema drivers</a></h1>
<p><code>dbcrossbar</code> allows you to specify a table's column names and types in a number of different ways. You can use <a href="./postgres-sql.html">Postgres <code>CREATE TABLE</code> statements</a>, or <a href="./bigquery-schema.html">BigQuery schema JSON</a>, or <a href="./dbcrossbar-schema.html"><code>dbcrossbar</code>'s internal schema format</a>.</p>
<p>These schema formats are typically used in one of two ways:</p>
<ul>
<li>
<p>As a <code>--schema</code> argument to the <a href="./cp.html"><code>cp</code> subcommand</a>.</p>
<pre><code class="language-sh">dbcrossbar cp \
  --if-exists=overwrite \
  --schema=postgres-sql:my_table.sql \
  csv:my_table.csv \
  'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
</li>
<li>
<p>As an argument to the <a href="./conv.html"><code>conv</code> subcommand</a>, which allows you to convert between different schema formats.</p>
<pre><code class="language-sh">dbcrossbar schema conv postgres-sql:table.sql bigquery-schema:table.json
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="postgres-create-table-statements"><a class="header" href="#postgres-create-table-statements">Postgres <code>CREATE TABLE</code> statements</a></h1>
<p>To specify the column names and types for table in SQL format, use:</p>
<pre><code class="language-txt">--schema postgres-sql:my_table.sql
</code></pre>
<p>The file <code>my_table.sql</code> can contain a single <code>CREATE TABLE</code> statement using a subset of PostgreSQL's syntax:</p>
<pre><code class="language-sql">CREATE TABLE my_table (
    id INT NOT NULL,
    name TEXT NOT NULL,
    quantity INT NOT NULL
);
</code></pre>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<p>This schema format offers support for singly-nested array types, and it doesn't support structure types at all.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigquery-json-schemas"><a class="header" href="#bigquery-json-schemas">BigQuery JSON schemas</a></h1>
<p>To specify the column names and types for table in BigQuery JSON format, use:</p>
<pre><code class="language-txt">--schema bigquery-schema:my_table.json
</code></pre>
<p>The file <code>my_table.json</code> should be a <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery JSON schema file</a>:</p>
<pre><code class="language-json">[
  {
    &quot;name&quot;: &quot;id&quot;,
    &quot;type&quot;: &quot;INT64&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  },
  {
    &quot;name&quot;: &quot;name&quot;,
    &quot;type&quot;: &quot;STRING&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  },
  {
    &quot;name&quot;: &quot;quantity&quot;,
    &quot;type&quot;: &quot;INT64&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  }
]
</code></pre>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<p>This schema format supports a small number of general types. For example, all integer types are represented as <code>INT64</code>, all floating-point types are represented as <code>FLOAT64</code>, and both JSON values and UUIDs are represented as <code>STRING</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="native-dbcrossbar-schemas"><a class="header" href="#native-dbcrossbar-schemas">Native <code>dbcrossbar</code> schemas</a></h1>
<p><code>dbcrossbar</code> supports a <a href="./schema.html">native schema format</a> that exactly represents all types supported by <code>dbcrossbar</code>. It can be used as follows:</p>
<pre><code class="language-txt">--schema dbcrossbar-schema:my_table.json
</code></pre>
<p>For more details and example, see the chaper on <a href="./schema.html">portable table schemas</a>.</p>
<h2 id="typical-uses"><a class="header" href="#typical-uses">Typical uses</a></h2>
<p>This format is cumbersome to edit by hand, but it is fairly useful in a number of circumstances:</p>
<ul>
<li>Specifying column types that can't be exactly represented by other schema formats.</li>
<li>Reading or editing schemas using scripts.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="typescript-schemas-unstable"><a class="header" href="#typescript-schemas-unstable">TypeScript schemas (UNSTABLE)</a></h1>
<p><strong>WARNING:</strong> This is highly experimental and subject to change. To use it, you must enable it using the <code>--enable-unstable</code> flag.</p>
<p>To specify the column names and types for table using a subset of TypeScript, use:</p>
<pre><code class="language-txt">--schema &quot;dbcrossbar-ts:my_table.ts#MyTable&quot;
</code></pre>
<p>The file <code>my_table.ts</code> can contain one or more <code>interface</code> definitions:</p>
<pre><code class="language-ts">interface MyTable {
    id: string,
    name: string,
    quantity: number,
}
</code></pre>
<h2 id="magic-types"><a class="header" href="#magic-types">&quot;Magic&quot; types</a></h2>
<p>Certain <code>dbcrossbar</code> types can be specified by adding the following declarations to a TypeScript file:</p>
<pre><code class="language-ts">// Decimal numbers which can exactly represent
// currency values with no rounding.
type decimal = number | string;

// Integers of various sizes.
type int16 = number | string;
type int32 = number | string;
type int64 = number | string;
</code></pre>
<p>These may then be used as follows:</p>
<pre><code class="language-ts">interface OrderItem {
    id: int64,
    sku: string,
    unit_price: decimal,
    quantity: int16,
}
</code></pre>
<p>When the TypeScript schema is converted to a portable <code>dbcrossbar</code> schema, the &quot;magic&quot; types will be replaced with the corresponding portable type.</p>
<h3 id="advanced-features"><a class="header" href="#advanced-features">Advanced features</a></h3>
<p>We also support nullable values, arrays and nested structures:</p>
<pre><code class="language-ts">type decimal = number | string;
type int16 = number | string;
type int32 = number | string;
type int64 = number | string;

interface Order {
    id: int64,
    line_items: OrderItem[],
    note: string | null,
}

interface OrderItem {
    id: int64,
    sku: string,
    unit_price: decimal,
    quantity: int16,
}
</code></pre>
<p>Nested arrays and structs will translate to appropriate database-specific types, such as BigQuery <code>ARRAY</code> and <code>STRUCT</code> types.</p>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>This schema format has a number of limitations:</p>
<ul>
<li>There's no way to convert other schema formats into this one (yet).</li>
<li>Some portable <code>dbcrossbar</code> types can't be represented in this format.</li>
<li>Only a small subset of TypeScript is supported (but we try to give good error messages).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="changelog"><a class="header" href="#changelog">Changelog</a></h1>
<p>All notable changes to this project will be documented in this file.</p>
<p>The format is based on <a href="https://keepachangelog.com/en/1.0.0/">Keep a Changelog</a>, and this project adheres to <a href="https://semver.org/spec/v2.0.0.html">Semantic Versioning</a>.</p>
<h2 id="unreleased"><a class="header" href="#unreleased">[Unreleased]</a></h2>
<h2 id="058-pre2---2024-10-29"><a class="header" href="#058-pre2---2024-10-29">[0.5.8-pre.2] - 2024-10-29</a></h2>
<h3 id="fixed"><a class="header" href="#fixed">Fixed</a></h3>
<ul>
<li><code>bigquery-test-fixture:</code>: Retry 503 errors affecting CREATE VIEW calls.</li>
</ul>
<h2 id="058-pre1---2024-10-22"><a class="header" href="#058-pre1---2024-10-22">[0.5.8-pre.1] - 2024-10-22</a></h2>
<h3 id="added"><a class="header" href="#added">Added</a></h3>
<ul>
<li>Honor <code>AWS_ENDPOINT</code> even if <code>awscli</code> does not support it. This is needed for use with some S3 replacements.</li>
</ul>
<h3 id="changed"><a class="header" href="#changed">Changed</a></h3>
<ul>
<li>redshift: Text columns are now represented as <code>VARCHAR(MAX)</code>, which has unlimited size, instead of <code>TEXT</code>, which is apparently limited to 255 bytes. This <em>may</em> be a breaking change for some use cases, but RedShift is our least supported driver and we have minimal test infrastructure.</li>
</ul>
<h3 id="security"><a class="header" href="#security">Security</a></h3>
<ul>
<li>Library updates for several advisories.</li>
</ul>
<h2 id="057---2024-01-26"><a class="header" href="#057---2024-01-26">[0.5.7] - 2024-01-26</a></h2>
<h3 id="added-1"><a class="header" href="#added-1">Added</a></h3>
<ul>
<li>gcloud: Allow passing <code>extra_scopes</code> to Google OAuth2.</li>
</ul>
<h3 id="security-1"><a class="header" href="#security-1">Security</a></h3>
<ul>
<li>Fix <a href="https://rustsec.org/advisories/RUSTSEC-2024-0003">RUSTSEC-2024-0003</a>, a Denial of Service vulnerability which could apparently cause a hostile HTTP server to cause out-of-memory errors
in a client. Since <code>dbcrossbar</code> is unlikely to connect to many hostile HTTP servers, this is probably a low-priority fix for almost all users.</li>
</ul>
<h2 id="056---2024-01-08"><a class="header" href="#056---2024-01-08">[0.5.6] - 2024-01-08</a></h2>
<h3 id="added-2"><a class="header" href="#added-2">Added</a></h3>
<ul>
<li>bigquery: Users can now specify an alternate job project, for example for billing purposes. See docs for details.</li>
</ul>
<h2 id="055---2023-09-14"><a class="header" href="#055---2023-09-14">[0.5.5] - 2023-09-14</a></h2>
<h3 id="added-3"><a class="header" href="#added-3">Added</a></h3>
<ul>
<li>Update to latest <code>opinionated_telemetry</code>. This should not affect users.</li>
</ul>
<h2 id="054---2023-09-08"><a class="header" href="#054---2023-09-08">[0.5.4] - 2023-09-08</a></h2>
<h3 id="added-4"><a class="header" href="#added-4">Added</a></h3>
<ul>
<li>Output in JSON Lines format.</li>
<li>Experimental new support for Prometheus and Google Cloud Trace via <a href="https://github.com/dbcrossbar/dbcrossbar/blob/main/crates/opinionated_telemetry/README.md"><code>opinionated_telemetry</code></a>.</li>
</ul>
<h3 id="removed"><a class="header" href="#removed">Removed</a></h3>
<ul>
<li><code>dbcrossbarlib</code> is not longer provided separately. As before, we recommend calling out to <code>dbcrossbar</code> using the CLI, rather than trying to link against its relatively complex dependencies.</li>
</ul>
<h2 id="053---2023-08-01"><a class="header" href="#053---2023-08-01">[0.5.3] - 2023-08-01</a></h2>
<h3 id="changed-1"><a class="header" href="#changed-1">Changed</a></h3>
<ul>
<li>The <code>csv:</code> driver has been superceded by a more generic <code>file:</code> driver. The old CSV driver <strong>will continue to work, perhaps indefinitely,</strong> but it may be increasingly hidden. By default, the <code>file:</code> driver determines the file format using the <code>--from-format</code> command line argument, or the file extension, falling back to CSV format as a last resort. I <em>think</em> this should be fully backwards-compatible. But if you encounter breaking changes, try adding <code>--from-format=csv</code> to your command line.</li>
</ul>
<h3 id="fixed-1"><a class="header" href="#fixed-1">Fixed</a></h3>
<ul>
<li>Autogenerated <code>--help</code> summaries in the docs are now up to date.</li>
<li>Fixed spelling of <code>geo_json</code> in help. Thanks, Tom!</li>
<li>Fixed handling of empty arrays in <code>bigquery-test-fixture:</code>. Thanks, Dave!</li>
</ul>
<h2 id="052---2022-11-03"><a class="header" href="#052---2022-11-03">[0.5.2] - 2022-11-03</a></h2>
<h3 id="fixed-2"><a class="header" href="#fixed-2">Fixed</a></h3>
<ul>
<li>Fix CI build and binary releases broken by new warnings in latest Rust update.</li>
</ul>
<h2 id="051---2022-11-03"><a class="header" href="#051---2022-11-03">[0.5.1] - 2022-11-03</a></h2>
<h3 id="fixed-3"><a class="header" href="#fixed-3">Fixed</a></h3>
<ul>
<li>bigquery-test-fixture: Add support for single-column inputs. Many thanks to Dave Shirley!</li>
<li>bigquery-test-fixture: Allow larger outputs before switching to fallback mode. Thanks again to Dave Shirley!</li>
</ul>
<h2 id="050---2022-10-22"><a class="header" href="#050---2022-10-22">[0.5.0] - 2022-10-22</a></h2>
<h3 id="added-5"><a class="header" href="#added-5">Added</a></h3>
<ul>
<li>New <code>bigquery-test-fixture:</code> schema creates small, read-only &quot;tables&quot; very quickly. Many thanks to Dave Shirley for the proof of concept and for pairing on the final implementation.</li>
</ul>
<h3 id="fixed-4"><a class="header" href="#fixed-4">Fixed</a></h3>
<ul>
<li>Updated a few dependencies with (likely non-exploitable) security advisories. We have not update <em>all</em> our dependencies, because there are at least two regressions affecting supposedly semver-compatible dependencies. This is actually very rare in the Rust ecosystem, but we were unlucky this time.</li>
</ul>
<h2 id="050-beta4---2022-01-01"><a class="header" href="#050-beta4---2022-01-01">[0.5.0-beta.4] - 2022-01-01</a></h2>
<h3 id="fixed-5"><a class="header" href="#fixed-5">Fixed</a></h3>
<ul>
<li>gs: Retry <code>delete</code> operations after 403 Forbidden errors. This appears to be part of the same hypothesized permissions race that affected <code>extract</code> in v0.5.0-beta.3.</li>
</ul>
<h2 id="050-beta3---2021-12-31"><a class="header" href="#050-beta3---2021-12-31">[0.5.0-beta.3] - 2021-12-31</a></h2>
<h3 id="fixed-6"><a class="header" href="#fixed-6">Fixed</a></h3>
<ul>
<li>bigquery: Retry <code>extract</code> operations after <code>accessDenied</code> errors. These errors sometimes occur even when BigQuery <em>should</em> have the necessary access permissions. This may be caused by a permissions race condition somewhere in Google Cloud?</li>
</ul>
<h2 id="050-beta2---2021-12-19"><a class="header" href="#050-beta2---2021-12-19">[0.5.0-beta.2] - 2021-12-19</a></h2>
<h3 id="changed-2"><a class="header" href="#changed-2">Changed</a></h3>
<ul>
<li>The format for printing backtraces has changed. We now use the standard <code>anyhow</code> format.</li>
<li>Our logging format has changed considerably, thanks to switch from <code>slog</code> to the much more popular <code>tracing</code>. Some logging of complex async streams may be less well-organized, but there's <em>more</em> of it, especially at <code>RUST_LOG=dbcrossbarlib=trace,dbcrossbar=trace,warn</code> level.</li>
</ul>
<h3 id="fixed-7"><a class="header" href="#fixed-7">Fixed</a></h3>
<ul>
<li>We no longer rely on the unmaintained <code>failure</code>, or the less popular <code>slog</code> ecosystem.</li>
</ul>
<h3 id="removed-1"><a class="header" href="#removed-1">Removed</a></h3>
<ul>
<li>The <code>--log-format</code> and <code>--log-extra</code> commands have been removed.</li>
</ul>
<h2 id="050-beta1---2021-12-15"><a class="header" href="#050-beta1---2021-12-15">[0.5.0-beta.1] - 2021-12-15</a></h2>
<h3 id="added-6"><a class="header" href="#added-6">Added</a></h3>
<ul>
<li>We provide MacOS X binaries for the new M1 processors. These are unsigned, like our existing Apple Intel binaries. So you'll need to continue to use <code>xattr -d com.apple.quarantine dbcrossbar</code> or a similar technique to run them. Or you could build your own binaries.</li>
</ul>
<h3 id="changed-3"><a class="header" href="#changed-3">Changed</a></h3>
<ul>
<li>Our downloadable <code>*.zip</code> files follow a new naming convention, allowing us to distinguish between Intel and M1 Macs.</li>
<li>OpenSSL has been completely removed from <code>dbcrossbar</code>, allowing us to support more platforms. This will also allow us to eventually centralize TLS configuration across all <code>dbcrossbar</code> drivers.</li>
</ul>
<h3 id="removed-2"><a class="header" href="#removed-2">Removed</a></h3>
<ul>
<li>We no longer support hosted Citus from Citus Data, because their TLS certificates do not include <code>subjectAltName</code>, which is <a href="https://github.com/briansmith/webpki/issues/11">required by the <code>rustls</code> library</a>. Citus Data will be shutting down shortly, so we recommend keeping around an older <code>dbcrossbar</code> for a few more weeks if you need to talk to them.</li>
</ul>
<h2 id="050-alpha3---2021-12-14"><a class="header" href="#050-alpha3---2021-12-14">[0.5.0-alpha.3] - 2021-12-14</a></h2>
<h3 id="added-7"><a class="header" href="#added-7">Added</a></h3>
<ul>
<li>gs: Allow single-file CSV output. This involves copying out of Google Cloud Storage, concatenating, and copying back. But it's handy when you need it.</li>
</ul>
<h3 id="changed-4"><a class="header" href="#changed-4">Changed</a></h3>
<ul>
<li>gs: Copying to a <code>gs://bucket/dir/</code> URL with <code>--if-exists=overwrite --display-output-locators</code> will print out a list of files in the destination bucket directory when we're done. Before, it just printed out the destination locator, which was technically allowed, but useless.</li>
</ul>
<h3 id="fixed-8"><a class="header" href="#fixed-8">Fixed</a></h3>
<ul>
<li>Updated many dependencies, fixing several CVEs (none known to be meaningfully exploitable in typical use cases), and possibly some library bugs.</li>
</ul>
<h2 id="050-alpha2---2021-04-27"><a class="header" href="#050-alpha2---2021-04-27">[0.5.0-alpha.2] - 2021-04-27</a></h2>
<h3 id="fixed-9"><a class="header" href="#fixed-9">Fixed</a></h3>
<ul>
<li>bigml: Always map columns with a <code>one-of</code> type (aka <code>CREATE ENUM</code>) to BigML <code>categorical</code> columns.</li>
</ul>
<h2 id="050-alpha1---2021-03-04"><a class="header" href="#050-alpha1---2021-03-04">[0.5.0-alpha.1] - 2021-03-04</a></h2>
<p>This release contains a breaking change to the <code>dbcrossbar-schema</code> output format to enable supporting named types and enumeration types. See below.</p>
<h3 id="added-8"><a class="header" href="#added-8">Added</a></h3>
<ul>
<li>
<p>(EXPERIMENTAL) postgres: The <code>postgres-sql</code> and <code>postgres</code> drivers now <code>CREATE TYPE</code> statements (but only in the <code>&quot;public&quot;</code> schema). These can be used as follows:</p>
<pre><code class="language-sql">CREATE TYPE &quot;format&quot; AS ENUM ('gif', 'jpeg');
CREATE TABLE &quot;images&quot; (
    &quot;id&quot; uuid NOT NULL,
    &quot;url&quot; text NOT NULL,
    &quot;image_format&quot; &quot;format&quot;,
    &quot;metadata&quot; jsonb
);
</code></pre>
<p>This change also requires some changes to the <code>dbcrossbar-schema</code> format, which are described below.</p>
</li>
<li>
<p>(EXPERIMENTAL) The native <code>dbcrossbar-schema</code> format now supports a set of <code>named_types</code> definitions. This allows named types to be defined once, and to then be referred to elsewhere using <code>{ &quot;named&quot;: &quot;my_custom_type&quot; }</code>.</p>
</li>
<li>
<p>(EXPERIMENTAL) The native <code>dbcrossbar-schema</code> format also supports string enumeration types using a <code>{ &quot;one_of&quot;: [&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;] }</code> syntax.</p>
</li>
</ul>
<h3 id="changed-5"><a class="header" href="#changed-5">Changed</a></h3>
<ul>
<li>BREAKING: The <code>dbcrossbar-schema</code> output format has changed! It now has top level <code>named_types</code> and <code>tables</code> members, and the old top-level table definition is now available as <code>.tables[0]</code>. See <a href="https://www.dbcrossbar.org/schema.html">the manual</a> for more details. However, <code>dbcrossbar</code> can still read the old input format with no problems, so this only affects other programs that parse native <code>dbcrossbar</code> schema.</li>
</ul>
<h3 id="fixed-10"><a class="header" href="#fixed-10">Fixed</a></h3>
<ul>
<li>The suggested fixes for RUSTSEC-2020-0146, RUSTSEC-2021-0020 and RUSTSEC-2021-0023 have been applied.</li>
</ul>
<h2 id="042-beta11---2021-02-03"><a class="header" href="#042-beta11---2021-02-03">0.4.2-beta.11 - 2021-02-03</a></h2>
<h3 id="fixed-11"><a class="header" href="#fixed-11">Fixed</a></h3>
<ul>
<li>gcloud: Retry OAuth2 failures for service accounts.</li>
</ul>
<h2 id="042-beta10---2021-02-02"><a class="header" href="#042-beta10---2021-02-02">0.4.2-beta.10 - 2021-02-02</a></h2>
<h3 id="fixed-12"><a class="header" href="#fixed-12">Fixed</a></h3>
<ul>
<li>Build fixes for recently-added clippy warnings.</li>
</ul>
<h2 id="042-beta9---2021-01-14"><a class="header" href="#042-beta9---2021-01-14">0.4.2-beta.9 - 2021-01-14</a></h2>
<h3 id="changed-6"><a class="header" href="#changed-6">Changed</a></h3>
<ul>
<li>Update many dependencies, including <code>tokio</code> and our many network-related libraries. Tests pass, but this affects almost everything, in one fashion or another.</li>
</ul>
<h2 id="042-beta8---2020-10-16"><a class="header" href="#042-beta8---2020-10-16">0.4.2-beta.8 - 2020-10-16</a></h2>
<h3 id="fixed-13"><a class="header" href="#fixed-13">Fixed</a></h3>
<ul>
<li>Linux: Fix Linux binary builds by updating to latest <code>rust-musl-builder</code> release, which has the new <code>cargo-deny</code>.</li>
</ul>
<h2 id="042-beta7---2020-10-14"><a class="header" href="#042-beta7---2020-10-14">0.4.2-beta.7 - 2020-10-14</a></h2>
<h3 id="added-9"><a class="header" href="#added-9">Added</a></h3>
<ul>
<li>shopify: Added a &quot;partner&quot; argument which can be used to include a &quot;-- partner:&quot; comment in all generated RedShift SQL for use by RedShift partners.</li>
</ul>
<h2 id="042-beta6---2020-09-15"><a class="header" href="#042-beta6---2020-09-15">0.4.2-beta.6 - 2020-09-15</a></h2>
<h3 id="fixed-14"><a class="header" href="#fixed-14">Fixed</a></h3>
<ul>
<li>shopify: Retry failed downloads a few times. We've been seeing some intermittent failures.</li>
</ul>
<h2 id="042-beta5---2020-08-01"><a class="header" href="#042-beta5---2020-08-01">0.4.2-beta.5 - 2020-08-01</a></h2>
<h3 id="fixed-15"><a class="header" href="#fixed-15">Fixed</a></h3>
<ul>
<li>gcloud: We now print more useful error messages when Google doesn't send JSON-formatted errors.</li>
<li>gcloud: We now retry Google Cloud GET requests automatically a few times if it looks like it might help. We'd also love to retry POST requests, but that will require the ability to try to restart streams.</li>
</ul>
<h2 id="042-beta4---2020-07-07"><a class="header" href="#042-beta4---2020-07-07">0.4.2-beta.4 - 2020-07-07</a></h2>
<h3 id="changed-7"><a class="header" href="#changed-7">Changed</a></h3>
<ul>
<li>Update dependencies. The latest <code>bigml</code> release contains tweaks to error retry behavior.</li>
</ul>
<h2 id="042-beta3---2020-07-07"><a class="header" href="#042-beta3---2020-07-07">0.4.2-beta.3 - 2020-07-07</a></h2>
<h3 id="changed-8"><a class="header" href="#changed-8">Changed</a></h3>
<ul>
<li>postgres: Our last <code>diesel</code> code has been removed, and replaced with <code>tokio-postgres</code> (which we use elsewhere).</li>
</ul>
<h3 id="fixed-16"><a class="header" href="#fixed-16">Fixed</a></h3>
<ul>
<li>postgres: Fixed <a href="https://github.com/dbcrossbar/dbcrossbar/issues/148">#148</a> to improve support for PostGIS under PostgreSQL 12.</li>
</ul>
<h3 id="removed-3"><a class="header" href="#removed-3">Removed</a></h3>
<ul>
<li>The experimental <code>citus</code>-related APIs have been removed from <code>dbcrossbarlib</code>, because they used <code>diesel</code>. This is technically a breaking change for <code>dbcrosslib</code>, but we don't claim to honor semver for <code>dbcrossbarlib</code> 0.x.y releases.</li>
</ul>
<h2 id="042-beta2---2020-06-28"><a class="header" href="#042-beta2---2020-06-28">0.4.2-beta.2 - 2020-06-28</a></h2>
<h3 id="added-10"><a class="header" href="#added-10">Added</a></h3>
<ul>
<li>redshift: Support <code>--if-exists=upsert-on:key1,key2</code>.</li>
<li>redshift: Enable <code>--if-exists=error</code>.</li>
</ul>
<h3 id="changed-9"><a class="header" href="#changed-9">Changed</a></h3>
<ul>
<li>postgres: Temporary tables now use the same schema (i.e. namespace) as the tables they're linked to. This shouldn't be a breaking change unless you've set up your database permissions to forbid it.</li>
</ul>
<h3 id="fixed-17"><a class="header" href="#fixed-17">Fixed</a></h3>
<ul>
<li>postgres: Fixed likely bug upserting into tables with a non-&quot;public&quot; schema.</li>
<li>postgres: Verify that upsert columns are NOT NULL to prevent possible incorrect upserts. This may be a breaking change, but it also prevents a possible bug.</li>
</ul>
<h2 id="042-beta1---2020-06-23"><a class="header" href="#042-beta1---2020-06-23">0.4.2-beta.1 - 2020-06-23</a></h2>
<h3 id="changed-10"><a class="header" href="#changed-10">Changed</a></h3>
<ul>
<li>Mac: Move configuration directory from <code>~/Library/Preferences/dbcrossbar</code> to <code>~/Library/Application Support/dbcrossbar</code>. If we detect a config directory in the old location, we should print a deprecation warning and use it.</li>
<li>Many dependencies have been updated.</li>
</ul>
<h3 id="fixed-18"><a class="header" href="#fixed-18">Fixed</a></h3>
<ul>
<li>We should now handle multiple sets of Google Cloud OAuth2 credentials correctly.</li>
</ul>
<h2 id="041---2020-06-16"><a class="header" href="#041---2020-06-16">0.4.1 - 2020-06-16</a></h2>
<p>A bug fix to <code>gs</code>, and other minor improvements.</p>
<h3 id="changed-11"><a class="header" href="#changed-11">Changed</a></h3>
<ul>
<li>Replace deprecated <code>tempdir</code> with <code>tempfile</code>.</li>
</ul>
<h3 id="fixed-19"><a class="header" href="#fixed-19">Fixed</a></h3>
<ul>
<li>gs: Correctly pass <code>page_token</code> when listing. This prevents an infinite loop in large directories.</li>
<li>Fix new Rust 0.44.0 warnings.</li>
</ul>
<h2 id="040---2020-06-02"><a class="header" href="#040---2020-06-02">0.4.0 - 2020-06-02</a></h2>
<p>This is a summary of all the major changes since the 0.3.3 release. For more details and minor changes, see the individual CHANGELOG entries for the 0.4.0 preleases.</p>
<h3 id="added-11"><a class="header" href="#added-11">Added</a></h3>
<ul>
<li><code>dbcrossbar</code> now supports &quot;struct&quot; types, which have a fixed set of named fields. These will be automatically translated to BigQuery STRUCT types or to JSON columns, depending on the destination database.</li>
<li>We now support a CLI-editable config file using commands like <code>dbcrossbar config add temporary s3://example/temp/</code>.</li>
<li>Parsing-related error messages should now include context.</li>
<li>bigquery: Users can now specify billing labels for jobs.</li>
<li><code>dbcrossbar license</code> will display the licences for all dependencies.</li>
<li>Unstable features can now be hidden behind the <code>--enable-unstable</code> flag, including two new drivers:
<ul>
<li>UNSTABLE: We now support specifying schemas using a subset of TypeScript.</li>
<li>UNSTABLE: We now support reading data from Shopify's REST API. This is a testbed for new struct and JSON-related features.</li>
</ul>
</li>
</ul>
<h3 id="changed-12"><a class="header" href="#changed-12">Changed</a></h3>
<ul>
<li><code>dbcrossbar conv</code> is now <code>dbcrossbar schema conv</code>.</li>
<li>Because of the new STRUCT support, some corner cases involving struct types and JSON may have changed subtly.</li>
<li>We replaced <code>gcloud auth</code>, <code>gsutil</code> and <code>bq</code> with native Rust. This simplifies installation and configuration substantially, and fixes a number of BigQuery-related issues.</li>
<li>AWS credentials must now always be passed via <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code> (optional) and <code>AWS_DEFAULT_REGION</code> (required). This lays the groundwork for replacing the <code>aws</code> CLI tool with native Rust code, so that we will someday be able to remove our last CLI dependency.</li>
</ul>
<h3 id="fixed-20"><a class="header" href="#fixed-20">Fixed</a></h3>
<ul>
<li>Lots of issues.</li>
</ul>
<h3 id="removed-4"><a class="header" href="#removed-4">Removed</a></h3>
<ul>
<li>The data type <code>{ &quot;other&quot;: string }</code> has been removed from the portable schema format. It was not actually generated by any of our drivers.</li>
<li>bigquery: We now export <code>ARRAY&lt;STRUCT&lt;...&gt;&gt;</code> as <code>{ &quot;array&quot;: { &quot;struct&quot;: ... } }</code>, instead of exporting it as as single <code>&quot;json&quot;</code> value.</li>
</ul>
<h2 id="040-rc2---2020-06-01"><a class="header" href="#040-rc2---2020-06-01">0.4.0-rc.2 - 2020-06-01</a></h2>
<h3 id="changed-13"><a class="header" href="#changed-13">Changed</a></h3>
<ul>
<li>postgres: We now transform the portable types <code>{ &quot;array&quot;: &quot;json&quot; }</code> and <code>{ &quot;array&quot;: { &quot;struct&quot;: fields } }</code> into <code>jsonb[]</code>, instead of automatically changing it to plain <code>jsonb</code> in an effort to help our users.</li>
</ul>
<h2 id="040-rc1---2020-05-31"><a class="header" href="#040-rc1---2020-05-31">0.4.0-rc.1 - 2020-05-31</a></h2>
<p>This is a release candidate for v0.4.0. If no issues are discovered, this will be published as 0.4.0.</p>
<p>This release contains a last few breaking changes that we want to include before we publicize <code>dbcrossbar</code> more widely. When migrating, particular attention to the <code>conv</code> subcommand and <code>AWS_DEFAULT_REGION</code> below, which have significant breaking changes.</p>
<h3 id="changed-14"><a class="header" href="#changed-14">Changed</a></h3>
<ul>
<li>Rename <code>dbcrossbar conv</code> to <code>dbcrossbar schema conv</code>.</li>
<li>s3: Require <code>AWS_DEFAULT_REGION</code> instead of optionally using <code>AWS_REGION</code>. This is more compatiable with the <code>aws</code> CLI command, and it doesn't rely on undocumented region defaults or <code>aws</code> configuration files.</li>
</ul>
<h3 id="added-12"><a class="header" href="#added-12">Added</a></h3>
<ul>
<li>Document our portable schema format.</li>
<li>Document schema-only drivers.</li>
<li>Improve the documentation in other minor ways.</li>
</ul>
<h3 id="removed-5"><a class="header" href="#removed-5">Removed</a></h3>
<ul>
<li>Remove <code>DataType::Other(String)</code>, which was not actually used by any of our drivers.</li>
</ul>
<h2 id="040-beta1---2020-05-28"><a class="header" href="#040-beta1---2020-05-28">0.4.0-beta.1 - 2020-05-28</a></h2>
<p>We're finally ready to start preparing for an 0.4.0 release! This beta will be deployed to several production systems to help verify that there are no surprising regressions.</p>
<h3 id="changed-15"><a class="header" href="#changed-15">Changed</a></h3>
<ul>
<li>gs: We now verify CRC32C checksums when uploading.</li>
<li>gs: We specify <code>isGenerationMatch</code> on many operations to make sure that nothing has been created or overridden that we didn't expect.</li>
</ul>
<h2 id="040-alpha7---2020-05-26"><a class="header" href="#040-alpha7---2020-05-26">0.4.0-alpha.7 - 2020-05-26</a></h2>
<p>This release adds support for labeling BigQuery jobs.</p>
<h3 id="added-13"><a class="header" href="#added-13">Added</a></h3>
<ul>
<li>bigquery: Optionally specify billing labels for jobs. See the manual for details.</li>
<li>Allow driver argument names to be specified as either <code>x.y</code> or <code>x[y]</code>, interchangeably. This makes <code>job_labels</code> look nicer.</li>
<li>Hide URL passwords from (most) logs using a dedicated wrapper type.</li>
</ul>
<h3 id="changed-16"><a class="header" href="#changed-16">Changed</a></h3>
<ul>
<li>We now have test cases that make sure we catch duplicate driver arguments and raise an error.</li>
<li>redshift: Authentication argument names may no longer include <code>-</code> characters. I'm not even sure whether these are valid, but they won't work with the new scheme for parsing driver arguments.</li>
<li><code>DriverArguments::from_cli_args</code> now takes an iterator instead of a slice.</li>
</ul>
<h2 id="040-alpha6---2020-05-22"><a class="header" href="#040-alpha6---2020-05-22">0.4.0-alpha.6 - 2020-05-22</a></h2>
<p>This release improves the example <code>shopify.ts</code> schema, and adds new features to <code>dbcrossbar-ts</code> to parse it.</p>
<h3 id="added-14"><a class="header" href="#added-14">Added</a></h3>
<ul>
<li>dbcrossbar-ts:
<ul>
<li>Parse <code>/* */</code> comments.</li>
<li>Allow <code>Date</code> to be used as a type. This requires the date to be a string in ISO 8601 format, including a time zone.</li>
<li>Allow <code>decimal</code>, <code>int16</code>, <code>int32</code> and <code>int64</code> to be defined as any of <code>number</code>, <code>string</code>, <code>number | string</code> or <code>string | number</code>. This allows the schema to more accurately represent what appears on the wire. It allows <code>decimal</code> values to be represented as a mix of floats and strings, which is seen in Shopify.</li>
</ul>
</li>
<li>postgres-sql: Use new format for parse errors.</li>
</ul>
<h3 id="fixed-21"><a class="header" href="#fixed-21">Fixed</a></h3>
<ul>
<li>shopify: The example <code>shopify.ts</code> schema has been updated to use <code>Date</code> and <code>int64</code> in many places. <code>Address</code> and <code>CustomerAddress</code> are now distinct types, and several other minor issues have been fixed.</li>
</ul>
<h2 id="040-alpha5---2020-05-21"><a class="header" href="#040-alpha5---2020-05-21">0.4.0-alpha.5 - 2020-05-21</a></h2>
<h3 id="added-15"><a class="header" href="#added-15">Added</a></h3>
<ul>
<li>BigQuery: Support <code>--if-exists=error</code>.</li>
</ul>
<h3 id="changed-17"><a class="header" href="#changed-17">Changed</a></h3>
<ul>
<li>Require <code>--enable-unstable</code> to use <code>dbcrossbar-ts</code> or <code>shopify</code> locators, which are unstable.</li>
<li>AWS credentials must now always be passed via <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code> (optional) and <code>AWS_REGION</code> (optional). This lays the groundwork for replacing the <code>aws</code> CLI tool with native Rust code, so that we will someday be able to remove our last CLI dependency.</li>
</ul>
<h2 id="040-alpha4---2020-05-19"><a class="header" href="#040-alpha4---2020-05-19">0.4.0-alpha.4 - 2020-05-19</a></h2>
<h3 id="added-16"><a class="header" href="#added-16">Added</a></h3>
<ul>
<li>BigQuery now imports and exports decimal (aka NUMERIC) values everywhere.</li>
<li>The <code>dbcrossbar-ts</code> driver now supports magic type aliases that will convert to the corresponding <code>dbcrossbar</code> types:
<ul>
<li><code>type decimal = string;</code></li>
<li><code>type int16 = number | string;</code></li>
<li><code>type int32 = number | string;</code></li>
<li><code>type int64 = number | string;</code></li>
</ul>
</li>
</ul>
<h3 id="changed-18"><a class="header" href="#changed-18">Changed</a></h3>
<ul>
<li>The sample <code>shopify.ts</code> schema now uses <code>decimal</code> instead of <code>string</code> when appropriate. It does not attempt to use <code>int64</code> yet.</li>
</ul>
<h2 id="040-alpha3---2020-05-19-0-yanked"><a class="header" href="#040-alpha3---2020-05-19-0-yanked">0.4.0-alpha.3 - 2020-05-19 0 YANKED</a></h2>
<p>This release was yanked because it was missing several things it should have included.</p>
<h2 id="040-alpha2---2020-05-19"><a class="header" href="#040-alpha2---2020-05-19">0.4.0-alpha.2 - 2020-05-19</a></h2>
<p>This is a significant release, with support for &quot;struct&quot; types.</p>
<h3 id="added-17"><a class="header" href="#added-17">Added</a></h3>
<ul>
<li>The portable schema now supports a <code>DataType::Struct(fields)</code> type that can be used to represent BigQuery STRUCT values (as long as they have unique, named fields) and JSON objects with known keys.</li>
<li>The BigQuery driver now supports importing and exporting <code>STRUCT</code> fields using the new <code>DataType::Struct(fields)</code> type.</li>
<li>EXPERIMENTAL: Schemas can now be specified using the <code>dbcrossbar-ts</code> driver, which supports subset of TypeScript type declarations. This is useful for specifying complex, nested structs. This can be used as <code>--schema=&quot;dbcrossbar-ts:shopify.ts#Order&quot;</code>, where <code>Order</code> is the name of the type within the <code>*.ts</code> file to use as the table's type.</li>
<li>EXPERIMENTAL: We now support a Shopify input driver that uses the Shopify REST API. See the manual for details.</li>
<li>We now have support for fancy parser error messages, which we use with the <code>dbcrossbar-ts</code> parser.</li>
<li>We now support a CLI-editable config file using commands like <code>dbcrossbar config add temporary s3://example/temp/</code>.</li>
</ul>
<h3 id="changed-19"><a class="header" href="#changed-19">Changed</a></h3>
<ul>
<li>BREAKING: Some corner cases involving struct types and JSON may have changed subtly.</li>
<li>We've upgraded to the latest <code>rust-peg</code> parser syntax everywhere.</li>
</ul>
<h3 id="fixed-22"><a class="header" href="#fixed-22">Fixed</a></h3>
<ul>
<li><code>--if-exists=overwrite</code> now overwrites when writing to local files (instead of appending).</li>
<li>We automatically create <code>~/.local/share</code> if it does not exist.</li>
<li>More <code>clippy</code> warnings have been fixed, and unsafe code has been forbidden.</li>
<li>Various obsolete casting libraries have been removed.</li>
</ul>
<h2 id="040-alpha1---2020-04-07"><a class="header" href="#040-alpha1---2020-04-07">0.4.0-alpha.1 - 2020-04-07</a></h2>
<h3 id="changed-20"><a class="header" href="#changed-20">Changed</a></h3>
<ul>
<li>Replace <code>gcloud auth</code>, <code>gsutil</code> and <code>bq</code> with native Rust. This changes how we authenticate to Google Cloud. In particular, we now support <code>GCLOUD_CLIENT_SECRET</code>, <code>~/.config/dbcrossbar/gcloud_client_secret.json</code>, <code>GCLOUD_SERVICE_ACCOUNT_KEY</code> or <code>~/.config/dbcrossbar/gcloud_service_account_key.json</code>, as <a href="https://www.dbcrossbar.org/gs.html#configuration--authentication">explained in the manual</a>. We no longer use <code>gcloud auth</code>, and the Google Cloud SDK tools are no longer required. In the current alpha version, uploads and deletions are probably slower than before.</li>
</ul>
<h3 id="fixed-23"><a class="header" href="#fixed-23">Fixed</a></h3>
<ul>
<li>gs: Avoid download stalls when backpressure is applied (<a href="https://github.com/dbcrossbar/dbcrossbar/issues/102">#103</a>).</li>
<li>bigquery: Display error messages more reliably (<a href="https://github.com/dbcrossbar/dbcrossbar/issues/110">#110</a>).</li>
<li>bigquery: Detect &quot;`&quot; quotes in the CLI form of table names, and report an error.</li>
</ul>
<h2 id="033---2020-03-30"><a class="header" href="#033---2020-03-30">0.3.3 - 2020-03-30</a></h2>
<h3 id="added-18"><a class="header" href="#added-18">Added</a></h3>
<ul>
<li>BigML: Honor BIGML_DOMAIN, allowing the user to point the BigML driver to a custom VPC instance of BigML.</li>
</ul>
<h2 id="032---2020-03-30"><a class="header" href="#032---2020-03-30">0.3.2 - 2020-03-30</a></h2>
<h3 id="fixed-24"><a class="header" href="#fixed-24">Fixed</a></h3>
<ul>
<li>Correctly quote BigQuery column names again (which regressed in 0.3.0), and added test cases to prevent further regressions.</li>
<li>Fix an error that caused <code>bigquery_upsert</code> test to fail.</li>
</ul>
<h2 id="031---2020-03-29"><a class="header" href="#031---2020-03-29">0.3.1 - 2020-03-29</a></h2>
<h3 id="added-19"><a class="header" href="#added-19">Added</a></h3>
<ul>
<li>Write a new <a href="https://www.dbcrossbar.org/">manual</a>!</li>
</ul>
<h3 id="changed-21"><a class="header" href="#changed-21">Changed</a></h3>
<ul>
<li>Encapsulate all calls to <code>bq</code> and <code>gsutil</code></li>
<li>Improve performance of <code>--stream-size</code></li>
</ul>
<h3 id="fixed-25"><a class="header" href="#fixed-25">Fixed</a></h3>
<ul>
<li>BigQuery: Honor NOT NULL on import (fixes #45)</li>
</ul>
<h2 id="030---2020-03-26"><a class="header" href="#030---2020-03-26">0.3.0 - 2020-03-26</a></h2>
<h3 id="added-20"><a class="header" href="#added-20">Added</a></h3>
<ul>
<li>Use <code>cargo deny</code> to enforce license and duplicate dependency policies</li>
<li>Add notes about license and contribution policies</li>
</ul>
<h3 id="changed-22"><a class="header" href="#changed-22">Changed</a></h3>
<ul>
<li>Update to tokio 0.2 and the latest stable Rust</li>
<li>Replace <code>wkb</code> with <code>postgis</code> for licensing reasons</li>
<li>BigML: Fail immediately if no S3 temporary bucket provided (fixes #101)</li>
</ul>
<h3 id="fixed-26"><a class="header" href="#fixed-26">Fixed</a></h3>
<ul>
<li>BigQuery: Handle mixed-case column names using BigQuery semantics (fixes #84)</li>
<li>PostgreSQL: Fix upserts with mixed-case column names</li>
<li>BigQuery: Correctly output NULL values in Boolean columns (#104)</li>
</ul>
<h3 id="removed-6"><a class="header" href="#removed-6">Removed</a></h3>
<ul>
<li>BREAKING: BigQuery: Remove code that tried to rename column names to make them valid (fixes #84)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="credits-and-contributors"><a class="header" href="#credits-and-contributors">Credits and contributors</a></h1>
<p>The development of <code>dbcrossbar</code> has been generously supported by <a href="https://faraday.io/">Faraday</a>, which provides datascience and AI products for B2C companies.</p>
<p>Ongoing development of <code>dbcrossbar</code> is also supported by <a href="http://kiddsoftware.com/">Kidd Software LLC</a>, which creates custom software for businesses around the world.</p>
<h2 id="contributors"><a class="header" href="#contributors">Contributors</a></h2>
<p><code>dbcrossbar</code> is primarily maintained by Eric Kidd. Other contributors include:</p>
<ul>
<li>Bill Morris</li>
<li>Forrest Wallace</li>
<li>Prithaj Nath</li>
<li>Seamus Abshere</li>
</ul>
<p>We have also received very helpful bug reports and feature requests from our users. Thank you!</p>

                </main>

                <nav class="nav-wrapper" aria-label="Page navigation">
                    <!-- Mobile navigation buttons -->
                    <div style="clear: both"></div>
                </nav>
            </div>
        </div>

        <nav class="nav-wide-wrapper" aria-label="Page navigation">
        </nav>

    </div>

    <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
    <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
    <script src="book.js" type="text/javascript" charset="utf-8"></script>

    <!-- Custom JS scripts -->
    <script type="text/javascript">
        window.addEventListener('load', function () {
            window.setTimeout(window.print, 100);
        });
    </script>
</body>

</html>