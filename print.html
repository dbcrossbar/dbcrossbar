<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">

<head>
    <!-- Book generated using mdBook -->
    <meta charset="UTF-8">
    <title>Using dbcrossbar</title>
    <meta name="robots" content="noindex" />
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta name="description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#ffffff" />

    <!-- dbcrossbar metadata for social media "unfurling" -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://www.dbcrossbar.org/">
    <meta property="og:title" content="Using dbcrossbar">
    <meta property="og:description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta property="og:image" content="https://www.dbcrossbar.org/dbcrossbar_intro.png">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="www.dbcrossbar.org">
    <meta property="twitter:title" content="Using dbcrossbar">
    <meta property="twitter:description" content="dbcrossbar copies tabular data between PostgreSQL, BigQuery, CSV and many other databases and formats.">
    <meta property="twitter:image" content="https://www.dbcrossbar.org/dbcrossbar_intro.png">
    <meta property="twitter:url" content="https://www.dbcrossbar.org/">

    <link rel="shortcut icon" href="">
    <link rel="stylesheet" href="css/variables.css">
    <link rel="stylesheet" href="css/general.css">
    <link rel="stylesheet" href="css/chrome.css">
    <link rel="stylesheet" href="css/print.css" media="print">

    <!-- Fonts -->
    <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
    <link
        href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

    <!-- Highlight.js Stylesheets -->
    <link rel="stylesheet" href="highlight.css">
    <link rel="stylesheet" href="tomorrow-night.css">
    <link rel="stylesheet" href="ayu-highlight.css">

    <!-- Custom theme stylesheets -->
</head>

<body>
    <!-- Provide site root to javascript -->
    <script type="text/javascript">
        var path_to_root = "";
        var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
    </script>

    <!-- Work around some values being stored in localStorage wrapped in quotes -->
    <script type="text/javascript">
        try {
            var theme = localStorage.getItem('mdbook-theme');
            var sidebar = localStorage.getItem('mdbook-sidebar');

            if (theme.startsWith('"') && theme.endsWith('"')) {
                localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
            }

            if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
            }
        } catch (e) { }
    </script>

    <!-- Set the theme before any content is loaded, prevents flash -->
    <script type="text/javascript">
        var theme;
        try { theme = localStorage.getItem('mdbook-theme'); } catch (e) { }
        if (theme === null || theme === undefined) { theme = default_theme; }
        var html = document.querySelector('html');
        html.classList.remove('no-js')
        html.classList.remove('light')
        html.classList.add(theme);
        html.classList.add('js');
    </script>

    <!-- Hide / unhide sidebar before it is displayed -->
    <script type="text/javascript">
        var html = document.querySelector('html');
        var sidebar = 'hidden';
        if (document.body.clientWidth >= 1080) {
            try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch (e) { }
            sidebar = sidebar || 'visible';
        }
        html.classList.remove('sidebar-visible');
        html.classList.add("sidebar-" + sidebar);
    </script>

    <nav id="sidebar" class="sidebar" aria-label="Table of contents">
        <div class="sidebar-scrollbox">
            <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">dbcrossbar Guide</a></li><li class="chapter-item expanded "><a href="features.html"><strong aria-hidden="true">1.</strong> Features &amp; philosophy</a></li><li class="chapter-item expanded "><a href="installing.html"><strong aria-hidden="true">2.</strong> Installing</a></li><li class="chapter-item expanded "><a href="how.html"><strong aria-hidden="true">3.</strong> How it works</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="csv_interchange.html"><strong aria-hidden="true">3.1.</strong> CSV interchange format</a></li><li class="chapter-item expanded "><a href="schema.html"><strong aria-hidden="true">3.2.</strong> Portable table schema</a></li></ol></li><li class="chapter-item expanded "><a href="config.html"><strong aria-hidden="true">4.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="commands.html"><strong aria-hidden="true">5.</strong> Commands</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cp.html"><strong aria-hidden="true">5.1.</strong> cp: Copying tables</a></li><li class="chapter-item expanded "><a href="count.html"><strong aria-hidden="true">5.2.</strong> count: Counting records</a></li><li class="chapter-item expanded "><a href="conv.html"><strong aria-hidden="true">5.3.</strong> schema conv: Transforming schemas</a></li></ol></li><li class="chapter-item expanded "><a href="drivers.html"><strong aria-hidden="true">6.</strong> Drivers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bigml.html"><strong aria-hidden="true">6.1.</strong> BigML</a></li><li class="chapter-item expanded "><a href="bigquery.html"><strong aria-hidden="true">6.2.</strong> BigQuery</a></li><li class="chapter-item expanded "><a href="csv.html"><strong aria-hidden="true">6.3.</strong> CSV</a></li><li class="chapter-item expanded "><a href="gs.html"><strong aria-hidden="true">6.4.</strong> Google Cloud Storage</a></li><li class="chapter-item expanded "><a href="postgres.html"><strong aria-hidden="true">6.5.</strong> PostgreSQL</a></li><li class="chapter-item expanded "><a href="redshift.html"><strong aria-hidden="true">6.6.</strong> RedShift</a></li><li class="chapter-item expanded "><a href="s3.html"><strong aria-hidden="true">6.7.</strong> S3</a></li><li class="chapter-item expanded "><a href="shopify.html"><strong aria-hidden="true">6.8.</strong> Shopify (UNSTABLE)</a></li></ol></li><li class="chapter-item expanded "><a href="schemas.html"><strong aria-hidden="true">7.</strong> Specifying table schemas</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="postgres-sql.html"><strong aria-hidden="true">7.1.</strong> Postgres CREATE TABLE</a></li><li class="chapter-item expanded "><a href="bigquery-schema.html"><strong aria-hidden="true">7.2.</strong> BigQuery JSON schemas</a></li><li class="chapter-item expanded "><a href="dbcrossbar-schema.html"><strong aria-hidden="true">7.3.</strong> Native dbcrossbar schemas</a></li><li class="chapter-item expanded "><a href="dbcrossbar-ts.html"><strong aria-hidden="true">7.4.</strong> TypeScript schemas (UNSTABLE)</a></li></ol></li><li class="chapter-item expanded "><a href="changes.html"><strong aria-hidden="true">8.</strong> Changes</a></li><li class="chapter-item expanded affix "><a href="credits.html">Credits and contributors</a></li></ol>
        </div>
        <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
    </nav>

    <div id="page-wrapper" class="page-wrapper">

        <div class="page">
            <div id="menu-bar-hover-placeholder"></div>
            <div id="menu-bar" class="menu-bar sticky bordered">
                <div class="left-buttons">
                    <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents"
                        aria-label="Toggle Table of Contents" aria-controls="sidebar">
                        <i class="fa fa-bars"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button" type="button" title="Change theme"
                        aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                        <i class="fa fa-paint-brush"></i>
                    </button>
                    <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                        <li role="none"><button role="menuitem" class="theme"
                                id="light">Light (default)</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="rust">Rust</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="coal">Coal</button></li>
                        <li role="none"><button role="menuitem" class="theme"
                                id="navy">Navy</button></li>
                        <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button>
                        </li>
                    </ul>
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)"
                        aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S"
                        aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                </div>

                <h1 class="menu-title">Using dbcrossbar</h1>

                <div class="right-buttons">
                    <a href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a href="https://github.com/dbcrossbar/dbcrossbar" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>

            <div id="search-wrapper" class="hidden">
                <form id="searchbar-outer" class="searchbar-outer">
                    <input type="search" name="search" id="searchbar" name="searchbar"
                        placeholder="Search this book ..." aria-controls="searchresults-outer"
                        aria-describedby="searchresults-header">
                </form>
                <div id="searchresults-outer" class="searchresults-outer hidden">
                    <div id="searchresults-header" class="searchresults-header"></div>
                    <ul id="searchresults">
                    </ul>
                </div>
            </div>
            <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
            <script type="text/javascript">
                document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                Array.from(document.querySelectorAll('#sidebar a')).forEach(function (link) {
                    link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                });
            </script>

            <div id="content" class="content">
                <main>
                    <h1 id="dbcrossbar-guide"><a class="header" href="#dbcrossbar-guide"><code>dbcrossbar</code> Guide</a></h1>
<p><code>dbcrossbar</code> is an <a href="https://github.com/dbcrossbar/dbcrossbar">open source</a> tool that copies large, tabular datasets between many different databases and storage formats. Data can be copied from any source to any destination.</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="514pt" height="368pt"
 viewBox="0.00 0.00 514.16 368.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 364)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-364 510.16,-364 510.16,4 -4,4"/><!-- csv_in --><g id="node1" class="node"><title>csv_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-342" rx="30.59" ry="18"/><text text-anchor="middle" x="77.34" y="-338.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar --><g id="node3" class="node"><title>dbcrossbar</title><ellipse fill="none" stroke="black" cx="253.08" cy="-180" rx="62.29" ry="18"/><text text-anchor="middle" x="253.08" y="-176.3" font-family="Times,serif" font-size="14.00">dbcrossbar</text></g><!-- csv_in&#45;&gt;dbcrossbar --><g id="edge1" class="edge"><title>csv_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M106.44,-336.4C121.79,-332.33 140.51,-325.62 154.69,-315 193.3,-286.08 222.97,-237.47 238.94,-207.15"/><polygon fill="black" stroke="black" points="242.18,-208.49 243.64,-198 235.96,-205.29 242.18,-208.49"/></g><!-- csv_out --><g id="node2" class="node"><title>csv_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-342" rx="30.59" ry="18"/><text text-anchor="middle" x="428.82" y="-338.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar&#45;&gt;csv_out --><g id="edge2" class="edge"><title>dbcrossbar&#45;&gt;csv_out</title><path fill="none" stroke="black" d="M262.53,-198C276.88,-226.72 308.68,-282.95 351.48,-315 362.77,-323.46 376.95,-329.44 390.02,-333.58"/><polygon fill="black" stroke="black" points="389.14,-336.97 399.72,-336.4 391.09,-330.25 389.14,-336.97"/></g><!-- postgres_out --><g id="node5" class="node"><title>postgres_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-288" rx="65.79" ry="18"/><text text-anchor="middle" x="428.82" y="-284.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- dbcrossbar&#45;&gt;postgres_out --><g id="edge4" class="edge"><title>dbcrossbar&#45;&gt;postgres_out</title><path fill="none" stroke="black" d="M270.96,-197.29C289.35,-215.29 320.31,-243.29 351.48,-261 358.26,-264.86 365.7,-268.31 373.19,-271.36"/><polygon fill="black" stroke="black" points="372.06,-274.67 382.65,-275 374.57,-268.14 372.06,-274.67"/></g><!-- bigquery_out --><g id="node7" class="node"><title>bigquery_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-234" rx="54.69" ry="18"/><text text-anchor="middle" x="428.82" y="-230.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- dbcrossbar&#45;&gt;bigquery_out --><g id="edge6" class="edge"><title>dbcrossbar&#45;&gt;bigquery_out</title><path fill="none" stroke="black" d="M296.14,-193.07C321.07,-200.82 352.79,-210.68 378.85,-218.78"/><polygon fill="black" stroke="black" points="377.94,-222.16 388.53,-221.79 380.02,-215.48 377.94,-222.16"/></g><!-- s3_out --><g id="node9" class="node"><title>s3_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-180" rx="27" ry="18"/><text text-anchor="middle" x="428.82" y="-176.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- dbcrossbar&#45;&gt;s3_out --><g id="edge8" class="edge"><title>dbcrossbar&#45;&gt;s3_out</title><path fill="none" stroke="black" d="M315.82,-180C341.14,-180 369.56,-180 391.44,-180"/><polygon fill="black" stroke="black" points="391.62,-183.5 401.62,-180 391.62,-176.5 391.62,-183.5"/></g><!-- gs_out --><g id="node11" class="node"><title>gs_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-126" rx="77.19" ry="18"/><text text-anchor="middle" x="428.82" y="-122.3" font-family="Times,serif" font-size="14.00">Cloud Storage</text></g><!-- dbcrossbar&#45;&gt;gs_out --><g id="edge10" class="edge"><title>dbcrossbar&#45;&gt;gs_out</title><path fill="none" stroke="black" d="M296.14,-166.93C318.92,-159.85 347.37,-151 371.99,-143.35"/><polygon fill="black" stroke="black" points="373.06,-146.69 381.57,-140.37 370.98,-140 373.06,-146.69"/></g><!-- redshift_out --><g id="node13" class="node"><title>redshift_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-72" rx="51.19" ry="18"/><text text-anchor="middle" x="428.82" y="-68.3" font-family="Times,serif" font-size="14.00">RedShift</text></g><!-- dbcrossbar&#45;&gt;redshift_out --><g id="edge12" class="edge"><title>dbcrossbar&#45;&gt;redshift_out</title><path fill="none" stroke="black" d="M270.96,-162.71C289.35,-144.71 320.31,-116.71 351.48,-99 359.91,-94.21 369.35,-90.03 378.63,-86.5"/><polygon fill="black" stroke="black" points="379.99,-89.73 388.21,-83.04 377.61,-83.14 379.99,-89.73"/></g><!-- etc_out --><g id="node15" class="node"><title>etc_out</title><ellipse fill="none" stroke="black" cx="428.82" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="428.82" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- dbcrossbar&#45;&gt;etc_out --><g id="edge14" class="edge"><title>dbcrossbar&#45;&gt;etc_out</title><path fill="none" stroke="black" d="M262.53,-162C276.88,-133.28 308.68,-77.05 351.48,-45 363.61,-35.91 379.06,-29.69 392.89,-25.53"/><polygon fill="black" stroke="black" points="393.89,-28.89 402.6,-22.86 392.03,-22.14 393.89,-28.89"/></g><!-- postgres_in --><g id="node4" class="node"><title>postgres_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-288" rx="65.79" ry="18"/><text text-anchor="middle" x="77.34" y="-284.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- postgres_in&#45;&gt;dbcrossbar --><g id="edge3" class="edge"><title>postgres_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M123.52,-275C134.06,-271.16 145.03,-266.49 154.69,-261 181.96,-245.5 209.07,-222.13 227.74,-204.47"/><polygon fill="black" stroke="black" points="230.42,-206.75 235.2,-197.29 225.57,-201.7 230.42,-206.75"/></g><!-- bigquery_in --><g id="node6" class="node"><title>bigquery_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-234" rx="54.69" ry="18"/><text text-anchor="middle" x="77.34" y="-230.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- bigquery_in&#45;&gt;dbcrossbar --><g id="edge5" class="edge"><title>bigquery_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M117.77,-221.75C142.18,-214.16 173.81,-204.33 200.24,-196.11"/><polygon fill="black" stroke="black" points="201.57,-199.36 210.08,-193.05 199.5,-192.68 201.57,-199.36"/></g><!-- s3_in --><g id="node8" class="node"><title>s3_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-180" rx="27" ry="18"/><text text-anchor="middle" x="77.34" y="-176.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- s3_in&#45;&gt;dbcrossbar --><g id="edge7" class="edge"><title>s3_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M104.35,-180C124.5,-180 153.42,-180 180.34,-180"/><polygon fill="black" stroke="black" points="180.54,-183.5 190.54,-180 180.54,-176.5 180.54,-183.5"/></g><!-- gs_in --><g id="node10" class="node"><title>gs_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-126" rx="77.19" ry="18"/><text text-anchor="middle" x="77.34" y="-122.3" font-family="Times,serif" font-size="14.00">Cloud Storage</text></g><!-- gs_in&#45;&gt;dbcrossbar --><g id="edge9" class="edge"><title>gs_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M124.45,-140.33C147.77,-147.58 176.17,-156.41 200.25,-163.89"/><polygon fill="black" stroke="black" points="199.35,-167.27 209.93,-166.9 201.42,-160.59 199.35,-167.27"/></g><!-- redshift_in --><g id="node12" class="node"><title>redshift_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-72" rx="51.19" ry="18"/><text text-anchor="middle" x="77.34" y="-68.3" font-family="Times,serif" font-size="14.00">RedShift</text></g><!-- redshift_in&#45;&gt;dbcrossbar --><g id="edge11" class="edge"><title>redshift_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M117.96,-83.04C130.14,-87.21 143.32,-92.54 154.69,-99 181.96,-114.5 209.07,-137.87 227.74,-155.53"/><polygon fill="black" stroke="black" points="225.57,-158.3 235.2,-162.71 230.42,-153.25 225.57,-158.3"/></g><!-- etc_in --><g id="node14" class="node"><title>etc_in</title><ellipse fill="none" stroke="black" cx="77.34" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="77.34" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- etc_in&#45;&gt;dbcrossbar --><g id="edge13" class="edge"><title>etc_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M103.56,-22.86C119.4,-26.84 139.61,-33.71 154.69,-45 193.3,-73.92 222.97,-122.53 238.94,-152.85"/><polygon fill="black" stroke="black" points="235.96,-154.71 243.64,-162 242.18,-151.51 235.96,-154.71"/></g></g></svg></div>
<h2 id="an-example"><a class="header" href="#an-example">An example</a></h2>
<p>If we have a CSV file <code>my_table.csv</code> containing data:</p>
<pre><code class="language-csv">id,name,quantity
1,Blue widget,10
2,Red widget,50
</code></pre>
<p>And a file <code>my_table.sql</code> containing a table definition:</p>
<pre><code class="language-sql">CREATE TABLE my_table (
    id INT NOT NULL,
    name TEXT NOT NULL,
    quantity INT NOT NULL
);
</code></pre>
<p>Then we can use these to create a PostgreSQL table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
<p>If we want to use this data to update an existing table in BigQuery, we can upsert into BigQuery using the <code>id</code> column:</p>
<pre><code class="language-sh">dbcrossbar config add temporary gs://$GS_TEMP_BUCKET
dbcrossbar config add temporary bigquery:$GCLOUD_PROJECT:temp_dataset
dbcrossbar cp \
    --if-exists=upsert-on:id \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table
</code></pre>
<p>This will stream the data out of PostgreSQL, upload it to <code>$GS_TEMP_BUCKET</code>, import it into a temporary BigQuery table, and run an appropriate <code>MERGE</code> command. The <code>config add temporary</code> commands tell <code>dbcrossbar</code> what cloud bucket and BigQuery dataset should be used for temporary files and tables, respectively.</p>
<p>Notice that we don't need to specify <code>--schema</code>, because <code>dbcrossbar</code> will automatically translate the PostgreSQL column types to corresponding BigQuery types.</p>
<h2 id="credits"><a class="header" href="#credits">Credits</a></h2>
<p><code>dbcrossbar</code> is generously supported by <a href="http://faraday.io/">Faraday</a> and by open source contributors. Please see the <a href="./credits.html">credits</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="features--philosophy"><a class="header" href="#features--philosophy">Features &amp; philosophy</a></h1>
<p><code>dbcrossbar</code> is designed to do a few things well. Typically, <code>dbcrossbar</code> is used for loading raw data, and for moving data back and forth between production databases and data warehouses. It supports a few core features:</p>
<ol>
<li>Copying tables.</li>
<li>Counting records in tables.</li>
<li>Converting between different table schema formats, including PostgreSQL <code>CREATE TABLE</code> statements and BigQuery schema JSON.</li>
</ol>
<p><code>dbcrossbar</code> offers a number of handy features:</p>
<ul>
<li>A single static binary on Linux, with no dependencies.</li>
<li>A stream-based architecture that limits the use of RAM and requires no temporary files.</li>
<li>Support for appending, overwriting or upserting into existing tables.</li>
<li>Support for selecting records using <code>--where</code>.</li>
</ul>
<p><code>dbcrossbar</code> also supports a rich variety of portable column types:</p>
<ul>
<li>Common types, including booleans, dates, timestamps, floats, integers, and text.</li>
<li>UUIDs.</li>
<li>JSON.</li>
<li>GeoJSON.</li>
<li>Arrays.</li>
</ul>
<h2 id="non-features"><a class="header" href="#non-features">Non-features</a></h2>
<p>The following features are explicitly excluded from <code>dbcrossbar</code>'s mission:</p>
<ul>
<li>Data cleaning and transformation.</li>
<li>Fixing invalid column names.</li>
<li>Copying multiple tables at time.</li>
<li>Automatically copying constraints, foreign keys, etc.</li>
</ul>
<p>If you need these features, then take a look at tools like <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> and <a href="https://pgloader.io/"><code>pgloader</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing"><a class="header" href="#installing">Installing</a></h1>
<p>Pre-built binaries for <code>dbcrossbar</code> are <a href="https://github.com/dbcrossbar/dbcrossbar/releases">available on GitHub</a>. These currently include:</p>
<ol>
<li>Fully-static Linux x86_64 binaries, which should work on any modern distribution (including Alpine Linux containers).</li>
<li>MacOS X binaries. Note that these are unsigned, and you may need to use <code>xattr -d com.apple.quarantine dbcrossbar</code> to make them runnable.</li>
</ol>
<p>Windows binaries are not available at this time, but it may be possible to build them with a little work.</p>
<h2 id="required-tools"><a class="header" href="#required-tools">Required tools</a></h2>
<p>To use the S3 and RedShift drivers, you will need to install the <a href="https://aws.amazon.com/cli/">AWS CLI tools</a>. We plan to replace the AWS CLI tools with native Rust libraries before the 1.0 release.</p>
<h2 id="installing-using-cargo"><a class="header" href="#installing-using-cargo">Installing using <code>cargo</code></a></h2>
<p>You can also install <code>dbcrossbar</code> using <code>cargo</code>. First, you will need to make sure you have the necessary C dependencies installed:</p>
<pre><code class="language-sh"># Ubuntu Linux (might be incomplete).
sudo apt install build-essential
</code></pre>
<p>Then, you can install using <code>cargo</code>:</p>
<pre><code class="language-sh">cargo install dbcrossbar
</code></pre>
<h2 id="building-from-source"><a class="header" href="#building-from-source">Building from source</a></h2>
<p>The source code is available <a href="https://github.com/dbcrossbar/dbcrossbar">on GitHub</a>. First, install the build dependencies as described above. Then run:</p>
<pre><code class="language-sh">git clone https://github.com/dbcrossbar/dbcrossbar.git
cd dbcrossbar
cargo build --release
</code></pre>
<p>This will create <code>target/release/dbcrossbar</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h1>
<p><code>dbcrossbar</code> uses pluggable input and output drivers, allowing any input to be copied to any output:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="467pt" height="260pt"
 viewBox="0.00 0.00 467.37 260.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 256)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-256 463.37,-256 463.37,4 -4,4"/><!-- csv_in --><g id="node1" class="node"><title>csv_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-234" rx="30.59" ry="18"/><text text-anchor="middle" x="65.64" y="-230.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar --><g id="node3" class="node"><title>dbcrossbar</title><ellipse fill="none" stroke="black" cx="229.68" cy="-126" rx="62.29" ry="18"/><text text-anchor="middle" x="229.68" y="-122.3" font-family="Times,serif" font-size="14.00">dbcrossbar</text></g><!-- csv_in&#45;&gt;dbcrossbar --><g id="edge1" class="edge"><title>csv_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M92.67,-225.17C104.82,-220.54 119.22,-214.31 131.29,-207 157.95,-190.85 184.95,-167.75 203.73,-150.35"/><polygon fill="black" stroke="black" points="206.37,-152.67 211.26,-143.28 201.57,-147.57 206.37,-152.67"/></g><!-- csv_out --><g id="node2" class="node"><title>csv_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-234" rx="30.59" ry="18"/><text text-anchor="middle" x="393.72" y="-230.3" font-family="Times,serif" font-size="14.00">CSV</text></g><!-- dbcrossbar&#45;&gt;csv_out --><g id="edge2" class="edge"><title>dbcrossbar&#45;&gt;csv_out</title><path fill="none" stroke="black" d="M248.11,-143.28C266.7,-161.01 297.61,-188.54 328.08,-207 337.13,-212.48 347.49,-217.36 357.23,-221.4"/><polygon fill="black" stroke="black" points="356.11,-224.72 366.69,-225.17 358.7,-218.22 356.11,-224.72"/></g><!-- postgres_out --><g id="node5" class="node"><title>postgres_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-180" rx="65.79" ry="18"/><text text-anchor="middle" x="393.72" y="-176.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- dbcrossbar&#45;&gt;postgres_out --><g id="edge4" class="edge"><title>dbcrossbar&#45;&gt;postgres_out</title><path fill="none" stroke="black" d="M271.15,-139.49C292.56,-146.62 319.06,-155.45 341.85,-163.05"/><polygon fill="black" stroke="black" points="340.76,-166.37 351.36,-166.22 342.98,-159.73 340.76,-166.37"/></g><!-- bigquery_out --><g id="node7" class="node"><title>bigquery_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-126" rx="54.69" ry="18"/><text text-anchor="middle" x="393.72" y="-122.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- dbcrossbar&#45;&gt;bigquery_out --><g id="edge6" class="edge"><title>dbcrossbar&#45;&gt;bigquery_out</title><path fill="none" stroke="black" d="M292.4,-126C304.3,-126 316.79,-126 328.76,-126"/><polygon fill="black" stroke="black" points="329.11,-129.5 339.11,-126 329.11,-122.5 329.11,-129.5"/></g><!-- s3_out --><g id="node9" class="node"><title>s3_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-72" rx="27" ry="18"/><text text-anchor="middle" x="393.72" y="-68.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- dbcrossbar&#45;&gt;s3_out --><g id="edge8" class="edge"><title>dbcrossbar&#45;&gt;s3_out</title><path fill="none" stroke="black" d="M271.15,-112.51C298.46,-103.41 334.05,-91.55 359.66,-83.02"/><polygon fill="black" stroke="black" points="360.9,-86.29 369.29,-79.81 358.69,-79.65 360.9,-86.29"/></g><!-- etc_out --><g id="node11" class="node"><title>etc_out</title><ellipse fill="none" stroke="black" cx="393.72" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="393.72" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- dbcrossbar&#45;&gt;etc_out --><g id="edge10" class="edge"><title>dbcrossbar&#45;&gt;etc_out</title><path fill="none" stroke="black" d="M248.11,-108.72C266.7,-90.99 297.61,-63.46 328.08,-45 337.85,-39.08 349.16,-33.87 359.56,-29.64"/><polygon fill="black" stroke="black" points="361.12,-32.79 369.17,-25.9 358.58,-26.26 361.12,-32.79"/></g><!-- postgres_in --><g id="node4" class="node"><title>postgres_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-180" rx="65.79" ry="18"/><text text-anchor="middle" x="65.64" y="-176.3" font-family="Times,serif" font-size="14.00">PostgreSQL</text></g><!-- postgres_in&#45;&gt;dbcrossbar --><g id="edge3" class="edge"><title>postgres_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M107.95,-166.23C129.35,-159.1 155.66,-150.33 178.25,-142.81"/><polygon fill="black" stroke="black" points="179.63,-146.04 188.01,-139.55 177.42,-139.4 179.63,-146.04"/></g><!-- bigquery_in --><g id="node6" class="node"><title>bigquery_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-126" rx="54.69" ry="18"/><text text-anchor="middle" x="65.64" y="-122.3" font-family="Times,serif" font-size="14.00">BigQuery</text></g><!-- bigquery_in&#45;&gt;dbcrossbar --><g id="edge5" class="edge"><title>bigquery_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M120.6,-126C132.25,-126 144.75,-126 156.96,-126"/><polygon fill="black" stroke="black" points="157.12,-129.5 167.12,-126 157.12,-122.5 157.12,-129.5"/></g><!-- s3_in --><g id="node8" class="node"><title>s3_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-72" rx="27" ry="18"/><text text-anchor="middle" x="65.64" y="-68.3" font-family="Times,serif" font-size="14.00">S3</text></g><!-- s3_in&#45;&gt;dbcrossbar --><g id="edge7" class="edge"><title>s3_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M90.21,-79.85C113.23,-87.53 149.02,-99.45 178.44,-109.26"/><polygon fill="black" stroke="black" points="177.56,-112.65 188.15,-112.49 179.77,-106.01 177.56,-112.65"/></g><!-- etc_in --><g id="node10" class="node"><title>etc_in</title><ellipse fill="none" stroke="black" cx="65.64" cy="-18" rx="27" ry="18"/><text text-anchor="middle" x="65.64" y="-14.3" font-family="Times,serif" font-size="14.00">...</text></g><!-- etc_in&#45;&gt;dbcrossbar --><g id="edge9" class="edge"><title>etc_in&#45;&gt;dbcrossbar</title><path fill="none" stroke="black" d="M90.19,-25.9C102.82,-30.61 118.39,-37.19 131.29,-45 157.95,-61.15 184.95,-84.25 203.73,-101.65"/><polygon fill="black" stroke="black" points="201.57,-104.43 211.26,-108.72 206.37,-99.33 201.57,-104.43"/></g></g></svg></div>
<h2 id="parallel-data-streams"><a class="header" href="#parallel-data-streams">Parallel data streams</a></h2>
<p>Internally, <code>dbcrossbar</code> uses parallel data streams. If we copy <code>s3://example/</code> to <code>csv:out/</code> using <code>--max-streams=4</code>, this will run up to 4 copies in parallel:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="451pt" height="206pt"
 viewBox="0.00 0.00 450.86 206.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 202)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-202 446.86,-202 446.86,4 -4,4"/><!-- src1 --><g id="node1" class="node"><title>src1</title><ellipse fill="none" stroke="black" cx="114.39" cy="-180" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-176.3" font-family="Times,serif" font-size="14.00">s3://example/file_1.csv</text></g><!-- dest1 --><g id="node2" class="node"><title>dest1</title><ellipse fill="none" stroke="black" cx="353.82" cy="-180" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-176.3" font-family="Times,serif" font-size="14.00">csv:out/file_1.csv</text></g><!-- src1&#45;&gt;dest1 --><g id="edge1" class="edge"><title>src1&#45;&gt;dest1</title><path fill="none" stroke="black" d="M228.89,-180C237.45,-180 246.05,-180 254.5,-180"/><polygon fill="black" stroke="black" points="254.73,-183.5 264.73,-180 254.73,-176.5 254.73,-183.5"/></g><!-- src2 --><g id="node3" class="node"><title>src2</title><ellipse fill="none" stroke="black" cx="114.39" cy="-126" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-122.3" font-family="Times,serif" font-size="14.00">s3://example/file_2.csv</text></g><!-- src1&#45;&gt;src2 --><!-- dest2 --><g id="node4" class="node"><title>dest2</title><ellipse fill="none" stroke="black" cx="353.82" cy="-126" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-122.3" font-family="Times,serif" font-size="14.00">csv:out/file_2.csv</text></g><!-- src2&#45;&gt;dest2 --><g id="edge2" class="edge"><title>src2&#45;&gt;dest2</title><path fill="none" stroke="black" d="M228.89,-126C237.45,-126 246.05,-126 254.5,-126"/><polygon fill="black" stroke="black" points="254.73,-129.5 264.73,-126 254.73,-122.5 254.73,-129.5"/></g><!-- src3 --><g id="node5" class="node"><title>src3</title><ellipse fill="none" stroke="black" cx="114.39" cy="-72" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-68.3" font-family="Times,serif" font-size="14.00">s3://example/file_3.csv</text></g><!-- src2&#45;&gt;src3 --><!-- dest3 --><g id="node6" class="node"><title>dest3</title><ellipse fill="none" stroke="black" cx="353.82" cy="-72" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-68.3" font-family="Times,serif" font-size="14.00">csv:out/file_3.csv</text></g><!-- src3&#45;&gt;dest3 --><g id="edge3" class="edge"><title>src3&#45;&gt;dest3</title><path fill="none" stroke="black" d="M228.89,-72C237.45,-72 246.05,-72 254.5,-72"/><polygon fill="black" stroke="black" points="254.73,-75.5 264.73,-72 254.73,-68.5 254.73,-75.5"/></g><!-- src4 --><g id="node8" class="node"><title>src4</title><ellipse fill="none" stroke="black" cx="114.39" cy="-18" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-14.3" font-family="Times,serif" font-size="14.00">s3://example/file_4.csv</text></g><!-- src3&#45;&gt;src4 --><!-- dest4 --><g id="node7" class="node"><title>dest4</title><ellipse fill="none" stroke="black" cx="353.82" cy="-18" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-14.3" font-family="Times,serif" font-size="14.00">csv:out/file_4.csv</text></g><!-- src4&#45;&gt;dest4 --><g id="edge4" class="edge"><title>src4&#45;&gt;dest4</title><path fill="none" stroke="black" d="M228.89,-18C237.45,-18 246.05,-18 254.5,-18"/><polygon fill="black" stroke="black" points="254.73,-21.5 264.73,-18 254.73,-14.5 254.73,-21.5"/></g></g></svg></div>
<p>As soon as one stream finishes, a new one will be started:</p>
<div><!-- Generated by graphviz version 2.43.0 (0)
 --><!-- Title: %3 Pages: 1 --><svg width="451pt" height="44pt"
 viewBox="0.00 0.00 450.86 44.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 40)"><title>%3</title><polygon fill="white" stroke="transparent" points="-4,4 -4,-40 446.86,-40 446.86,4 -4,4"/><!-- src5 --><g id="node1" class="node"><title>src5</title><ellipse fill="none" stroke="black" cx="114.39" cy="-18" rx="114.28" ry="18"/><text text-anchor="middle" x="114.39" y="-14.3" font-family="Times,serif" font-size="14.00">s3://example/file_5.csv</text></g><!-- dest5 --><g id="node2" class="node"><title>dest5</title><ellipse fill="none" stroke="black" cx="353.82" cy="-18" rx="89.08" ry="18"/><text text-anchor="middle" x="353.82" y="-14.3" font-family="Times,serif" font-size="14.00">csv:out/file_5.csv</text></g><!-- src5&#45;&gt;dest5 --><g id="edge1" class="edge"><title>src5&#45;&gt;dest5</title><path fill="none" stroke="black" d="M228.89,-18C237.45,-18 246.05,-18 254.5,-18"/><polygon fill="black" stroke="black" points="254.73,-21.5 264.73,-18 254.73,-14.5 254.73,-21.5"/></g></g></svg></div>
<p><code>dbcrossbar</code> accomplishes this using a <strong>stream of CSV streams.</strong> This allows us to make extensive use of <a href="https://ferd.ca/queues-don-t-fix-overload.html">backpressure</a> to control how data flows through the system, eliminating the need for temporary files. This makes it easier to work with 100GB+ CSV files and 1TB+ datasets.</p>
<h2 id="shortcuts"><a class="header" href="#shortcuts">Shortcuts</a></h2>
<p>When copying between certain drivers, <code>dbcrossbar</code> supports &quot;shortcuts.&quot; For example, it can load data directly from Google Cloud Storage into BigQuery.</p>
<h2 id="multi-threaded-asynchronous-rust"><a class="header" href="#multi-threaded-asynchronous-rust">Multi-threaded, asynchronous Rust</a></h2>
<p><code>dbcrossbar</code> is written using <a href="https://rust-lang.github.io/async-book/">asynchronous</a> <a href="https://www.rust-lang.org/">Rust</a>, and it makes heavy use of a multi-threaded worker pool. Internally, it works something like a set of classic Unix pipelines running in parallel. Thanks to Rust, it has been possible to get native performance and multithreading without spending too much time debugging.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csv-interchange-format"><a class="header" href="#csv-interchange-format">CSV interchange format</a></h1>
<p>Internally, <code>dbcrossbar</code> converts all data into CSV streams. For many standard types, all input drivers are required to provide byte-for-byte identical CSV data:</p>
<pre><code class="language-csv">id,test_bool,test_date,test_int16,test_int32,test_int64,test_text,test_timestamp_without_time_zone,test_timestamp_with_time_zone,test_uuid,select,testCapitalized,test_enum
1,t,1969-07-20,-32768,-2147483648,-9223372036854775808,hello,1969-07-20T20:17:39,1969-07-20T20:17:39Z,084ec3bb-3193-4ffb-8b74-99a288e8432c,,,red
2,f,2001-01-01,32767,2147483647,9223372036854775807,,,,,,,green
3,,,,,,,,,,,,
</code></pre>
<p>For more complex types such as arrays, structs, JSON, and GeoJSON data, we embed JSON into the CSV file:</p>
<pre><code class="language-csv">test_null,test_not_null,test_bool,test_bool_array,test_date,test_date_array,test_float32,test_float32_array,test_float64,test_float64_array,test_geojson,test_geojson_3857,test_int16,test_int16_array,test_int32,test_int32_array,test_int64,test_int64_array,test_json,test_text,test_text_array,test_timestamp_without_time_zone,test_timestamp_without_time_zone_array,test_timestamp_with_time_zone,test_timestamp_with_time_zone_array,test_uuid,test_uuid_array,test_enum
,hi,t,&quot;[true,false]&quot;,1969-07-20,&quot;[&quot;&quot;1969-07-20&quot;&quot;]&quot;,1e+37,&quot;[1e-37,0,100.125,1e+37]&quot;,1e+37,&quot;[1e-37,0,1000.125,1e+37]&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,16,&quot;[-32768,0,32767]&quot;,32,&quot;[-2147483648,0,2147483647]&quot;,64,&quot;[&quot;&quot;-9223372036854775808&quot;&quot;,&quot;&quot;0&quot;&quot;,&quot;&quot;9223372036854775807&quot;&quot;]&quot;,&quot;{&quot;&quot;x&quot;&quot;: 1, &quot;&quot;y&quot;&quot;: 2}&quot;,hello,&quot;[&quot;&quot;hello&quot;&quot;,&quot;&quot;&quot;&quot;]&quot;,1969-07-20T20:17:39.5,&quot;[&quot;&quot;1969-07-20T20:17:39.5&quot;&quot;]&quot;,1969-07-20T20:17:39.5Z,&quot;[&quot;&quot;1969-07-20T20:17:39.5Z&quot;&quot;]&quot;,084ec3bb-3193-4ffb-8b74-99a288e8432c,&quot;[&quot;&quot;084ec3bb-3193-4ffb-8b74-99a288e8432c&quot;&quot;]&quot;,red
</code></pre>
<h2 id="tricks-for-preparing-csv-data"><a class="header" href="#tricks-for-preparing-csv-data">Tricks for preparing CSV data</a></h2>
<p>If your input CSV files use an incompatible format, there are several things that might help. If your CSV files are invalid, non-standard, or full of junk, then you may be able to use <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> or <a href="https://github.com/BurntSushi/xsv"><code>xsv</code></a> to fix the worst problems.</p>
<p>If you need to clean up your data manually, then you may want to consider using <code>dbcrossbar</code> to load your data into BigQuery, and set your columns to type <code>STRING</code>. Once this is done, you can parse and normalize your data quickly using SQL queries.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="portable-table-schema"><a class="header" href="#portable-table-schema">Portable table schema</a></h1>
<p>Internally, <code>dbcrossbar</code> uses a portable table &quot;schema&quot; format. This provides a common ground between PostgreSQL's <code>CREATE TABLE</code> statements, <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery's JSON schemas</a>, and equivalent formats for other databases. For more information, see:</p>
<ul>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/index.html">The <code>dbcrossbar</code> schema format</a>.</li>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/enum.DataType.html">The <code>dbcrossbar</code> column types</a>.</li>
</ul>
<p>All table schemas and column types are converted into the portable format and then into the appropriate destination format.</p>
<p>Normally, you won't need to work with this schema format directly, because <code>dbcrossbar</code> can parse BigQuery schemas, PostgreSQL <code>CREATE TABLE</code> statments, and several other popular schema formats. It can also read schemas directly from some databases. See the <a href="./conv.html"><code>conv</code> command</a> for details.</p>
<h2 id="example-schema"><a class="header" href="#example-schema">Example schema</a></h2>
<pre><code class="language-json">{
    &quot;named_data_types&quot;: [
        {
            &quot;name&quot;: &quot;format&quot;,
            &quot;data_type&quot;: {
                &quot;one_of&quot;: [
                    &quot;gif&quot;,
                    &quot;jpeg&quot;
                ]
            }
        }
    ],
    &quot;tables&quot;: [
        {
            &quot;name&quot;: &quot;images&quot;,
            &quot;columns&quot;: [
                {
                    &quot;name&quot;: &quot;id&quot;,
                    &quot;is_nullable&quot;: false,
                    &quot;data_type&quot;: &quot;uuid&quot;
                },
                {
                    &quot;name&quot;: &quot;url&quot;,
                    &quot;is_nullable&quot;: false,
                    &quot;data_type&quot;: &quot;text&quot;
                },
                {
                    &quot;name&quot;: &quot;format&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: {
                        &quot;named&quot;: &quot;format&quot;
                    }
                },
                {
                    &quot;name&quot;: &quot;metadata&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: &quot;json&quot;
                },
                {
                    &quot;name&quot;: &quot;thumbnails&quot;,
                    &quot;is_nullable&quot;: true,
                    &quot;data_type&quot;: {
                        &quot;array&quot;: {
                            &quot;struct&quot;: [
                                {
                                    &quot;name&quot;: &quot;url&quot;,
                                    &quot;is_nullable&quot;: false,
                                    &quot;data_type&quot;: &quot;text&quot;
                                },
                                {
                                    &quot;name&quot;: &quot;width&quot;,
                                    &quot;data_type&quot;: &quot;float64&quot;,
                                    &quot;is_nullable&quot;: false
                                },
                                {
                                    &quot;name&quot;: &quot;height&quot;,
                                    &quot;data_type&quot;: &quot;float64&quot;,
                                    &quot;is_nullable&quot;: false
                                }
                            ]
                        }
                    }
                }
            ]
        }
    ]
}
</code></pre>
<h2 id="schema-properties"><a class="header" href="#schema-properties">Schema properties</a></h2>
<ul>
<li><code>named_data_types</code> (experimental): Named data types. These are used to declare custom column types. They're analogous to a Postgres <code>CREATE TYPE</code> statement, or a TypeScript <code>interface</code> or <code>type</code> statement. Two types with different names but the same layout are considered to be different types (<a href="https://en.wikipedia.org/wiki/Nominal_type_system">nominal typing</a>). Many database drivers place restrictions on these types for now, but we hope to relax those restrictions in the future.</li>
<li><code>tables</code>: A list of table definitions. For now, this must contain exactly one element.</li>
</ul>
<h2 id="table-properties"><a class="header" href="#table-properties">Table properties</a></h2>
<ul>
<li><code>name</code>: The name of this table. This is normally only used when serializing to schema formats that require a table name.</li>
<li><code>columns</code>: A list of columns in the table.</li>
</ul>
<h2 id="column-properties"><a class="header" href="#column-properties">Column properties</a></h2>
<ul>
<li><code>name</code>: The name of the column.</li>
<li><code>is_nullable</code>: Can the column contain <code>NULL</code> values?</li>
<li><code>data_type</code>: The type of data stored in the column.</li>
</ul>
<h2 id="data-types"><a class="header" href="#data-types">Data types</a></h2>
<p>The <code>data_type</code> field can contain any of:</p>
<ul>
<li><code>{ &quot;array&quot;: element_type }</code>: An array of <code>element_type</code> values.</li>
<li><code>&quot;bool&quot;</code>: A boolean value.</li>
<li><code>&quot;date&quot;</code>: A date, with no associated time value.</li>
<li><code>&quot;decimal&quot;</code>: A decimal integer (can represent currency, etc., without rounding errors).</li>
<li><code>&quot;float32&quot;</code>: A 32-bit floating point number.</li>
<li><code>&quot;float64&quot;</code>: A 64-bit floating point number.</li>
<li><code>{ &quot;geojson&quot;: srid }</code>: Geodata in GeoJSON format, using the specified <a href="https://en.wikipedia.org/wiki/Spatial_reference_system">SRID</a>, to specify the spatial reference system.</li>
<li><code>&quot;int16&quot;</code>: A 16-bit signed integer.</li>
<li><code>&quot;int32&quot;</code>: A 32-bit signed integer.</li>
<li><code>&quot;int64&quot;</code>: A 64-bit signed integer.</li>
<li><code>&quot;json&quot;</code>: An arbitrary JSON value.</li>
<li><code>{ &quot;named&quot;: name }</code>: A named type from the <code>named_types</code> list in this schema.</li>
<li><code>{ &quot;one_of&quot;: string_list }</code>: One of the specified string values. This may be represented a string enumeration by certain drivers, or as a &quot;categorical&quot; value in machine-learning systems (as opposed to a free-form textual value). <strong>NOTE:</strong> In many cases, it would be more efficient to have a separate table with an <code>(id, value)</code> entry for each enum value, and to refer to it using a foreign key. <code>one_of</code> is most useful when importing cleaned data, and when working with databases that support efficient string enumerations.</li>
<li><code>{ &quot;struct&quot;: fields }</code>: A structure with a list of specific, named fields. Each field has the following properties:
<ul>
<li><code>name</code>: The name of the field.</li>
<li><code>is_nullable</code>: Can the field contain <code>NULL</code> values?</li>
<li><code>data_type</code>: The type of data stored in the field.</li>
</ul>
</li>
<li><code>&quot;text&quot;</code>: A string.</li>
<li><code>&quot;timestamp_without_time_zone&quot;</code>: A date and time without an associated timezone.</li>
<li><code>&quot;timestamp_with_time_zone&quot;</code>: A date and time with an associated timezone.</li>
<li><code>&quot;uuid&quot;</code>: A UUID value.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuring-dbcrossbar"><a class="header" href="#configuring-dbcrossbar">Configuring <code>dbcrossbar</code></a></h1>
<p><code>dbcrossbar</code> can read information from a configuration directory. By default, this can be found at:</p>
<ul>
<li>Linux: <code>~/.config</code></li>
<li>MacOS: <code>~/Library/Preferences</code></li>
</ul>
<p>To override this default location, you can set <code>DBCROSSBAR_CONFIG_DIR</code> to point to an alternate configuration directory.</p>
<p>If a file <code>dbcrossbar.toml</code> appears in this directory, <code>dbcrossbar</code> will read its configuration from that file. Other files may be placed in this directory, including certain local credential files.</p>
<h2 id="modifying-the-configuration-file"><a class="header" href="#modifying-the-configuration-file">Modifying the configuration file</a></h2>
<p>You can modify the <code>dbcrossbar.toml</code> file from the command line using the <code>config</code> subcommand. For example:</p>
<pre><code class="language-sh">dbcrossbar config add temporary s3://example/temp/
dbcrossbar config rm temporary s3://example/temp/
</code></pre>
<p>Using <code>config add temporary</code> allows you to specify default values for <code>--temporary</code> flags. You can still override specific defaults by passing <code>--temporary</code> to commands that use it.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="commands"><a class="header" href="#commands">Commands</a></h1>
<p><code>dbcrossbar</code> supports four main subcommands:</p>
<ul>
<li><code>dbcrossbar cp</code>: Copy tabular data.</li>
<li><code>dbcrossbar count</code>: Count records.</li>
<li><code>dbcrossbar schema conv</code>: Convert table schemas between databases.</li>
</ul>
<p>For more information, type <code>dbcrossbar --help</code> or <code>dbcrossbar $CMD --help</code>.</p>
<p>Not all drivers support all the features of each command. To see the available drivers and what commands they support, run <code>dbcrossbar features</code> and <code>dbcrossbar features $DRIVER_NAME</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cp-copying-tables"><a class="header" href="#cp-copying-tables"><code>cp</code>: Copying tables</a></h1>
<p>The <code>cp</code> command copies tabular data from a source location to a destination location. For example, we can copy a CSV file into PostgreSQL, replacing any existing table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
<p>Or we copy data from PostgreSQL and upsert it into a BigQuery table:</p>
<pre><code class="language-sh">dbcrossbar config add temporary gs://$GS_TEMP_BUCKET
dbcrossbar config add temporary bigquery:$GCLOUD_PROJECT:temp_dataset
dbcrossbar cp \
    --if-exists=upsert-on:id \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table
</code></pre>
<h2 id="command-line-help"><a class="header" href="#command-line-help">Command-line help</a></h2>
<pre><code class="language-txt">Copy tables from one location to another

USAGE:
    dbcrossbar cp [FLAGS] [OPTIONS] &lt;from-locator&gt; &lt;to-locator&gt;

FLAGS:
        --display-output-locators
            Display where we wrote our output data

    -h, --help                       Prints help information
    -V, --version                    Prints version information

OPTIONS:
        --from-arg &lt;from-args&gt;...
            Pass an extra argument of the form `key=value` to the
            source driver
        --if-exists &lt;if-exists&gt;
            One of `error`, `overwrite`, `append` or `upsert-on:COL`
            [default: error]
    -J, --max-streams &lt;max-streams&gt;
            How many data streams should we attempt to copy in
            parallel? [default: 4]
        --schema &lt;schema&gt;
            The schema to use (defaults to input table schema)

        --stream-size &lt;stream-size&gt;
            Specify the approximate size of the CSV streams
            manipulated by `dbcrossbar`. This can be used to split a
            large input into multiple smaller outputs. Actual data
            streams may be bigger or smaller depending on a number of
            factors. Examples: &quot;100000&quot;, &quot;1Gb&quot;
        --temporary &lt;temporaries&gt;...
            Temporary directories, cloud storage buckets, datasets to
            use during transfer (can be repeated)
        --to-arg &lt;to-args&gt;...
            Pass an extra argument of the form `key=value` to the
            destination driver
        --where &lt;where-clause&gt;
            SQL where clause specifying rows to use


ARGS:
    &lt;from-locator&gt;    The input table
    &lt;to-locator&gt;      The output table

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table
</code></pre>
<h2 id="flags"><a class="header" href="#flags">Flags</a></h2>
<p>Not all command-line options are supported by all drivers. See the chapter on each driver for details.</p>
<h3 id="--where"><a class="header" href="#--where"><code>--where</code></a></h3>
<p>Specify a <code>WHERE</code> clause to include in the SQL query. This can be used to select a subset of the source rows.</p>
<h3 id="--from-arg"><a class="header" href="#--from-arg"><code>--from-arg</code></a></h3>
<p>This can be used to specify driver-specific options for the source driver. See the chapter for that driver.</p>
<h3 id="--if-existserror"><a class="header" href="#--if-existserror"><code>--if-exists=error</code></a></h3>
<p>If the destination location already contains data, exit with an error.</p>
<h3 id="--if-existsappend"><a class="header" href="#--if-existsappend"><code>--if-exists=append</code></a></h3>
<p>If the destination location already contains data, append the new data.</p>
<h3 id="--if-existsoverwrite"><a class="header" href="#--if-existsoverwrite"><code>--if-exists=overwrite</code></a></h3>
<p>If the destination location already contains data, replace it with the new data.</p>
<h3 id="--if-existsupset-oncol1"><a class="header" href="#--if-existsupset-oncol1"><code>--if-exists=upset-on:COL1,..</code></a></h3>
<p>For every row in the new data:</p>
<ul>
<li>If a row with a matching <code>col1</code>, <code>col2</code>, etc., exists, use the new data to update that row.</li>
<li>If no row matching <code>col1</code>, <code>col2</code>, etc., exists, then insert the new row.</li>
</ul>
<p>The columns <code>col1</code>, <code>col2</code>, etc., must be marked as <code>NOT NULL</code>.</p>
<h3 id="--schema"><a class="header" href="#--schema"><code>--schema</code></a></h3>
<p>By default, <code>dbcrossbar</code> will use the schema of the source table. But when this can't be inferred automatically, <code>--schema</code> can be used to specify a table schema:</p>
<ul>
<li><code>--schema=postgres-sql:my_table.sql</code>: A PostgreSQL <code>CREATE TABLE</code> statement.</li>
<li><code>--schema=bigquery-schema:my_table.json</code>: A <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery JSON schema</a>.</li>
<li><code>--schema=dbcrossbar-schema:my_table.json</code>: An <a href="./schema.html">internal <code>dbcrossbar</code> schema</a>.</li>
</ul>
<p>It's also possible to use a schema from an existing database table:</p>
<ul>
<li><code>--schema=postgres://localhost:5432/db#table</code></li>
<li><code>--schema=bigquery:project:dataset.table</code></li>
</ul>
<p>Note that it's possible to create a BigQuery table using a PostgreSQL schema, or vice versa. Internally, all schemes are first converted to the <a href="./schema.html">internal schema format</a>.</p>
<h3 id="--temporary"><a class="header" href="#--temporary"><code>--temporary</code></a></h3>
<p>Specify temporary storage, which is required by certain drivers. Typical values include:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code></li>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code></li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<h3 id="--to-arg"><a class="header" href="#--to-arg"><code>--to-arg</code></a></h3>
<p>This can be used to specify driver-specific options for the destination driver. See the chapter for that driver.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="count-counting-records"><a class="header" href="#count-counting-records">count: Counting records</a></h1>
<p>This command mostly works like the <a href="./cp.html"><code>cp</code></a> command, except that it prints out a number of rows. Check your driver to see if it supports <code>count</code>.</p>
<h2 id="command-line-help-1"><a class="header" href="#command-line-help-1">Command-line help</a></h2>
<pre><code class="language-txt">Count records

USAGE:
    dbcrossbar count [OPTIONS] &lt;locator&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
        --from-arg &lt;from-args&gt;...
            Pass an extra argument of the form `key=value` to the
            source driver
        --schema &lt;schema&gt;
            The schema to use (defaults to input table schema)

        --temporary &lt;temporaries&gt;...
            Temporary directories, cloud storage buckets, datasets to
            use during transfer (can be repeated)
        --where &lt;where-clause&gt;
            SQL where clause specifying rows to use


ARGS:
    &lt;locator&gt;    The locator specifying the records to count

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-conv-transforming-schemas"><a class="header" href="#schema-conv-transforming-schemas"><code>schema conv</code>: Transforming schemas</a></h1>
<p>The <code>schema conv</code> command can be used to convert between different database schemas. To convert from a PostgreSQL <code>CREATE TABLE</code> statement to a BigQuery schema, run:</p>
<pre><code class="language-sh">dbcrossbar schema conv postgres-sql:table.sql bigquery-schema:table.json
</code></pre>
<p>As a handy trick, you can also use a CSV source, which will generate a <code>CREATE TABLE</code> where all columns have the type <code>TEXT</code>:</p>
<pre><code class="language-sh">dbcrossbar schema conv csv:data.csv postgres-sql:table.sql
</code></pre>
<p>This can then be edited to specify appropriate column types.</p>
<h2 id="command-line-help-2"><a class="header" href="#command-line-help-2">Command-line help</a></h2>
<pre><code class="language-txt">Convert table schemas from one format to another

USAGE:
    dbcrossbar schema conv [OPTIONS] &lt;from-locator&gt; &lt;to-locator&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
        --if-exists &lt;if-exists&gt;
            One of `error`, `overrwrite` or `append` [default: error]


ARGS:
    &lt;from-locator&gt;    The input schema
    &lt;to-locator&gt;      The output schema

EXAMPLE LOCATORS:
    postgres-sql:table.sql
    postgres://localhost:5432/db#table
    bigquery-schema:table.json
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drivers"><a class="header" href="#drivers">Drivers</a></h1>
<p><code>dbcrossbar</code> uses built-in &quot;drivers&quot; to read and write CSV data and table schemas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigml"><a class="header" href="#bigml">BigML</a></h1>
<p><a href="https://bigml.com/">BigML</a> is a hosted machine-learning service, with support for many common algorithms and server-side batch scripts.</p>
<h2 id="example-locators"><a class="header" href="#example-locators">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>bigml:dataset/$ID</code>: Read data from a BigML dataset.</li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>bigml:source</code>: Create a single BigML &quot;source&quot; resource from the input data.</li>
<li><code>bigml:sources</code>: Create multiple BigML &quot;source&quot; resources from the input data.</li>
<li><code>bigml:dataset</code>: Create a single BigML &quot;dataset&quot; resource from the input data.</li>
<li><code>bigml:datasets</code>: Create multiple BigML &quot;dataset&quot; resources from the input data.</li>
</ul>
<p>If you use BigML as a destination, <code>dbcrossbar</code> will automatically activate <code>--display-output-locators</code>, and it will print locators for all the created resources on standard output. Column types on created &quot;source&quot; resources will be set something appropriate (but see <code>optype_for_text</code> below.)</p>
<h2 id="configuration--authentication"><a class="header" href="#configuration--authentication">Configuration &amp; authentication</a></h2>
<p>The BigML driver requires more configuration than most.</p>
<p>You'll need to set the following environment variables:</p>
<ul>
<li><code>BIGML_USERNAME</code>: Set this to your BigML username.</li>
<li><code>BIGML_API_KEY</code>: Set this to your BigML API key.</li>
<li><code>BIGML_DOMAIN</code> (optional): Set this to the domain name of your BigML instance, if it's not located at the standard address.</li>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials when using BigML as a destination. Do <strong>not</strong> set <code>AWS_SESSION_TOKEN</code>; it will not work with BigML.</li>
</ul>
<p>You'll also need to pass the following on the command line when using:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading into BigML. This is not needed when using BigML as a source.</li>
</ul>
<p>You can also specify the following <code>--to-arg</code> values:</p>
<ul>
<li><code>name</code>: The human-readable name of the resource to create.</li>
<li><code>optype_for_text</code>: The BigML optype to use for text fields. This defaults to <code>text</code>. You may want to set it to <code>categorical</code> if your text fields contain a small set of fixed strings. But you should probably use <code>dbcrossbar</code>'s <code>{ &quot;one_of&quot;: string_list }</code> types instead, which will always map to <code>categorical</code>.</li>
<li><code>tag</code>: This may be specified repeatedly to attach tags to the created resources.</li>
</ul>
<h2 id="supported-features"><a class="header" href="#supported-features">Supported features</a></h2>
<pre><code class="language-txt">bigml features:
- conv FROM
- cp FROM:
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<p>Note that <code>--if-exists</code> is simply ignored, because BigML will always create new resources.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigquery"><a class="header" href="#bigquery">BigQuery</a></h1>
<p>Google's <a href="https://cloud.google.com/bigquery/">BigQuery</a> is a extremely scalable data warehouse that supports rich SQL queries and petabytes of data. If you need to transform or analyze huge data sets, it's an excellent tool.</p>
<p>When loading data into BigQuery, or extracting it, we always go via Google Cloud Storage. This is considerably faster than the load and extract functionality supplied by tools like <code>bq</code>.</p>
<p><strong>COMPATIBILITY WARNING:</strong> This driver currently relies on <code>gsutil</code> and <code>bq</code> for many tasks, but those tools are poorly-suited to the kind of automation we need. In particular, <code>gsutil</code> uses too much RAM, and <code>bq</code> sometimes print status messages on standard output instead of standard error. We plan to replace those tools with native Rust libraries at some point. This will change how the BigQuery driver handles authentication in a future version.</p>
<h2 id="example-locators-1"><a class="header" href="#example-locators-1">Example locators</a></h2>
<ul>
<li><code>bigquery:$PROJECT:$DATASET.$TABLE</code>: A BigQuery table.</li>
</ul>
<h2 id="configuration--authentication-1"><a class="header" href="#configuration--authentication-1">Configuration &amp; authentication</a></h2>
<p>See <a href="./gs.html#configuration--authentication">the Cloud Storage driver</a> for authentication details.</p>
<p>The following command-line options will usually need to be specified for both sources and destinations:</p>
<ul>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code>: A Google Cloud Storage bucket to use for staging data in both directions.</li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<p>You can also specify Google Cloud resource labels to apply to all BigQuery jobs. Labels are often used to track query costs.</p>
<ul>
<li><code>--from-arg=job_labels[department]=marketing</code></li>
<li><code>--to-arg=job_labels[project]=project1</code></li>
</ul>
<h2 id="supported-features-1"><a class="header" href="#supported-features-1">Supported features</a></h2>
<pre><code class="language-txt">bigquery features:
- conv FROM
- count
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp FROM:
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="csv"><a class="header" href="#csv">CSV</a></h1>
<p><code>dbcrossbar</code> works with valid CSV files in our <a href="./csv_interchange.html">CSV interchange format</a>. For invalid CSV files, take a look at <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a>. For CSV files which need further transformation and parsing, considering loading them into BigQuery and cleaning them up using SQL. This works very well even for large datasets.</p>
<h2 id="example-locators-2"><a class="header" href="#example-locators-2">Example locators</a></h2>
<p>The following locators can be used for both input and output:</p>
<ul>
<li><code>csv:file.csv</code>: A single CSV file.</li>
<li><code>csv:dir/</code>: A directory tree containing CSV files.</li>
<li><code>csv:-</code>: Read from standard input, or write to standard output.</li>
</ul>
<p>To concatenate CSV files, use:</p>
<pre><code class="language-sh">dbcrossbar cp csv:input/ csv:merged.csv
</code></pre>
<p>To split a CSV file, use <code>--stream-size</code>:</p>
<pre><code class="language-sh">dbcrossbar cp --stream-size=&quot;100Mb&quot; csv:giant.csv csv:split/
</code></pre>
<h2 id="configuration--authentication-2"><a class="header" href="#configuration--authentication-2">Configuration &amp; authentication</a></h2>
<p>None.</p>
<h2 id="supported-features-2"><a class="header" href="#supported-features-2">Supported features</a></h2>
<pre><code class="language-txt">csv features:
- conv FROM
- cp FROM:
- cp TO:
  --if-exists=error --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="google-cloud-storage"><a class="header" href="#google-cloud-storage">Google Cloud Storage</a></h1>
<p>Google Cloud Storage is a bucket-based storage system similar to Amazon's S3. It's frequently used in connection with BigQuery and other Google Cloud services.</p>
<h2 id="example-locators-3"><a class="header" href="#example-locators-3">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>gs://bucket/dir/file.csv</code></li>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<h2 id="configuration--authentication-3"><a class="header" href="#configuration--authentication-3">Configuration &amp; authentication</a></h2>
<p><strong>0.4.x and later:</strong> You can authenticate using either a client secret or a service key, which you can create using the <a href="https://console.cloud.google.com/apis/credentials">console credentials page</a>.</p>
<ul>
<li>Client secrets can be stored in <code>$DBCROSSBAR_CONFIG_DIR/gcloud_client_secret.json</code> or in <code>GCLOUD_CLIENT_SECRET</code>. These are strongly recommended for interactive use.</li>
<li>Service account keys can be stored in <code>$DBCROSSBAR_CONFIG_DIR/gcloud_service_account_key.json</code> or in <code>GCLOUD_SERVICE_ACCOUNT_KEY</code>. These are recommended for server and container use.</li>
</ul>
<p>For more information on <code>DBCROSSBAR_CONFIG_DIR</code>, see <a href="./config.html">Configuration</a>.</p>
<p>For a service account, you can use the following permissions:</p>
<ul>
<li>Storage Object Admin (Cloud Storage and BigQuery drivers)</li>
<li>BigQuery Data Editor (BigQuery driver only)</li>
<li>BigQuery Job User (BigQuery driver only)</li>
<li>BigQuery User (BigQuery driver only)</li>
</ul>
<p>There's probably a more limited set of permissions which will work if you set them up manually.</p>
<h2 id="supported-features-3"><a class="header" href="#supported-features-3">Supported features</a></h2>
<pre><code class="language-txt">gs features:
- cp FROM:
- cp TO:
  --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="postgresql"><a class="header" href="#postgresql">PostgreSQL</a></h1>
<p><a href="https://www.postgresql.org/">PostgreSQL</a> is an excellent general-purpose SQL database.</p>
<h2 id="example-locators-4"><a class="header" href="#example-locators-4">Example locators</a></h2>
<p><code>dbcrossbar</code> supports standard PostgreSQL locators followed by <code>#table_name</code>:</p>
<ul>
<li><code>postgres://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<p>Note that PostgreSQL sources will currently output all data as a single stream. This can be split into multiple streams using the <code>--stream-size</code> option if desired.</p>
<h2 id="configuration--authentication-4"><a class="header" href="#configuration--authentication-4">Configuration &amp; authentication</a></h2>
<p>Authentication is currently handled using standard <code>postgres://user:pass@...</code> syntax, similar to <code>psql</code>. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<h2 id="supported-features-4"><a class="header" href="#supported-features-4">Supported features</a></h2>
<pre><code class="language-txt">postgres features:
- conv FROM
- count
  --where=$SQL_EXPR
- cp FROM:
  --where=$SQL_EXPR
- cp TO:
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redshift"><a class="header" href="#redshift">RedShift</a></h1>
<p>Amazon's <a href="https://aws.amazon.com/redshift/">Redshift</a> is a cloud-based data warehouse designed to support analytical queries. This driver receives less testing than our BigQuery driver, because the cheapest possible RedShift test system costs over $100/month. Sponsors are welcome!</p>
<h2 id="example-locators-5"><a class="header" href="#example-locators-5">Example locators</a></h2>
<p>These are identical to <a href="./postgres.html#example-locators">PostgreSQL locators</a>, except that <code>postgres</code> is replaced by <code>redshift</code>:</p>
<ul>
<li><code>redshift://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<h2 id="configuration--authentication-5"><a class="header" href="#configuration--authentication-5">Configuration &amp; authentication</a></h2>
<p>Authentication is currently handled using the <code>redshift://user:pass@...</code> syntax. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<p>The following environment variables are required.</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): This should work, but it hasn't been tested.</li>
</ul>
<p>The following <code>--temporary</code> flag is required:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading or unloading data.</li>
</ul>
<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/loading-data-access-permissions.html">Authentication credentials for <code>COPY</code></a> may be passed using <code>--to-arg</code>. For example:</p>
<ul>
<li><code>--to-arg=iam_role=$ROLE</code></li>
<li><code>--to-arg=region=$REGION</code></li>
</ul>
<p>This may require some experimentation.</p>
<p>If you need to generate &quot;-- partner:&quot; SQL comments for an AWS RedShift partner program, you can do it as follows:</p>
<ul>
<li><code>--to-arg=partner=&quot;myapp v1.0&quot;</code></li>
</ul>
<h2 id="supported-features-5"><a class="header" href="#supported-features-5">Supported features</a></h2>
<pre><code class="language-txt">redshift features:
- conv FROM
- cp FROM:
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=append --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="s3"><a class="header" href="#s3">S3</a></h1>
<p>Amazon's <a href="https://aws.amazon.com/s3/">S3</a> is a bucket-based system for storing data in the cloud.</p>
<h2 id="example-locators-6"><a class="header" href="#example-locators-6">Example locators</a></h2>
<p>Source locators:</p>
<ul>
<li><code>s3://bucket/dir/file.csv</code></li>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<h2 id="configuration--authentication-6"><a class="header" href="#configuration--authentication-6">Configuration &amp; authentication</a></h2>
<p>The following environment variables are used to authenticate:</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> (required): The ID for your AWS credentials.</li>
<li><code>AWS_SECRET_ACCESS_KEY</code> (required): The secret part of your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): Set this to use temporary AWS crdentials.</li>
<li><code>AWS_DEFAULT_REGION</code> (required): Set this to your AWS region.</li>
</ul>
<h2 id="supported-features-6"><a class="header" href="#supported-features-6">Supported features</a></h2>
<pre><code class="language-txt">s3 features:
- cp FROM:
- cp TO:
  --if-exists=overwrite
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shopify-unstable"><a class="header" href="#shopify-unstable">Shopify (UNSTABLE)</a></h1>
<p><strong>WARNING:</strong> This is highly experimental and subject to change. To use it, you must enable it using the <code>--enable-unstable</code> flag.</p>
<p>Shopify is an online e-commerce platform with a REST API for fetching data.</p>
<h2 id="example-locators-7"><a class="header" href="#example-locators-7">Example locators</a></h2>
<p>Locators look just like Shopify REST API URLs, but with <code>https:</code> replaced with <code>shopify</code>:</p>
<ul>
<li><code>shopify://$SHOP/admin/api/2020-04/orders.json?status=any</code></li>
</ul>
<p>For a schema, download <a href="https://github.com/dbcrossbar/dbcrossbar/blob/master/dbcrossbar/fixtures/shopify.ts">shopify.ts</a>, and refer to it as follows:</p>
<ul>
<li><code>--schema=&quot;dbcrossbar-ts:shopify.ts#Order&quot;</code></li>
</ul>
<p>We do not currently include a default Shopify schema in <code>dbcrossbar</code> itself, because it's still undergoing significant changes.</p>
<h2 id="configuration--authentication-7"><a class="header" href="#configuration--authentication-7">Configuration &amp; authentication</a></h2>
<p>The following environment variables are required:</p>
<ul>
<li><code>SHOPIFY_AUTH_TOKEN</code>: The Shopify authorization token to use. (We don't yet support password authentication, but it would be easy enough to add.)</li>
</ul>
<h2 id="supported-features-7"><a class="header" href="#supported-features-7">Supported features</a></h2>
<pre><code class="language-txt">shopify features:
- cp FROM:

This driver is UNSTABLE and may change without warning.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-drivers"><a class="header" href="#schema-drivers">Schema drivers</a></h1>
<p><code>dbcrossbar</code> allows you to specify a table's column names and types in a number of different ways. You can use <a href="./postgres-sql.html">Postgres <code>CREATE TABLE</code> statements</a>, or <a href="./bigquery-schema.html">BigQuery schema JSON</a>, or <a href="./dbcrossbar-schema.html"><code>dbcrossbar</code>'s internal schema format</a>.</p>
<p>These schema formats are typically used in one of two ways:</p>
<ul>
<li>
<p>As a <code>--schema</code> argument to the <a href="./cp.html"><code>cp</code> subcommand</a>.</p>
<pre><code class="language-sh">dbcrossbar cp \
  --if-exists=overwrite \
  --schema=postgres-sql:my_table.sql \
  csv:my_table.csv \
  'postgres://postgres@127.0.0.1:5432/postgres#my_table'
</code></pre>
</li>
<li>
<p>As an argument to the <a href="./conv.html"><code>conv</code> subcommand</a>, which allows you to convert between different schema formats.</p>
<pre><code class="language-sh">dbcrossbar schema conv postgres-sql:table.sql bigquery-schema:table.json
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="postgres-create-table-statements"><a class="header" href="#postgres-create-table-statements">Postgres <code>CREATE TABLE</code> statements</a></h1>
<p>To specify the column names and types for table in SQL format, use:</p>
<pre><code class="language-txt">--schema postgres-sql:my_table.sql
</code></pre>
<p>The file <code>my_table.sql</code> can contain a single <code>CREATE TABLE</code> statement using a subset of PostgreSQL's syntax:</p>
<pre><code class="language-sql">CREATE TABLE my_table (
    id INT NOT NULL,
    name TEXT NOT NULL,
    quantity INT NOT NULL
);
</code></pre>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<p>This schema format offers support for singly-nested array types, and it doesn't support structure types at all.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bigquery-json-schemas"><a class="header" href="#bigquery-json-schemas">BigQuery JSON schemas</a></h1>
<p>To specify the column names and types for table in BigQuery JSON format, use:</p>
<pre><code class="language-txt">--schema bigquery-schema:my_table.json
</code></pre>
<p>The file <code>my_table.json</code> should be a <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery JSON schema file</a>:</p>
<pre><code class="language-json">[
  {
    &quot;name&quot;: &quot;id&quot;,
    &quot;type&quot;: &quot;INT64&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  },
  {
    &quot;name&quot;: &quot;name&quot;,
    &quot;type&quot;: &quot;STRING&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  },
  {
    &quot;name&quot;: &quot;quantity&quot;,
    &quot;type&quot;: &quot;INT64&quot;,
    &quot;mode&quot;: &quot;REQUIRED&quot;
  }
]
</code></pre>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<p>This schema format supports a small number of general types. For example, all integer types are represented as <code>INT64</code>, all floating-point types are represented as <code>FLOAT64</code>, and both JSON values and UUIDs are represented as <code>STRING</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="native-dbcrossbar-schemas"><a class="header" href="#native-dbcrossbar-schemas">Native <code>dbcrossbar</code> schemas</a></h1>
<p><code>dbcrossbar</code> supports a <a href="./schema.html">native schema format</a> that exactly represents all types supported by <code>dbcrossbar</code>. It can be used as follows:</p>
<pre><code class="language-txt">--schema dbcrossbar-schema:my_table.json
</code></pre>
<p>For more details and example, see the chaper on <a href="./schema.html">portable table schemas</a>.</p>
<h2 id="typical-uses"><a class="header" href="#typical-uses">Typical uses</a></h2>
<p>This format is cumbersome to edit by hand, but it is fairly useful in a number of circumstances:</p>
<ul>
<li>Specifying column types that can't be exactly represented by other schema formats.</li>
<li>Reading or editing schemas using scripts.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="typescript-schemas-unstable"><a class="header" href="#typescript-schemas-unstable">TypeScript schemas (UNSTABLE)</a></h1>
<p><strong>WARNING:</strong> This is highly experimental and subject to change. To use it, you must enable it using the <code>--enable-unstable</code> flag.</p>
<p>To specify the column names and types for table using a subset of TypeScript, use:</p>
<pre><code class="language-txt">--schema &quot;dbcrossbar-ts:my_table.ts#MyTable&quot;
</code></pre>
<p>The file <code>my_table.ts</code> can contain one or more <code>interface</code> definitions:</p>
<pre><code class="language-ts">interface MyTable {
    id: string,
    name: string,
    quantity: number,
}
</code></pre>
<h2 id="magic-types"><a class="header" href="#magic-types">&quot;Magic&quot; types</a></h2>
<p>Certain <code>dbcrossbar</code> types can be specified by adding the following declarations to a TypeScript file:</p>
<pre><code class="language-ts">// Decimal numbers which can exactly represent
// currency values with no rounding.
type decimal = number | string;

// Integers of various sizes.
type int16 = number | string;
type int32 = number | string;
type int64 = number | string;
</code></pre>
<p>These may then be used as follows:</p>
<pre><code class="language-ts">interface OrderItem {
    id: int64,
    sku: string,
    unit_price: decimal,
    quantity: int16,
}
</code></pre>
<p>When the TypeScript schema is converted to a portable <code>dbcrossbar</code> schema, the &quot;magic&quot; types will be replaced with the corresponding portable type.</p>
<h3 id="advanced-features"><a class="header" href="#advanced-features">Advanced features</a></h3>
<p>We also support nullable values, arrays and nested structures:</p>
<pre><code class="language-ts">type decimal = number | string;
type int16 = number | string;
type int32 = number | string;
type int64 = number | string;

interface Order {
    id: int64,
    line_items: OrderItem[],
    note: string | null,
}

interface OrderItem {
    id: int64,
    sku: string,
    unit_price: decimal,
    quantity: int16,
}
</code></pre>
<p>Nested arrays and structs will translate to appropriate database-specific types, such as BigQuery <code>ARRAY</code> and <code>STRUCT</code> types.</p>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>This schema format has a number of limitations:</p>
<ul>
<li>There's no way to convert other schema formats into this one (yet).</li>
<li>Some portable <code>dbcrossbar</code> types can't be represented in this format.</li>
<li>Only a small subset of TypeScript is supported (but we try to give good error messages).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="changelog"><a class="header" href="#changelog">Changelog</a></h1>
<p>All notable changes to this project will be documented in this file.</p>
<p>The format is based on <a href="https://keepachangelog.com/en/1.0.0/">Keep a Changelog</a>, and this project adheres to <a href="https://semver.org/spec/v2.0.0.html">Semantic Versioning</a> for the <code>dbcrossbar</code> CLI tool. (The <code>dbcrossbarlib</code> crate is an internal-only dependency with no versioning policy at this time.)</p>
<h2 id="050-beta3---2021-12-31"><a class="header" href="#050-beta3---2021-12-31">[0.5.0-beta.3] - 2021-12-31</a></h2>
<h3 id="fixed"><a class="header" href="#fixed">Fixed</a></h3>
<ul>
<li>bigquery: Retry <code>extract</code> operations after <code>accessDenied</code> errors. These errors sometimes occur even when BigQuery <em>should</em> have the necessary access permissions. This may be caused by a permissions race condition somewhere in Google Cloud?</li>
</ul>
<h2 id="050-beta2---2021-12-19"><a class="header" href="#050-beta2---2021-12-19">[0.5.0-beta.2] - 2021-12-19</a></h2>
<h3 id="changed"><a class="header" href="#changed">Changed</a></h3>
<ul>
<li>The format for printing backtraces has changed. We now use the standard <code>anyhow</code> format.</li>
<li>Our logging format has changed considerably, thanks to switch from <code>slog</code> to the much more popular <code>tracing</code>. Some logging of complex async streams may be less well-organized, but there's <em>more</em> of it, especially at <code>RUST_LOG=dbcrossbarlib=trace,dbcrossbar=trace,warn</code> level.</li>
</ul>
<h3 id="fixed-1"><a class="header" href="#fixed-1">Fixed</a></h3>
<ul>
<li>We no longer rely on the unmaintained <code>failure</code>, or the less popular <code>slog</code> ecosystem.</li>
</ul>
<h3 id="removed"><a class="header" href="#removed">Removed</a></h3>
<ul>
<li>The <code>--log-format</code> and <code>--log-extra</code> commands have been removed.</li>
</ul>
<h2 id="050-beta1---2021-12-15"><a class="header" href="#050-beta1---2021-12-15">[0.5.0-beta.1] - 2021-12-15</a></h2>
<h3 id="added"><a class="header" href="#added">Added</a></h3>
<ul>
<li>We provide MacOS X binaries for the new M1 processors. These are unsigned, like our existing Apple Intel binaries. So you'll need to continue to use <code>xattr -d com.apple.quarantine dbcrossbar</code> or a similar technique to run them. Or you could build your own binaries.</li>
</ul>
<h3 id="changed-1"><a class="header" href="#changed-1">Changed</a></h3>
<ul>
<li>Our downloadable <code>*.zip</code> files follow a new naming convention, allowing us to distinguish between Intel and M1 Macs.</li>
<li>OpenSSL has been completely removed from <code>dbcrossbar</code>, allowing us to support more platforms. This will also allow us to eventually centralize TLS configuration across all <code>dbcrossbar</code> drivers.</li>
</ul>
<h3 id="removed-1"><a class="header" href="#removed-1">Removed</a></h3>
<ul>
<li>We no longer support hosted Citus from Citus Data, because their TLS certificates do not include <code>subjectAltName</code>, which is <a href="https://github.com/briansmith/webpki/issues/11">required by the <code>rustls</code> library</a>. Citus Data will be shutting down shortly, so we recommend keeping around an older <code>dbcrossbar</code> for a few more weeks if you need to talk to them.</li>
</ul>
<h2 id="050-alpha3---2021-12-14"><a class="header" href="#050-alpha3---2021-12-14">[0.5.0-alpha.3] - 2021-12-14</a></h2>
<h3 id="added-1"><a class="header" href="#added-1">Added</a></h3>
<ul>
<li>gs: Allow single-file CSV output. This involves copying out of Google Cloud Storage, concatenating, and copying back. But it's handy when you need it.</li>
</ul>
<h3 id="changed-2"><a class="header" href="#changed-2">Changed</a></h3>
<ul>
<li>gs: Copying to a <code>gs://bucket/dir/</code> URL with <code>--if-exists=overwrite --display-output-locators</code> will print out a list of files in the destination bucket directory when we're done. Before, it just printed out the destination locator, which was technically allowed, but useless.</li>
</ul>
<h3 id="fixed-2"><a class="header" href="#fixed-2">Fixed</a></h3>
<ul>
<li>Updated many dependencies, fixing several CVEs (none known to be meaningfully exploitable in typical use cases), and possibly some library bugs.</li>
</ul>
<h2 id="050-alpha2---2021-04-27"><a class="header" href="#050-alpha2---2021-04-27">[0.5.0-alpha.2] - 2021-04-27</a></h2>
<h3 id="fixed-3"><a class="header" href="#fixed-3">Fixed</a></h3>
<ul>
<li>bigml: Always map columns with a <code>one-of</code> type (aka <code>CREATE ENUM</code>) to BigML <code>categorical</code> columns.</li>
</ul>
<h2 id="050-alpha1---2021-03-04"><a class="header" href="#050-alpha1---2021-03-04">[0.5.0-alpha.1] - 2021-03-04</a></h2>
<p>This release contains a breaking change to the <code>dbcrossbar-schema</code> output format to enable supporting named types and enumeration types. See below.</p>
<h3 id="added-2"><a class="header" href="#added-2">Added</a></h3>
<ul>
<li>
<p>(EXPERIMENTAL) postgres: The <code>postgres-sql</code> and <code>postgres</code> drivers now <code>CREATE TYPE</code> statements (but only in the <code>&quot;public&quot;</code> schema). These can be used as follows:</p>
<pre><code class="language-sql">CREATE TYPE &quot;format&quot; AS ENUM ('gif', 'jpeg');
CREATE TABLE &quot;images&quot; (
    &quot;id&quot; uuid NOT NULL,
    &quot;url&quot; text NOT NULL,
    &quot;image_format&quot; &quot;format&quot;,
    &quot;metadata&quot; jsonb
);
</code></pre>
<p>This change also requires some changes to the <code>dbcrossbar-schema</code> format, which are described below.</p>
</li>
<li>
<p>(EXPERIMENTAL) The native <code>dbcrossbar-schema</code> format now supports a set of <code>named_types</code> definitions. This allows named types to be defined once, and to then be referred to elsewhere using <code>{ &quot;named&quot;: &quot;my_custom_type&quot; }</code>.</p>
</li>
<li>
<p>(EXPERIMENTAL) The native <code>dbcrossbar-schema</code> format also supports string enumeration types using a <code>{ &quot;one_of&quot;: [&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;] }</code> syntax.</p>
</li>
</ul>
<h3 id="changed-3"><a class="header" href="#changed-3">Changed</a></h3>
<ul>
<li>BREAKING: The <code>dbcrossbar-schema</code> output format has changed! It now has top level <code>named_types</code> and <code>tables</code> members, and the old top-level table definition is now available as <code>.tables[0]</code>. See <a href="https://www.dbcrossbar.org/schema.html">the manual</a> for more details. However, <code>dbcrossbar</code> can still read the old input format with no problems, so this only affects other programs that parse native <code>dbcrossbar</code> schema.</li>
</ul>
<h3 id="fixed-4"><a class="header" href="#fixed-4">Fixed</a></h3>
<ul>
<li>The suggested fixes for RUSTSEC-2020-0146, RUSTSEC-2021-0020 and RUSTSEC-2021-0023 have been applied.</li>
</ul>
<h2 id="042-beta11---2021-02-03"><a class="header" href="#042-beta11---2021-02-03">0.4.2-beta.11 - 2021-02-03</a></h2>
<h3 id="fixed-5"><a class="header" href="#fixed-5">Fixed</a></h3>
<ul>
<li>gcloud: Retry OAuth2 failures for service accounts.</li>
</ul>
<h2 id="042-beta10---2021-02-02"><a class="header" href="#042-beta10---2021-02-02">0.4.2-beta.10 - 2021-02-02</a></h2>
<h3 id="fixed-6"><a class="header" href="#fixed-6">Fixed</a></h3>
<ul>
<li>Build fixes for recently-added clippy warnings.</li>
</ul>
<h2 id="042-beta9---2021-01-14"><a class="header" href="#042-beta9---2021-01-14">0.4.2-beta.9 - 2021-01-14</a></h2>
<h3 id="changed-4"><a class="header" href="#changed-4">Changed</a></h3>
<ul>
<li>Update many dependencies, including <code>tokio</code> and our many network-related libraries. Tests pass, but this affects almost everything, in one fashion or another.</li>
</ul>
<h2 id="042-beta8---2020-10-16"><a class="header" href="#042-beta8---2020-10-16">0.4.2-beta.8 - 2020-10-16</a></h2>
<h3 id="fixed-7"><a class="header" href="#fixed-7">Fixed</a></h3>
<ul>
<li>Linux: Fix Linux binary builds by updating to latest <code>rust-musl-builder</code> release, which has the new <code>cargo-deny</code>.</li>
</ul>
<h2 id="042-beta7---2020-10-14"><a class="header" href="#042-beta7---2020-10-14">0.4.2-beta.7 - 2020-10-14</a></h2>
<h3 id="added-3"><a class="header" href="#added-3">Added</a></h3>
<ul>
<li>shopify: Added a &quot;partner&quot; argument which can be used to include a &quot;-- partner:&quot; comment in all generated RedShift SQL for use by RedShift partners.</li>
</ul>
<h2 id="042-beta6---2020-09-15"><a class="header" href="#042-beta6---2020-09-15">0.4.2-beta.6 - 2020-09-15</a></h2>
<h3 id="fixed-8"><a class="header" href="#fixed-8">Fixed</a></h3>
<ul>
<li>shopify: Retry failed downloads a few times. We've been seeing some intermittent failures.</li>
</ul>
<h2 id="042-beta5---2020-08-01"><a class="header" href="#042-beta5---2020-08-01">0.4.2-beta.5 - 2020-08-01</a></h2>
<h3 id="fixed-9"><a class="header" href="#fixed-9">Fixed</a></h3>
<ul>
<li>gcloud: We now print more useful error messages when Google doesn't send JSON-formatted errors.</li>
<li>gcloud: We now retry Google Cloud GET requests automatically a few times if it looks like it might help. We'd also love to retry POST requests, but that will require the ability to try to restart streams.</li>
</ul>
<h2 id="042-beta4---2020-07-07"><a class="header" href="#042-beta4---2020-07-07">0.4.2-beta.4 - 2020-07-07</a></h2>
<h3 id="changed-5"><a class="header" href="#changed-5">Changed</a></h3>
<ul>
<li>Update dependencies. The latest <code>bigml</code> release contains tweaks to error retry behavior.</li>
</ul>
<h2 id="042-beta3---2020-07-07"><a class="header" href="#042-beta3---2020-07-07">0.4.2-beta.3 - 2020-07-07</a></h2>
<h3 id="changed-6"><a class="header" href="#changed-6">Changed</a></h3>
<ul>
<li>postgres: Our last <code>diesel</code> code has been removed, and replaced with <code>tokio-postgres</code> (which we use elsewhere).</li>
</ul>
<h3 id="fixed-10"><a class="header" href="#fixed-10">Fixed</a></h3>
<ul>
<li>postgres: Fixed <a href="https://github.com/dbcrossbar/dbcrossbar/issues/148">#148</a> to improve support for PostGIS under PostgreSQL 12.</li>
</ul>
<h3 id="removed-2"><a class="header" href="#removed-2">Removed</a></h3>
<ul>
<li>The experimental <code>citus</code>-related APIs have been removed from <code>dbcrossbarlib</code>, because they used <code>diesel</code>. This is technically a breaking change for <code>dbcrosslib</code>, but we don't claim to honor semver for <code>dbcrossbarlib</code> 0.x.y releases.</li>
</ul>
<h2 id="042-beta2---2020-06-28"><a class="header" href="#042-beta2---2020-06-28">0.4.2-beta.2 - 2020-06-28</a></h2>
<h3 id="added-4"><a class="header" href="#added-4">Added</a></h3>
<ul>
<li>redshift: Support <code>--if-exists=upsert-on:key1,key2</code>.</li>
<li>redshift: Enable <code>--if-exists=error</code>.</li>
</ul>
<h3 id="changed-7"><a class="header" href="#changed-7">Changed</a></h3>
<ul>
<li>postgres: Temporary tables now use the same schema (i.e. namespace) as the tables they're linked to. This shouldn't be a breaking change unless you've set up your database permissions to forbid it.</li>
</ul>
<h3 id="fixed-11"><a class="header" href="#fixed-11">Fixed</a></h3>
<ul>
<li>postgres: Fixed likely bug upserting into tables with a non-&quot;public&quot; schema.</li>
<li>postgres: Verify that upsert columns are NOT NULL to prevent possible incorrect upserts. This may be a breaking change, but it also prevents a possible bug.</li>
</ul>
<h2 id="042-beta1---2020-06-23"><a class="header" href="#042-beta1---2020-06-23">0.4.2-beta.1 - 2020-06-23</a></h2>
<h3 id="changed-8"><a class="header" href="#changed-8">Changed</a></h3>
<ul>
<li>Mac: Move configuration directory from <code>~/Library/Preferences/dbcrossbar</code> to <code>~/Library/Application Support/dbcrossbar</code>. If we detect a config directory in the old location, we should print a deprecation warning and use it.</li>
<li>Many dependencies have been updated.</li>
</ul>
<h3 id="fixed-12"><a class="header" href="#fixed-12">Fixed</a></h3>
<ul>
<li>We should now handle multiple sets of Google Cloud OAuth2 credentials correctly.</li>
</ul>
<h2 id="041---2020-06-16"><a class="header" href="#041---2020-06-16">0.4.1 - 2020-06-16</a></h2>
<p>A bug fix to <code>gs</code>, and other minor improvements.</p>
<h3 id="changed-9"><a class="header" href="#changed-9">Changed</a></h3>
<ul>
<li>Replace deprecated <code>tempdir</code> with <code>tempfile</code>.</li>
</ul>
<h3 id="fixed-13"><a class="header" href="#fixed-13">Fixed</a></h3>
<ul>
<li>gs: Correctly pass <code>page_token</code> when listing. This prevents an infinite loop in large directories.</li>
<li>Fix new Rust 0.44.0 warnings.</li>
</ul>
<h2 id="040---2020-06-02"><a class="header" href="#040---2020-06-02">0.4.0 - 2020-06-02</a></h2>
<p>This is a summary of all the major changes since the 0.3.3 release. For more details and minor changes, see the individual CHANGELOG entries for the 0.4.0 preleases.</p>
<h3 id="added-5"><a class="header" href="#added-5">Added</a></h3>
<ul>
<li><code>dbcrossbar</code> now supports &quot;struct&quot; types, which have a fixed set of named fields. These will be automatically translated to BigQuery STRUCT types or to JSON columns, depending on the destination database.</li>
<li>We now support a CLI-editable config file using commands like <code>dbcrossbar config add temporary s3://example/temp/</code>.</li>
<li>Parsing-related error messages should now include context.</li>
<li>bigquery: Users can now specify billing labels for jobs.</li>
<li><code>dbcrossbar license</code> will display the licences for all dependencies.</li>
<li>Unstable features can now be hidden behind the <code>--enable-unstable</code> flag, including two new drivers:
<ul>
<li>UNSTABLE: We now support specifying schemas using a subset of TypeScript.</li>
<li>UNSTABLE: We now support reading data from Shopify's REST API. This is a testbed for new struct and JSON-related features.</li>
</ul>
</li>
</ul>
<h3 id="changed-10"><a class="header" href="#changed-10">Changed</a></h3>
<ul>
<li><code>dbcrossbar conv</code> is now <code>dbcrossbar schema conv</code>.</li>
<li>Because of the new STRUCT support, some corner cases involving struct types and JSON may have changed subtly.</li>
<li>We replaced <code>gcloud auth</code>, <code>gsutil</code> and <code>bq</code> with native Rust. This simplifies installation and configuration substantially, and fixes a number of BigQuery-related issues.</li>
<li>AWS credentials must now always be passed via <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code> (optional) and <code>AWS_DEFAULT_REGION</code> (required). This lays the groundwork for replacing the <code>aws</code> CLI tool with native Rust code, so that we will someday be able to remove our last CLI dependency.</li>
</ul>
<h3 id="fixed-14"><a class="header" href="#fixed-14">Fixed</a></h3>
<ul>
<li>Lots of issues.</li>
</ul>
<h3 id="removed-3"><a class="header" href="#removed-3">Removed</a></h3>
<ul>
<li>The data type <code>{ &quot;other&quot;: string }</code> has been removed from the portable schema format. It was not actually generated by any of our drivers.</li>
<li>bigquery: We now export <code>ARRAY&lt;STRUCT&lt;...&gt;&gt;</code> as <code>{ &quot;array&quot;: { &quot;struct&quot;: ... } }</code>, instead of exporting it as as single <code>&quot;json&quot;</code> value.</li>
</ul>
<h2 id="040-rc2---2020-06-01"><a class="header" href="#040-rc2---2020-06-01">0.4.0-rc.2 - 2020-06-01</a></h2>
<h3 id="changed-11"><a class="header" href="#changed-11">Changed</a></h3>
<ul>
<li>postgres: We now transform the portable types <code>{ &quot;array&quot;: &quot;json&quot; }</code> and <code>{ &quot;array&quot;: { &quot;struct&quot;: fields } }</code> into <code>jsonb[]</code>, instead of automatically changing it to plain <code>jsonb</code> in an effort to help our users.</li>
</ul>
<h2 id="040-rc1---2020-05-31"><a class="header" href="#040-rc1---2020-05-31">0.4.0-rc.1 - 2020-05-31</a></h2>
<p>This is a release candidate for v0.4.0. If no issues are discovered, this will be published as 0.4.0.</p>
<p>This release contains a last few breaking changes that we want to include before we publicize <code>dbcrossbar</code> more widely. When migrating, particular attention to the <code>conv</code> subcommand and <code>AWS_DEFAULT_REGION</code> below, which have significant breaking changes.</p>
<h3 id="changed-12"><a class="header" href="#changed-12">Changed</a></h3>
<ul>
<li>Rename <code>dbcrossbar conv</code> to <code>dbcrossbar schema conv</code>.</li>
<li>s3: Require <code>AWS_DEFAULT_REGION</code> instead of optionally using <code>AWS_REGION</code>. This is more compatiable with the <code>aws</code> CLI command, and it doesn't rely on undocumented region defaults or <code>aws</code> configuration files.</li>
</ul>
<h3 id="added-6"><a class="header" href="#added-6">Added</a></h3>
<ul>
<li>Document our portable schema format.</li>
<li>Document schema-only drivers.</li>
<li>Improve the documentation in other minor ways.</li>
</ul>
<h3 id="removed-4"><a class="header" href="#removed-4">Removed</a></h3>
<ul>
<li>Remove <code>DataType::Other(String)</code>, which was not actually used by any of our drivers.</li>
</ul>
<h2 id="040-beta1---2020-05-28"><a class="header" href="#040-beta1---2020-05-28">0.4.0-beta.1 - 2020-05-28</a></h2>
<p>We're finally ready to start preparing for an 0.4.0 release! This beta will be deployed to several production systems to help verify that there are no surprising regressions.</p>
<h3 id="changed-13"><a class="header" href="#changed-13">Changed</a></h3>
<ul>
<li>gs: We now verify CRC32C checksums when uploading.</li>
<li>gs: We specify <code>isGenerationMatch</code> on many operations to make sure that nothing has been created or overridden that we didn't expect.</li>
</ul>
<h2 id="040-alpha7---2020-05-26"><a class="header" href="#040-alpha7---2020-05-26">0.4.0-alpha.7 - 2020-05-26</a></h2>
<p>This release adds support for labeling BigQuery jobs.</p>
<h3 id="added-7"><a class="header" href="#added-7">Added</a></h3>
<ul>
<li>bigquery: Optionally specify billing labels for jobs. See the manual for details.</li>
<li>Allow driver argument names to be specified as either <code>x.y</code> or <code>x[y]</code>, interchangeably. This makes <code>job_labels</code> look nicer.</li>
<li>Hide URL passwords from (most) logs using a dedicated wrapper type.</li>
</ul>
<h3 id="changed-14"><a class="header" href="#changed-14">Changed</a></h3>
<ul>
<li>We now have test cases that make sure we catch duplicate driver arguments and raise an error.</li>
<li>redshift: Authentication argument names may no longer include <code>-</code> characters. I'm not even sure whether these are valid, but they won't work with the new scheme for parsing driver arguments.</li>
<li><code>DriverArguments::from_cli_args</code> now takes an iterator instead of a slice.</li>
</ul>
<h2 id="040-alpha6---2020-05-22"><a class="header" href="#040-alpha6---2020-05-22">0.4.0-alpha.6 - 2020-05-22</a></h2>
<p>This release improves the example <code>shopify.ts</code> schema, and adds new features to <code>dbcrossbar-ts</code> to parse it.</p>
<h3 id="added-8"><a class="header" href="#added-8">Added</a></h3>
<ul>
<li>dbcrossbar-ts:
<ul>
<li>Parse <code>/* */</code> comments.</li>
<li>Allow <code>Date</code> to be used as a type. This requires the date to be a string in ISO 8601 format, including a time zone.</li>
<li>Allow <code>decimal</code>, <code>int16</code>, <code>int32</code> and <code>int64</code> to be defined as any of <code>number</code>, <code>string</code>, <code>number | string</code> or <code>string | number</code>. This allows the schema to more accurately represent what appears on the wire. It allows <code>decimal</code> values to be represented as a mix of floats and strings, which is seen in Shopify.</li>
</ul>
</li>
<li>postgres-sql: Use new format for parse errors.</li>
</ul>
<h3 id="fixed-15"><a class="header" href="#fixed-15">Fixed</a></h3>
<ul>
<li>shopify: The example <code>shopify.ts</code> schema has been updated to use <code>Date</code> and <code>int64</code> in many places. <code>Address</code> and <code>CustomerAddress</code> are now distinct types, and several other minor issues have been fixed.</li>
</ul>
<h2 id="040-alpha5---2020-05-21"><a class="header" href="#040-alpha5---2020-05-21">0.4.0-alpha.5 - 2020-05-21</a></h2>
<h3 id="added-9"><a class="header" href="#added-9">Added</a></h3>
<ul>
<li>BigQuery: Support <code>--if-exists=error</code>.</li>
</ul>
<h3 id="changed-15"><a class="header" href="#changed-15">Changed</a></h3>
<ul>
<li>Require <code>--enable-unstable</code> to use <code>dbcrossbar-ts</code> or <code>shopify</code> locators, which are unstable.</li>
<li>AWS credentials must now always be passed via <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code> (optional) and <code>AWS_REGION</code> (optional). This lays the groundwork for replacing the <code>aws</code> CLI tool with native Rust code, so that we will someday be able to remove our last CLI dependency.</li>
</ul>
<h2 id="040-alpha4---2020-05-19"><a class="header" href="#040-alpha4---2020-05-19">0.4.0-alpha.4 - 2020-05-19</a></h2>
<h3 id="added-10"><a class="header" href="#added-10">Added</a></h3>
<ul>
<li>BigQuery now imports and exports decimal (aka NUMERIC) values everywhere.</li>
<li>The <code>dbcrossbar-ts</code> driver now supports magic type aliases that will convert to the corresponding <code>dbcrossbar</code> types:
<ul>
<li><code>type decimal = string;</code></li>
<li><code>type int16 = number | string;</code></li>
<li><code>type int32 = number | string;</code></li>
<li><code>type int64 = number | string;</code></li>
</ul>
</li>
</ul>
<h3 id="changed-16"><a class="header" href="#changed-16">Changed</a></h3>
<ul>
<li>The sample <code>shopify.ts</code> schema now uses <code>decimal</code> instead of <code>string</code> when appropriate. It does not attempt to use <code>int64</code> yet.</li>
</ul>
<h2 id="040-alpha3---2020-05-19-0-yanked"><a class="header" href="#040-alpha3---2020-05-19-0-yanked">0.4.0-alpha.3 - 2020-05-19 0 YANKED</a></h2>
<p>This release was yanked because it was missing several things it should have included.</p>
<h2 id="040-alpha2---2020-05-19"><a class="header" href="#040-alpha2---2020-05-19">0.4.0-alpha.2 - 2020-05-19</a></h2>
<p>This is a significant release, with support for &quot;struct&quot; types.</p>
<h3 id="added-11"><a class="header" href="#added-11">Added</a></h3>
<ul>
<li>The portable schema now supports a <code>DataType::Struct(fields)</code> type that can be used to represent BigQuery STRUCT values (as long as they have unique, named fields) and JSON objects with known keys.</li>
<li>The BigQuery driver now supports importing and exporting <code>STRUCT</code> fields using the new <code>DataType::Struct(fields)</code> type.</li>
<li>EXPERIMENTAL: Schemas can now be specified using the <code>dbcrossbar-ts</code> driver, which supports subset of TypeScript type declarations. This is useful for specifying complex, nested structs. This can be used as <code>--schema=&quot;dbcrossbar-ts:shopify.ts#Order&quot;</code>, where <code>Order</code> is the name of the type within the <code>*.ts</code> file to use as the table's type.</li>
<li>EXPERIMENTAL: We now support a Shopify input driver that uses the Shopify REST API. See the manual for details.</li>
<li>We now have support for fancy parser error messages, which we use with the <code>dbcrossbar-ts</code> parser.</li>
<li>We now support a CLI-editable config file using commands like <code>dbcrossbar config add temporary s3://example/temp/</code>.</li>
</ul>
<h3 id="changed-17"><a class="header" href="#changed-17">Changed</a></h3>
<ul>
<li>BREAKING: Some corner cases involving struct types and JSON may have changed subtly.</li>
<li>We've upgraded to the latest <code>rust-peg</code> parser syntax everywhere.</li>
</ul>
<h3 id="fixed-16"><a class="header" href="#fixed-16">Fixed</a></h3>
<ul>
<li><code>--if-exists=overwrite</code> now overwrites when writing to local files (instead of appending).</li>
<li>We automatically create <code>~/.local/share</code> if it does not exist.</li>
<li>More <code>clippy</code> warnings have been fixed, and unsafe code has been forbidden.</li>
<li>Various obsolete casting libraries have been removed.</li>
</ul>
<h2 id="040-alpha1---2020-04-07"><a class="header" href="#040-alpha1---2020-04-07">0.4.0-alpha.1 - 2020-04-07</a></h2>
<h3 id="changed-18"><a class="header" href="#changed-18">Changed</a></h3>
<ul>
<li>Replace <code>gcloud auth</code>, <code>gsutil</code> and <code>bq</code> with native Rust. This changes how we authenticate to Google Cloud. In particular, we now support <code>GCLOUD_CLIENT_SECRET</code>, <code>~/.config/dbcrossbar/gcloud_client_secret.json</code>, <code>GCLOUD_SERVICE_ACCOUNT_KEY</code> or <code>~/.config/dbcrossbar/gcloud_service_account_key.json</code>, as <a href="https://www.dbcrossbar.org/gs.html#configuration--authentication">explained in the manual</a>. We no longer use <code>gcloud auth</code>, and the Google Cloud SDK tools are no longer required. In the current alpha version, uploads and deletions are probably slower than before.</li>
</ul>
<h3 id="fixed-17"><a class="header" href="#fixed-17">Fixed</a></h3>
<ul>
<li>gs: Avoid download stalls when backpressure is applied (<a href="https://github.com/dbcrossbar/dbcrossbar/issues/102">#103</a>).</li>
<li>bigquery: Display error messages more reliably (<a href="https://github.com/dbcrossbar/dbcrossbar/issues/110">#110</a>).</li>
<li>bigquery: Detect &quot;`&quot; quotes in the CLI form of table names, and report an error.</li>
</ul>
<h2 id="033---2020-03-30"><a class="header" href="#033---2020-03-30">0.3.3 - 2020-03-30</a></h2>
<h3 id="added-12"><a class="header" href="#added-12">Added</a></h3>
<ul>
<li>BigML: Honor BIGML_DOMAIN, allowing the user to point the BigML driver to a custom VPC instance of BigML.</li>
</ul>
<h2 id="032---2020-03-30"><a class="header" href="#032---2020-03-30">0.3.2 - 2020-03-30</a></h2>
<h3 id="fixed-18"><a class="header" href="#fixed-18">Fixed</a></h3>
<ul>
<li>Correctly quote BigQuery column names again (which regressed in 0.3.0), and added test cases to prevent further regressions.</li>
<li>Fix an error that caused <code>bigquery_upsert</code> test to fail.</li>
</ul>
<h2 id="031---2020-03-29"><a class="header" href="#031---2020-03-29">0.3.1 - 2020-03-29</a></h2>
<h3 id="added-13"><a class="header" href="#added-13">Added</a></h3>
<ul>
<li>Write a new <a href="https://www.dbcrossbar.org/">manual</a>!</li>
</ul>
<h3 id="changed-19"><a class="header" href="#changed-19">Changed</a></h3>
<ul>
<li>Encapsulate all calls to <code>bq</code> and <code>gsutil</code></li>
<li>Improve performance of <code>--stream-size</code></li>
</ul>
<h3 id="fixed-19"><a class="header" href="#fixed-19">Fixed</a></h3>
<ul>
<li>BigQuery: Honor NOT NULL on import (fixes #45)</li>
</ul>
<h2 id="030---2020-03-26"><a class="header" href="#030---2020-03-26">0.3.0 - 2020-03-26</a></h2>
<h3 id="added-14"><a class="header" href="#added-14">Added</a></h3>
<ul>
<li>Use <code>cargo deny</code> to enforce license and duplicate dependency policies</li>
<li>Add notes about license and contribution policies</li>
</ul>
<h3 id="changed-20"><a class="header" href="#changed-20">Changed</a></h3>
<ul>
<li>Update to tokio 0.2 and the latest stable Rust</li>
<li>Replace <code>wkb</code> with <code>postgis</code> for licensing reasons</li>
<li>BigML: Fail immediately if no S3 temporary bucket provided (fixes #101)</li>
</ul>
<h3 id="fixed-20"><a class="header" href="#fixed-20">Fixed</a></h3>
<ul>
<li>BigQuery: Handle mixed-case column names using BigQuery semantics (fixes #84)</li>
<li>PostgreSQL: Fix upserts with mixed-case column names</li>
<li>BigQuery: Correctly output NULL values in Boolean columns (#104)</li>
</ul>
<h3 id="removed-5"><a class="header" href="#removed-5">Removed</a></h3>
<ul>
<li>BREAKING: BigQuery: Remove code that tried to rename column names to make them valid (fixes #84)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="credits-and-contributors"><a class="header" href="#credits-and-contributors">Credits and contributors</a></h1>
<p>The development of <code>dbcrossbar</code> has been generously supported by <a href="https://faraday.io/">Faraday</a>, which provides datascience and AI products for B2C companies.</p>
<p>Ongoing development of <code>dbcrossbar</code> is also supported by <a href="http://kiddsoftware.com/">Kidd Software LLC</a>, which creates custom software for businesses around the world.</p>
<h2 id="contributors"><a class="header" href="#contributors">Contributors</a></h2>
<p><code>dbcrossbar</code> is primarily maintained by Eric Kidd. Other contributors include:</p>
<ul>
<li>Bill Morris</li>
<li>Forrest Wallace</li>
<li>Prithaj Nath</li>
<li>Seamus Abshere</li>
</ul>
<p>We have also received very helpful bug reports and feature requests from our users. Thank you!</p>

                </main>

                <nav class="nav-wrapper" aria-label="Page navigation">
                    <!-- Mobile navigation buttons -->
                    <div style="clear: both"></div>
                </nav>
            </div>
        </div>

        <nav class="nav-wide-wrapper" aria-label="Page navigation">
        </nav>

    </div>

    <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
    <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
    <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
    <script src="book.js" type="text/javascript" charset="utf-8"></script>

    <!-- Custom JS scripts -->
    <script type="text/javascript">
        window.addEventListener('load', function () {
            window.setTimeout(window.print, 100);
        });
    </script>
</body>

</html>